# csc-583-search-project

## About
This repository is a school project for University of Arizona's Spring 2023 semester of `CSC 583 Text Retrieval and Web Search` course. it is not intended to be used in a production environment. This project was intended as a hands on experience in building a search index and evaluating various performance statistics between a classic search index built with Lucene an index build with a Transformer model and Spotify Annoy

## Hardware Requirements
The Python files in this for generating CLS embeddings from a Transformer network were run on a Google Collab VM with an NVidia GPU, and further processed offline on a desktop with 48GB of RAM. Some parts of this code may not run 
effectively on a system with less memory and may require you to tweak a `batchSize` setting 
`SentenceEmbeddings_CPU.py` and `SentenceEmbeddings.ipynb`

## Java Code Documentation
Prior to running the application you will need to  have maven installed and  extract the [Arxiv data set](https://www.kaggle.com/datasets/Cornell-University/arxiv ) to `src/main/resources`. You do not need to do any preprocessing to the arxive data set prior to running the app. 

The Java code is responsible for the following tasks
1) Building a Lucene Index from the Arxiv data set, and saving it locally for later use
2) Running queries found in `src/main/resources/lucene-queries.txt` against the index and saving the top 10 results to a JSON file for further analysis
3) Reverse lookup of of a document ID that was pulled from query results from an Annoy index that was generated by the Python code in this repo to fetch the document title and abstract.

### Building the App
To run the Java code in this repository, you will need to install Maven. Once that is done simply call 
```
mvn compile
mvn exec:java
```
If a lucene index does not exist, the app will parse the data file and build a new index before running the queries

### Running the App 
 To build a Lucene index and run queries from a pre-existing file, enter `L` when prompted by the application. If a lucene index does not exist, the app will parse the data file and build a new index before running the queries. 
 
 If you have results from an Annoy Index and need to resolve the source documents the results map to, enter `A` at the prompt instead. In both cases, the results are saved as a JSON file with with the following properties
* Score - the Lucene score or Annoy Score the document was ranked as. For annoy results, this will remain the original Annoy score after document resolution
* docId - The “line number” the document was located at in the original data set
* query - The query executed that retrieved the document
* paperAbstract - The full abstract for the document from the Arxiv data set
* title - The full title for the document from the Arxiv data set 

## Python Code Documentation
The Python code assumes that the JSON file for the  Arxiv data set has been extracted to `src/main/resources`. Once extracted, open a terminal and navigate to the root folder containing the source code and run  `python -u "src/main/python/DataPreprocessor.py"` to preform preprocessing on the data set prior to facilitate building the Annoy index



### Python Modules to install for local Development
This project was mostly built using Vanilla Python instead of a more complete data science distribution 
line Anaconda. To install the necessary Python packages used by this project, from the root directory 
run the following command

`pip install -r requirements.txt`

Alternatively, you can run the following series of pip commands

```
 pip install gensim
 pip install torch
 pip install transformers
 pip install smart_open
 pip install termcolor
 pip install annoy
```

Windows Users: in order to install annoy download `annoy-1.17.0-cp311-cp311-win_amd64.whl` from [https://www.lfd.uci.edu/~gohlke/pythonlibs/]( here ) as described at [https://www.programmersought.com/article/95834605670/]()

 Then copy it into the root directory of this repo on your local system and run `pip install annoy-1.17.0-cp311-cp311-win_amd64.whl`

### Generating Word Embeddings 
This projected utilized a Google Collab instance for GPU access. To run this upload the notebook `SentenceEmbeddings.ipynb` to a Google Collab Instance as well as the `stopwords` data file generated by `DataPreprocessory.py` to  a Google Collab instance.that has GPU support enabled. Without GPU support, building the annoy index will take in excess of  140 hours. The Notebook has additional documentation in it on file path expectations. It will generate and download a zip file containing the CLS embeddings used to build an Annoy Index since the team maxed out its free GPU time during development. 

### Building the Annoy Index
Download the batched zip file and extract its contents to `src/main/resources/batches`. This directory should contain only the batch files and not have them hidden in sub directories. Once extracted simply run `python -u "src/main/python/SentenceEmbeddings_CPU.py"`. This will build the Annoy index using a CPU and then run the queries in `src/main/resources/lucene-queries.txt`.  

Once the queries have been run against annoy, a results file will be saved to `src/main/resources/results`. To resolve the contents of the document the results refer to, simply run the java app but pass in `A` when prompted. 

## File and Directory Manifest


### Python File Descriptions
* DataFilePaths.py - Contains variables for the file paths used by the various python scripts in this repo. Not used by `SentenceEmbeddings.py`
* DataPreprocessor.py - Preforms Stop Word removal and Tokenization of the Arxiv data set so Scripts building the Annoy Index do not need to preform preprocessing each time they are run.
* SentenceEmbeddings.ipynb - A Jupyter Notebook used to generate the CLS Embedding Vectors to train the Annoy Index
* SentenceEmbeddings_CPU.py - Uses the CLS embeddings generated by the Jupyter to build and save an Annoy Index. Will run queries saved in a txt file against the index and save the query output to `src/main/resources/results`. This file is configured to run on a CPU.
* annoy_gensim.py - Builds an Annoy Index using GenSim `doc2Vec`. Note the team was unable to get meaningful query results from this approach and abandoned it midway due to time constraints.
* CumGainCalculator.py - Computes Normalized Discounted Cumulative Gain scores from the Annoy Index results. Since this metric relies on user defined scores, results used by our project submission use the saveD JSON files in `results/` .
* Ensemble_scoring.py - Compute Ensemble ranking scores from 2 different Index+Scoring methods. 
* arxiv-metadata-oai-snapshot-lite.json - The first 10 entries in the Arxiv data set used for quick testing and debugging

### Java File Descriptions
* SearchEngineMain.java - Main Class
* ArxiveMetadata.java - Model Class used map the JSON objects in the Arxiv data set for indexing
* AuthorsParsed.java - Additional Model Class
* Version.java - Additional Model class for parsing the Arxiv Data Set 
* FullResult.java - Model Class for the JSON objects representing the results that get saved to files for additional processing
* ScoredResult.java - Base Model Class of FullResult.java
* ProcessJSONAndCreateIndex.java - Multiple Responsibilities
	* Build a Lucene Index from the Arxiv Data Set
	* Run queries from a txt file against the Lucene Index using TF-IDF scoring and save the results to a JSON file
	* Takes a JSON results file generated by the Python Annoy index and find the title and abstract the each result originates to since Annoy does not contain this information.


## References 
https://nguyen-hoang-nguyen.medium.com/how-to-check-your-google-colab-session-for-allocated-resources-912b1af9b99a
### Word2Vec and Doc2Vec
https://github.com/clulab/gentlenlp/blob/main/notebooks/chap09_classification.ipynb
https://github.com/v1shwa/document-similarity
https://stackoverflow.com/questions/65852710/text-similarity-using-word2vec
https://www.codegram.com/blog/finding-similar-documents-with-transformers/

