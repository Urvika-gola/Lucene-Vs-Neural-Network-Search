[{
        "title": "Black Holes and Gravitational Properties of Antimatter",
        "paperAbstract": "  The gravitational properties of antimatter are still a secret of nature. One\noutstanding possibility is that there is a gravitational repulsion between\nmatter and antimatter (in short we call it antigravity). We argue that in the\ncase of antigravity the collapse of a black hole doesn\u0027t end with singularity\nand that deep inside the horizon, the gravitational field may be sufficiently\nstrong to create (from the vacuum) neutrino-antineutrino pairs of all flavours.\nThe created antineutrinos (neutrinos) should be violently ejected outside the\nhorizon of a black hole composed from matter (antimatter). Our rudimentary\ncalculations suggest that both, the supermassive black hole in the centre of\nour Galaxy and in the centre of the Andromeda Galaxy may produce a flux of\nantineutrinos measurable with the new generation of one cubic kilometre\nneutrino telescopes. In addition, we suggest two signatures of antigravity in\nthe case of microscopic black holes which may be eventually produced at CERN:\nthe decay products should exhibit a strong matter-antimatter asymmetry and the\nHawking radiation should not be the main mechanism for decay.\n",
        "query": "dark matter black holes anti-desitter space",
        "docId": 1991655,
        "score": 0.6632948313873595,
        "userScore":1
    },
    {
        "title": "Anti-de Sitter space and black holes",
        "paperAbstract": "  Anti-de Sitter space with identified points give rise to black-hole\nstructures. This was first pointed out in three dimensions, and generalized to\nhigher dimensions by Aminneborg et al. In this paper, we analyse several\naspects of the five dimensional anti-de Sitter black hole including, its\nrelation to thermal anti-de Sitter space, its embedding in a Chern-Simons\nsupergravity theory, its global charges and holonomies, and the existence of\nKilling spinors.\n",
        "query": "dark matter black holes anti-desitter space",
        "docId": 2104809,
        "score": 0.6261864685002081,
        "userScore":2
    },
    {
        "title": "Antimatter and the second law of thermodynamics",
        "paperAbstract": "  In this short paper we make a proposal that the second law of thermodynamics\nholds true for a closed physical system consisting of pure antimatter in the\nthermodynamical limit, but in a reversed form. We give two plausible arguments\nin favour to this proposal: one refers to the CPT theorem of relativistic\nquantum field theories while the other one is based on general thermodynamical\narguments. However in our understanding the ultimate validity or invalidity of\nthis idea can be decided only by future physical experiments.\n  As a consequence of the proposal we argue that the dynamical evolution of\npure macroscopic antimatter systems can be very different from that of ordinary\nmatter systems in the sense that sufficiently massive antimatter systems could\nhave stronger tendency to form black holes during time evolution than their\nordinary counterparts. Taking into account the various uniqueness theorems in\nblack hole physics as well, as a result, antimatter could tracelessly disappear\nbehind black hole event horizons faster in time than ordinary matter. The\nobserved asymmetry of matter and antimatter could then be explained even if\ntheir presence in the Universe was symmetric in the beginning.\n",
        "query": "dark matter black holes anti-desitter space",
        "docId": 836262,
        "score": 0.6179811859664639,
        "userScore":1
    },
    {
        "title": "On the Penrose inequality in anti-deSitter space",
        "paperAbstract": "  For asymptotically flat spacetimes the Penrose inequality gives an initial\ndata test for the weak cosmic censorship hypothesis. We give a formulation of\nthis inequality for asymptotically anti-deSitter (AAdS) spacetimes, and show\nthat the inequality holds for time asymmetric data in spherical symmetry. Our\nanalysis is motivated by the constant-negative-spatial-curvature form of the\nAdS black hole metric.\n",
        "query": "dark matter black holes anti-desitter space",
        "docId": 887261,
        "score": 0.6011371984374989,
        "userScore":0
    },
    {
        "title": "Negative Mass Black Holes in de-Sitter Space",
        "paperAbstract": "  We show that asymptotically de Sitter black holes of negative mass can exist\nin Lovelock gravity. Such black holes have horizon geometries with non-constant\ncurvature and are known as Exotic Black Holes. We explicitly examine the case\nof Gauss-Bonnet gravity. We briefly discuss the positive mass case where we\nshow how the transverse space geometry affects whether a black hole will exist\nor not.For negative mass solutions we shown how three different black hole\nspacetimes are possible depending on the transverse space geometry. We also\nprovide closed form expressions for the geometric parameters to ensure that a\nblack hole spacetime is observed. We close with a discussion of the massless\ncase, where there are many different spacetimes that are permitted.\n",
        "query": "dark matter black holes anti-desitter space",
        "docId": 1679405,
        "score": 0.6005072191292111,
        "userScore":2
    },
    {
        "title": "Black Holes of Constant Curvature",
        "paperAbstract": "  Black holes of constant curvature are constructed by identifying points in\nanti-de Sitter space. In n dimensions the resulting topology is R^{n-1} * S_1,\nas opposed to the usual R^2 * S_{n-2} Schwarzschild black hole.\n",
        "query": "dark matter black holes anti-desitter space",
        "docId": 1995763,
        "score": 0.5983536967589185,
        "userScore":1
    },
    {
        "title": "The evolution of unstable black holes in anti-de Sitter space",
        "paperAbstract": "  We examine the thermodynamic stability of large black holes in\nfour-dimensional anti-de Sitter space, and we demonstrate numerically that\nblack holes which lack local thermodynamic stability often also lack stability\nagainst small perturbations. This shows that no-hair theorems do not apply in\nanti-de Sitter space. A heuristic argument, based on thermodynamics only,\nsuggests that if there are any violations of Cosmic Censorship in the evolution\nof unstable black holes in anti-de Sitter space, they are beyond the reach of a\nperturbative analysis.\n",
        "query": "dark matter black holes anti-desitter space",
        "docId": 2069027,
        "score": 0.5924532963314206,
        "userScore":3
    },
    {
        "title": "Quantum generation of Schwarzschild-de Sitter (Nariai) black holes in\n  effective dilaton-Maxwell gravity",
        "paperAbstract": "  Dilaton coupled electromagnetic field is essential element of low-energy\nstring effective action or it may be considered as result of spherical\ncompactification of Maxwell theory in higher dimensions. The large $N$ and\nlarge curvature effective action for $N$ dilaton coupled vectors is calculated.\nAdding such quantum correction to classical dilaton gravity action we show that\neffective dilaton-Maxwell gravity under consideration may generate\nSchwarzschild-de Sitter black holes (SdS BHs) with constant dilaton as\nsolutions of the theory. That suggests a mechanism (alternative to BHs\nproduction) for quantum generation of SdS BHs in early universe (actually, for\nquantum creation of inflationary Universe) due to back-reaction of dilaton\ncoupled matter. The possibility of proliferation of anti-de Sitter space is\nbriefly discussed.\n",
        "query": "dark matter black holes anti-desitter space",
        "docId": 2105541,
        "score": 0.5850631622259925,
        "userScore":1
    },
    {
        "title": "Pair Production of Topological anti de Sitter Black Holes",
        "paperAbstract": "  The pair creation of black holes with event horizons of non-trivial topology\nis described. The spacetimes are all limiting cases of the cosmological $C$\nmetric. They are generalizations of the $(2+1)$ dimensional black hole and have\nasymptotically anti de Sitter behaviour. Domain walls instantons can mediate\ntheir pair creation for a wide range of mass and charge.\n",
        "query": "dark matter black holes anti-desitter space",
        "docId": 1994660,
        "score": 0.5804108403119059,
        "userScore":1
    },
    {
        "title": "Making Anti-de Sitter Black Holes",
        "paperAbstract": "  It is known from the work of Banados et al. that a space-time with event\nhorizons (much like the Schwarzschild black hole) can be obtained from 2+1\ndimensional anti-de Sitter space through a suitable identification of points.\nWe point out that this can be done in 3+1 dimensions as well. In this way we\nobtain black holes with event horizons that are tori or Riemann surfaces of\ngenus higher than one. They can have either one or two asymptotic regions.\nLocally, the space-time is isometric to anti-de Sitter space.\n",
        "query": "dark matter black holes anti-desitter space",
        "docId": 1994369,
        "score": 0.5789320686514383,
        "userScore":2
    },
    {
        "title": "Novel approach to Room Temperature Superconductivity problem",
        "paperAbstract": "  A long-standing problem of observing Room Temperature Superconductivity is\nfinally solved by a novel approach. Instead of increasing the critical\ntemperature Tc of a superconductor, the temperature of the room was decreased\nto an appropriate Tc value. We consider this approach more promising for\nobtaining a large number of materials possessing Room Temperature\nSuperconductivity in the near future.\n",
        "query": "room temperature super conductor",
        "docId": 1264676,
        "score": 0.7296710276273117,
        "userScore":1
    },
    {
        "title": "Room-Temperature Superconductivity",
        "paperAbstract": "  This is the first book on the subject of room-temperature superconductivity.\nThe main purpose of the book is twofold. First, to show that, under suitable\nconditions, superconductivity can occur above room temperature. Secondly, to\npresent general guidelines on how to synthesize a room temperature\nsuperconductor. The book begins with an introduction into the physics of the\nsuperconducting state and superconducting materials. The mechanisms of\nconventional, half-conventional and unconventional superconductivity are\ndiscussed in the following chapters. The last three chapters of the book are\ndevoted to room temperature superconductivity. In Chapter 2, an attempt to\nreview the basic properties of the superconducting state independently of any\nspecific mechanism is made for the first time. In addition, four principles of\nsuperconductivity valid for any type of superconductivity are introduced in\nChapter 4. The book is mainly addressed to specialists in materials science and\nin the field of superconductivity. At the same time, students will also benefit\nfrom reading the first nine chapters of the book.\n",
        "query": "room temperature super conductor",
        "docId": 1945581,
        "score": 0.7086689009596068,
        "userScore":3
    },
    {
        "title": "The model of a hypothetical room-temperature superconductor",
        "paperAbstract": "  The model of hypothetical superconductivity, where the energy gap\nasymptotically approaches zero as temperature increases, has been proposed.\nFormally the critical temperature of such a superconductor is equal to\ninfinity. For practical realization of the hypothesis a superconducting\nmaterial with such properties is predicted.\n",
        "query": "room temperature super conductor",
        "docId": 552067,
        "score": 0.6729107368122396,
        "userScore":2
    },
    {
        "title": "All-Temperature Superconductor",
        "paperAbstract": "  Several surprises are beginning to emerge from studies of nanostructures:\nwhereas increased resistance with decreased thickness has been expected, the\nexact opposite in several instances has been found. It is beginning to seem we\nare in the process of wedging open the way to a superconductor that will\noperate at least at room temperature. This paper will examine some pertinent\nexperimental findings since the turn of the century.\n",
        "query": "room temperature super conductor",
        "docId": 1943489,
        "score": 0.6458046545658345,
        "userScore":1
    },
    {
        "title": "High-temperature superconductors: underlying physics and applications",
        "paperAbstract": "  Superconductivity was discovered in 1911 by Kamerlingh Onnes and Holst in\nmercury at the temperature of liquid helium (4.2 K). It took almost 50 years\nuntil in 1957 a microscopic theory of superconductivity, the so-called BCS\ntheory, was developed. Since the discovery a number of superconducting\nmaterials were found with transition temperatures up to 23 K. A breakthrough in\nthe field happened in 1986 when Bednorz and M\\\"uller discovered a new class of\nsuperconductors, the so-called cuprate high-temperature superconductors with\ntransition temperatures as high as 135 K. This surprising discovery initiated\nnew efforts with respect to fundamental physics, material science, and\ntechnological applications. In this brief review the basic physics of the\nconventional low-temperature superconductors as well as of the high-temperature\nsuperconductors are presented with a brief introduction to applications\nexemplified from high-power to low-power electronic devices. Finally, a short\noutlook and future challenges are presented, finished with possible\nimaginations for applications of room-temperature superconductivity.\n",
        "query": "room temperature super conductor",
        "docId": 1200897,
        "score": 0.6455389714782243,
        "userScore":2
    },
    {
        "title": "New Theory of Superconductivity (Part II): Confirmation at low\n  temperature",
        "paperAbstract": "  An application of the general equation obtained in Part I to low critical\ntemperature superconductors utilizing an \"ad hoc\" phononic theory is developed.\nThen, we arrive to a specific expression for the bounding energy as a function\nof temperature. The density of states of the electron pairs is calculated and\nused to obtain an equation for the critical magnetic field. This result is\nneeded for determine the electrodynamical properties. Finally, we obtain the\nspecific heat as a function of temperature and compare it to experimental data\nfor Sn, and calculate its jump at Tc for eight superconductors.\n",
        "query": "room temperature super conductor",
        "docId": 1925766,
        "score": 0.6429280415576386,
        "userScore":2
    },
    {
        "title": "Maintaining the local temperature below the critical value in thermally\n  out of equilibrium superconducting wires",
        "paperAbstract": "  A generalized theory of open quantum systems combined with mean-field theory\nis used to study a superconducting wire in contact with thermal baths at\ndifferent temperatures. It is shown that, depending on the temperature of the\ncolder bath, the temperature of the hotter bath can greatly exceed the\nequilibrium critical temperature, and still the local temperature in the wire\nis maintained below the critical temperature and hence the wire remains in the\nsuperconducting state. The effects of contact areas and disorder are studied.\nFinally, an experimental setup is suggested to test our predictions.\n",
        "query": "room temperature super conductor",
        "docId": 134663,
        "score": 0.6347090837125631,
        "userScore":1
    },
    {
        "title": "Elevated critical temperature at BCS superconductor-band insulator\n  interfaces",
        "paperAbstract": "  We consider the interface between a Bardeen-Cooper-Schrieffer superconductor\nand non-superconducting band insulator. We show that under certain conditions,\nsuch interfaces can have an elevated superconducting critical temperature,\nwithout increasing the strength of the pairing interaction at the interface. We\nidentify the regimes where the interface critical temperature exceeds the\ncritical temperature associated with a superconductor-vacuum interface.\n",
        "query": "room temperature super conductor",
        "docId": 1596986,
        "score": 0.6334853950427757,
        "userScore":2
    },
    {
        "title": "AC/RF Superconductivity",
        "paperAbstract": "  This contribution provides a brief introduction to AC/RF superconductivity,\nwith an emphasis on application to accelerators. The topics covered include the\nsurface impedance of normal conductors and superconductors, the residual\nresistance, the field dependence of the surface resistance, and the\nsuperheating field.\n",
        "query": "room temperature super conductor",
        "docId": 594146,
        "score": 0.6292888445934111,
        "userScore":1
    },
    {
        "title": "Cuprates - An Overview",
        "paperAbstract": "  A brief overview is given of the problem of high temperature\nsuperconductivity in the cuprates, with an emphasis on theoretical ideas.\n",
        "query": "room temperature super conductor",
        "docId": 281178,
        "score": 0.6250860576239745,
        "userScore":1
    },
    {
        "title": "Google\u0027s Quantum Supremacy Claim: Data, Documentation, and Discussion",
        "paperAbstract": "  In October 2019, Nature published a paper describing an experiment that took\nplace at Google. The paper claims to demonstrate quantum (computational)\nsupremacy on a 53-qubit quantum computer. Since September 2019 we have been\ninvolved in a long-term project to study various statistical aspects of the\nGoogle experiment. We have been trying to gather the relevant data and\ninformation in order to reconstruct and verify those parts of the Google\nexperiment that are based on classical computations (except when the required\ncomputation is too heavy), and to perform a statistical analysis on the data.\nThis document describes the available data and information for the Google\nexperiment, some main questions in the evaluation of the experiment, and some\nof our results and plans.\n",
        "query": "quantum supremacy",
        "docId": 1734108,
        "score": 0.6292096488119778,
        "userScore":2
    },
    {
        "title": "What Have Google\u0027s Random Quantum Circuit Simulation Experiments\n  Demonstrated about Quantum Supremacy?",
        "paperAbstract": "  Quantum computing is of high interest because it promises to perform at least\nsome kinds of computations much faster than classical computers. Arute et al.\n2019 (informally, \"the Google Quantum Team\") report the results of experiments\nthat purport to demonstrate \"quantum supremacy\" -- the claim that the\nperformance of some quantum computers is better than that of classical\ncomputers on some problems. Do these results close the debate over quantum\nsupremacy? We argue that they do not. We provide an overview of the Google\nQuantum Team\u0027s experiments, then identify some open questions in the quest to\ndemonstrate quantum supremacy.\n",
        "query": "quantum supremacy",
        "docId": 1348862,
        "score": 0.6172472662246307,
        "userScore":2
    },
    {
        "title": "Quantum Scholasticism: On Quantum Contexts, Counterfactuals, and the\n  Absurdities of Quantum Omniscience",
        "paperAbstract": "  Unlike classical information, quantum knowledge is restricted to the outcome\nof measurements of maximal observables corresponding to single contexts.\n",
        "query": "quantum supremacy",
        "docId": 34365,
        "score": 0.6083631902617679,
        "userScore":0
    },
    {
        "title": "Information Theoretically Secure Hypothesis Test for Temporally\n  Unstructured Quantum Computation",
        "paperAbstract": "  We propose a new composable and information-theoretically secure protocol to\nverify that a server has the power to sample from a sub-universal quantum\nmachine implementing only commuting gates. By allowing the client to manipulate\nsingle qubits, we exploit properties of Measurement based Blind Quantum\nComputing to prove security against a malicious Server and therefore certify\nquantum supremacy without the need for a universal quantum computer.\n",
        "query": "quantum supremacy",
        "docId": 836510,
        "score": 0.6001684045056361,
        "userScore":0
    },
    {
        "title": "A classical leash for a quantum system: Command of quantum systems via\n  rigidity of CHSH games",
        "paperAbstract": "  Can a classical system command a general adversarial quantum system to\nrealize arbitrary quantum dynamics? If so, then we could realize the dream of\ndevice-independent quantum cryptography: using untrusted quantum devices to\nestablish a shared random key, with security based on the correctness of\nquantum mechanics. It would also allow for testing whether a claimed quantum\ncomputer is truly quantum. Here we report a technique by which a classical\nsystem can certify the joint, entangled state of a bipartite quantum system, as\nwell as command the application of specific operators on each subsystem. This\nis accomplished by showing a strong converse to Tsirelson\u0027s optimality result\nfor the Clauser-Horne-Shimony-Holt (CHSH) game: the only way to win many games\nis if the bipartite state is close to the tensor product of EPR states, and the\nmeasurements are the optimal CHSH measurements on successive qubits. This leads\ndirectly to a scheme for device-independent quantum key distribution. Control\nover the state and operators can also be leveraged to create more elaborate\nprotocols for realizing general quantum circuits, and to establish that QMIP \u003d\nMIP*.\n",
        "query": "quantum supremacy",
        "docId": 367612,
        "score": 0.5948538278796729,
        "userScore":0
    },
    {
        "title": "Classical command of quantum systems via rigidity of CHSH games",
        "paperAbstract": "  Can a classical system command a general adversarial quantum system to\nrealize arbitrary quantum dynamics? If so, then we could realize the dream of\ndevice-independent quantum cryptography: using untrusted quantum devices to\nestablish a shared random key, with security based on the correctness of\nquantum mechanics. It would also allow for testing whether a claimed quantum\ncomputer is truly quantum. Here we report a technique by which a classical\nsystem can certify the joint, entangled state of a bipartite quantum system, as\nwell as command the application of specific operators on each subsystem. This\nis accomplished by showing a strong converse to Tsirelson\u0027s optimality result\nfor the Clauser-Horne-Shimony-Holt (CHSH) game: the only way to win many games\nis if the bipartite state is close to the tensor product of EPR states, and the\nmeasurements are the optimal CHSH measurements on successive qubits. This leads\ndirectly to a scheme for device-independent quantum key distribution. Control\nover the state and operators can also be leveraged to create more elaborate\nprotocols for reliably realizing general quantum circuits.\n",
        "query": "quantum supremacy",
        "docId": 367613,
        "score": 0.5911854022328509,
        "userScore":0
    },
    {
        "title": "Quantum computing and the entanglement frontier",
        "paperAbstract": "  Quantum information science explores the frontier of highly complex quantum\nstates, the \"entanglement frontier.\" This study is motivated by the observation\n(widely believed but unproven) that classical systems cannot simulate highly\nentangled quantum systems efficiently, and we hope to hasten the day when well\ncontrolled quantum systems can perform tasks surpassing what can be done in the\nclassical world. One way to achieve such \"quantum supremacy\" would be to run an\nalgorithm on a quantum computer which solves a problem with a super-polynomial\nspeedup relative to classical computers, but there may be other ways that can\nbe achieved sooner, such as simulating exotic quantum states of strongly\ncorrelated matter. To operate a large scale quantum computer reliably we will\nneed to overcome the debilitating effects of decoherence, which might be done\nusing \"standard\" quantum hardware protected by quantum error-correcting codes,\nor by exploiting the nonabelian quantum statistics of anyons realized in solid\nstate systems, or by combining both methods. Only by challenging the\nentanglement frontier will we learn whether Nature provides extravagant\nresources far beyond what the classical world would allow.\n",
        "query": "quantum supremacy",
        "docId": 331172,
        "score": 0.5882051887729176,
        "userScore":1
    },
    {
        "title": "Nonexistence of a universal quantum machine to examine the precision of\n  unknown quantum states",
        "paperAbstract": "  In this work, we reveal a new type of impossibility discovered in our recent\nresearch which forbids comparing the closeness of multiple unknown quantum\nstates with any non-trivial threshold in a perfect or an unambiguous way. This\nimpossibility is distinct from the existing impossibilities in that it is a\n\"collective\" impossibility on multiple quantum states while most other \"no-go\"\ntheorems concern with only one single state each time, i.e., it is an\nimpossibility on a non-local quantum operation. This novel impossibility may\nprovide a new insight into the nature of quantum mechanics and it implies more\nlimitations on quantum information tasks than the existing \"no-go\" theorems.\n",
        "query": "quantum supremacy",
        "docId": 295846,
        "score": 0.5834150420521507,
        "userScore":1
    },
    {
        "title": "Relativistic Quantum Computing",
        "paperAbstract": "  We present some informal remarks on aspects of relativistic quantum\ncomputing.\n",
        "query": "quantum supremacy",
        "docId": 2226360,
        "score": 0.5816360833330165,
        "userScore":0
    },
    {
        "title": "Echoing the recent Google success: Foundational Roots of Quantum\n  Supremacy",
        "paperAbstract": "  The recent Google\u0027s claim on breakthrough in quantum computing is a gong\nsignal for further analysis of foundational roots of (possible) superiority of\nsome quantum algorithms over the corresponding classical algorithms. This note\nis a step in this direction. We start with critical analysis of rather common\nreference to entanglement and quantum nonlocality as the basic sources of\nquantum superiority. We elevate the role of the Bohr\u0027s {\\it principle of\ncomplementarity}\\footnote{} (PCOM) by interpreting the Bell-experiments as\nstatistical tests of this principle. (Our analysis also includes comparison of\nclassical vs genuine quantum entanglements.) After a brief presentation of PCOM\nand endowing it with the information interpretation, we analyze its\ncomputational counterpart. The main implication of PCOM is that by using the\nquantum representation of probability, one need not compute the joint\nprobability distribution (jpd) for observables involved in the process of\ncomputation. Jpd\u0027s calculation is exponentially time consuming. Consequently,\nclassical probabilistic algorithms involving calculation of jpd for $n$ random\nvariables can be over-performed by quantum algorithms (for big values of $n).$\nQuantum algorithms are based on quantum probability calculus. It is crucial\nthat the latter modifies the classical formula of total probability (FTP).\nProbability inference based on the quantum version of FTP leads to constructive\ninterference of probabilities increasing probabilities of some events. We also\nstress the role the basic feature of the genuine quantum superposition\ncomparing with the classical wave superposition: generation of discrete events\nin measurements on superposition states. Finally, the problem of superiority of\nquantum computations is coupled with the quantum measurement problem and\nlinearity of dynamics of the quantum state update.\n",
        "query": "quantum supremacy",
        "docId": 1208931,
        "score": 0.579479307985606,
        "userScore":2
    },
    {
        "title": "Spooky Action at a Distance",
        "paperAbstract": "  This article studies quantum mechanical entanglement. We begin by\nillustrating why entanglement implies action at a distance. We then introduce a\nsimple criterion for determining when a pure quantum state is entangled.\nFinally, we present a measure for the amount of entanglement for a pure state.\n",
        "query": "spooky action distance quantum entanglement encryption",
        "docId": 1291665,
        "score": 0.7977778291542279,
        "userScore":2
    },
    {
        "title": "Bounding the speed of `spooky action at a distance\u0027",
        "paperAbstract": "  In the well-known EPR paper, Einstein et al. called the nonlocal correlation\nin quantum entanglement as `spooky action at a distance\u0027. If the spooky action\ndoes exist, what is its speed? All previous experiments along this direction\nhave locality loopholes and thus can be explained without having to invoke any\n`spooky action\u0027 at all. Here, we strictly closed the locality loopholes by\nobserving a 12-hour continuous violation of Bell inequality and concluded that\nthe lower bound speed of `spooky action\u0027 was four orders of magnitude of the\nspeed of light if the Earth\u0027s speed in any inertial reference frame was less\nthan 10^(-3) times of the speed of light.\n",
        "query": "spooky action distance quantum entanglement encryption",
        "docId": 412271,
        "score": 0.7199323242432172,
        "userScore":3
    },
    {
        "title": "Private entanglement over arbitrary distances, even using noisy\n  apparatus",
        "paperAbstract": "  We give a security proof of quantum cryptography based entirely on\nentanglement purification. Our proof applies to all possible attacks\n(individual and coherent). It implies the security of cryptographic keys\ndistributed with the help of entanglement-based quantum repeaters. We prove the\nsecurity of the obtained quantum channel which may not only be used for quantum\nkey distribution, but also for secure, albeit noisy, transmission of quantum\ninformation.\n",
        "query": "spooky action distance quantum entanglement encryption",
        "docId": 2206382,
        "score": 0.700492087097869,
        "userScore":2
    },
    {
        "title": "End of Several Quantum Mysteries",
        "paperAbstract": "  I report on the discovery of quantum compatible local variables that are\nshared between subsystems of quantum-conventionally entangled physical systems\nsuch that they determine the correlations of spatially separated systems while\npreserving strict Einstein locality. This puts an end to the mystery of spooky\naction at a distance and alleged collapse at a distance, answering vital\nquestions, first raised in the EPR paper, on the behaviour of spatially\nseparated entangled systems. The solution helps to understand quantitative\nmeasures of entanglement in a transparent way. It also provides new insight,\nconsistent with strict locality, of the physics of quantum teleportation and\nrelated phenomena.\n",
        "query": "spooky action distance quantum entanglement encryption",
        "docId": 242681,
        "score": 0.6689919696688946,
        "userScore":2
    },
    {
        "title": "Quantum cryptography with and without entanglement",
        "paperAbstract": "  Quantum cryptography is reviewed, first using entanglement both for the\nintuition and for the experimental realizations. Next, the implementation is\nsimplified in several steps until it becomes practical. At this point\nentanglement has disappeared. This method can be seen as a lesson of Applied\nPhysics. Finally, security issues, e.g. photon number splitting attacks, and\ncounter-measures are discussed.\n",
        "query": "spooky action distance quantum entanglement encryption",
        "docId": 2213284,
        "score": 0.6589142551560752,
        "userScore":2
    },
    {
        "title": "Unicity Distance of Quantum Encryption Protocols",
        "paperAbstract": "  Shannon presented the concept `unicity distance\u0027 for describing the security\nof secret key encryption protocols against various ciphertext-only attacks. We\ndevelop this important concept of cryptanalysis into the quantum context, and\nfind that there exist quantum encryption protocols reusing a short key have\ninfinite unicity distance.\n",
        "query": "spooky action distance quantum entanglement encryption",
        "docId": 338146,
        "score": 0.6532994865531734,
        "userScore":3
    },
    {
        "title": "Entangled-state cryptographic protocol that remains secure even if\n  nonlocal hidden variables exist and can be measured with arbitrary precision",
        "paperAbstract": "  Standard quantum cryptographic protocols are not secure if one assumes that\nnonlocal hidden variables exist and can be measured with arbitrary precision.\nThe security can be restored if one of the communicating parties randomly\nswitches between two standard protocols.\n",
        "query": "spooky action distance quantum entanglement encryption",
        "docId": 2216112,
        "score": 0.6375653990412218,
        "userScore":2
    },
    {
        "title": "Secure key from bound entanglement",
        "paperAbstract": "  We characterize the set of shared quantum states which contain a\ncryptographically private key. This allows us to recast the theory of privacy\nas a paradigm closely related to that used in entanglement manipulation. It is\nshown that one can distill an arbitrarily secure key from bound entangled\nstates. There are also states which have less distillable private key than the\nentanglement cost of the state. In general the amount of distillable key is\nbounded from above by the relative entropy of entanglement. Relationships\nbetween distillability and distinguishability are found for a class of states\nwhich have Bell states correlated to separable hiding states. We also describe\na technique for finding states exhibiting irreversibility in entanglement\ndistillation.\n",
        "query": "spooky action distance quantum entanglement encryption",
        "docId": 2212768,
        "score": 0.6318578216105095,
        "userScore":2
    },
    {
        "title": "Characterizing entanglement using quantum discord over state extensions",
        "paperAbstract": "  We propose a framework to characterize entanglement with quantum discord,\nboth asymmetric and symmetric, over state extensions. In particular, we show\nthat the minimal Bures distance of discord over state extensions is equivalent\nto Bures distance of entanglement. This equivalence places quantum discord at a\nmore primitive position than entanglement conceptually in the sense that\nentanglement can be interpreted as an irreducible part of discord over all\nstate extensions. Based on this equivalence, we also offer an operational\nmeaning of Bures distance of entanglement by connecting it to quantum state\ndiscriminations. Moreover, for the relative entropy part, we prove that the\nentanglement measure introduced by Devi and Rajagopal [A. R. U. Devi and A. K.\nRajagopal, Phys. Rev. Lett. 100, 140502 (2008)] is actually equivalent to the\nrelative entropy of entanglement. We also provide several quantifications of\nentanglement based on discord measures.\n",
        "query": "spooky action distance quantum entanglement encryption",
        "docId": 1391713,
        "score": 0.6294798958945744,
        "userScore":2
    },
    {
        "title": "Entanglement measure and distance",
        "paperAbstract": "  Recently a new entanglemenet dilution scheme has been constructed by Lo and\nPopescu. This paper points out that this result has a deep implication that the\nentanglement measure for bipartite pure states is independent of the distance\nbetween entangled two systems.\n",
        "query": "spooky action distance quantum entanglement encryption",
        "docId": 2225640,
        "score": 0.628104543387634,
        "userScore":1
    },
    {
        "title": "Faster Shortest Path Algorithm for H-Minor Free Graphs with Negative\n  Edge Weights",
        "paperAbstract": "  Let $H$ be a fixed graph and let $G$ be an $H$-minor free $n$-vertex graph\nwith integer edge weights and no negative weight cycles reachable from a given\nvertex $s$. We present an algorithm that computes a shortest path tree in $G$\nrooted at $s$ in $\\tilde{O}(n^{4/3}\\log L)$ time, where $L$ is the absolute\nvalue of the smallest edge weight. The previous best bound was\n$\\tilde{O}(n^{\\sqrt{11.5}-2}\\log L) \u003d O(n^{1.392}\\log L)$. Our running time\nmatches an earlier bound for planar graphs by Henzinger et al.\n",
        "query": "shortest path negative graph weights",
        "docId": 205807,
        "score": 0.5961478897709167,
        "userScore":3
    },
    {
        "title": "Bellman-Ford is optimal for shortest hop-bounded paths",
        "paperAbstract": "  This paper is about the problem of finding a shortest $s$-$t$ path using at\nmost $h$ edges in edge-weighted graphs. The Bellman--Ford algorithm solves this\nproblem in $O(hm)$ time, where $m$ is the number of edges. We show that this\nrunning time is optimal, up to subpolynomial factors, under popular\nfine-grained complexity assumptions.\n  More specifically, we show that under the APSP Hypothesis the problem cannot\nbe solved faster already in undirected graphs with non-negative edge weights.\nThis lower bound holds even restricted to graphs of arbitrary density and for\narbitrary $h \\in O(\\sqrt{m})$. Moreover, under a stronger assumption, namely\nthe Min-Plus Convolution Hypothesis, we can eliminate the restriction $h \\in\nO(\\sqrt{m})$. In other words, the $O(hm)$ bound is tight for the entire space\nof parameters $h$, $m$, and $n$, where $n$ is the number of nodes.\n  Our lower bounds can be contrasted with the recent near-linear time algorithm\nfor the negative-weight Single-Source Shortest Paths problem, which is the\ntextbook application of the Bellman--Ford algorithm.\n",
        "query": "shortest path negative graph weights",
        "docId": 1746231,
        "score": 0.5841693660672433,
        "userScore":2
    },
    {
        "title": "A linear time algorithm for the next-to-shortest path problem on\n  undirected graphs with nonnegative edge lengths",
        "paperAbstract": "  For two vertices $s$ and $t$ in a graph $G\u003d(V,E)$, the next-to-shortest path\nis an $st$-path which length is minimum amongst all $st$-paths strictly longer\nthan the shortest path length. In this paper we show that, when the graph is\nundirected and all edge lengths are nonnegative, the problem can be solved in\nlinear time if the distances from $s$ and $t$ to all other vertices are given.\nThis result generalizes the previous work (DOI 10.1007/s00453-011-9601-7) to\nallowing zero-length edges.\n",
        "query": "shortest path negative graph weights",
        "docId": 330594,
        "score": 0.5648731389601789,
        "userScore":1
    },
    {
        "title": "Improving The Floyd-Warshall All Pairs Shortest Paths Algorithm",
        "paperAbstract": "  The Floyd-Warshall algorithm is the most popular algorithm for determining\nthe shortest paths between all pairs in a graph. It is very a simple and an\nelegant algorithm. However, if the graph does not contain any negative weighted\nedge, using Dijkstra\u0027s shortest path algorithm for every vertex as a source\nvertex to produce all pairs shortest paths of the graph works much better than\nthe Floyd-Warshall algorithm for sparse graphs. Also, for the graphs with\nnegative weighted edges, with no negative cycle, Johnson\u0027s algorithm still\nperforms significantly better than the Floyd-Warshall algorithm for sparse\ngraphs. Johnson\u0027s algorithm transforms the graph into a non-negative one by\nusing the Bellman-Ford algorithm, then, applies the Dijkstra\u0027s algorithm. Thus,\nin general the Floyd-Warshall algorithm becomes very inefficient especially for\nsparse graphs. In this paper, we show a simple improvement on the\nFloyd-Warshall algorithm that will increases its performance especially for the\nsparse graphs, so it can be used instead of more complicated alternatives.\n",
        "query": "shortest path negative graph weights",
        "docId": 1524693,
        "score": 0.5579265975581436,
        "userScore":2
    },
    {
        "title": "Faster Parametric Shortest Path and Minimum Balance Algorithms",
        "paperAbstract": "  The parametric shortest path problem is to find the shortest paths in graph\nwhere the edge costs are of the form w_ij+lambda where each w_ij is constant\nand lambda is a parameter that varies. The problem is to find shortest path\ntrees for every possible value of lambda.\n  The minimum-balance problem is to find a ``weighting\u0027\u0027 of the vertices so\nthat adjusting the edge costs by the vertex weights yields a graph in which,\nfor every cut, the minimum weight of any edge crossing the cut in one direction\nequals the minimum weight of any edge crossing the cut in the other direction.\n  The paper presents fast algorithms for both problems. The algorithms run in\nO(nm+n^2 log n) time. The paper also describes empirical studies of the\nalgorithms on random graphs, suggesting that the expected time for finding a\nminimum-mean cycle (an important special case of both problems) is O(n log(n) +\nm).\n",
        "query": "shortest path negative graph weights",
        "docId": 1974681,
        "score": 0.5555768499886824,
        "userScore":1
    },
    {
        "title": "Negative-Weight Shortest Paths and Unit Capacity Minimum Cost Flow in\n  $\\tilde{O}(m^{10/7} \\log W)$ Time",
        "paperAbstract": "  In this paper, we study a set of combinatorial optimization problems on\nweighted graphs: the shortest path problem with negative weights, the weighted\nperfect bipartite matching problem, the unit-capacity minimum-cost maximum flow\nproblem and the weighted perfect bipartite $b$-matching problem under the\nassumption that $\\Vert b\\Vert_1\u003dO(m)$. We show that each one of these four\nproblems can be solved in $\\tilde{O}(m^{10/7}\\log W)$ time, where $W$ is the\nabsolute maximum weight of an edge in the graph, which gives the first in over\n25 years polynomial improvement in their sparse-graph time complexity.\n  At a high level, our algorithms build on the interior-point method-based\nframework developed by Madry (FOCS 2013) for solving unit-capacity maximum flow\nproblem. We develop a refined way to analyze this framework, as well as provide\nnew variants of the underlying preconditioning and perturbation techniques.\nConsequently, we are able to extend the whole interior-point method-based\napproach to make it applicable in the weighted graph regime.\n",
        "query": "shortest path negative graph weights",
        "docId": 729832,
        "score": 0.5550776469575123,
        "userScore":3
    },
    {
        "title": "Algorithmic Applications of Baur-Strassen\u0027s Theorem: Shortest Cycles,\n  Diameter and Matchings",
        "paperAbstract": "  Consider a directed or an undirected graph with integral edge weights from\nthe set [-W, W], that does not contain negative weight cycles. In this paper,\nwe introduce a general framework for solving problems on such graphs using\nmatrix multiplication. The framework is based on the usage of Baur-Strassen\u0027s\ntheorem and of Strojohann\u0027s determinant algorithm. It allows us to give new and\nsimple solutions to the following problems:\n  * Finding Shortest Cycles -- We give a simple \\tilde{O}(Wn^{\\omega}) time\nalgorithm for finding shortest cycles in undirected and directed graphs. For\ndirected graphs (and undirected graphs with non-negative weights) this matches\nthe time bounds obtained in 2011 by Roditty and Vassilevska-Williams. On the\nother hand, no algorithm working in \\tilde{O}(Wn^{\\omega}) time was previously\nknown for undirected graphs with negative weights. Furthermore our algorithm\nfor a given directed or undirected graph detects whether it contains a negative\nweight cycle within the same running time.\n  * Computing Diameter and Radius -- We give a simple \\tilde{O}(Wn^{\\omega})\ntime algorithm for computing a diameter and radius of an undirected or directed\ngraphs. To the best of our knowledge no algorithm with this running time was\nknown for undirected graphs with negative weights.\n  * Finding Minimum Weight Perfect Matchings -- We present an\n\\tilde{O}(Wn^{\\omega}) time algorithm for finding minimum weight perfect\nmatchings in undirected graphs. This resolves an open problem posted by\nSankowski in 2006, who presented such an algorithm but only in the case of\nbipartite graphs.\n  In order to solve minimum weight perfect matching problem we develop a novel\ncombinatorial interpretation of the dual solution which sheds new light on this\nproblem. Such a combinatorial interpretation was not know previously, and is of\nindependent interest.\n",
        "query": "shortest path negative graph weights",
        "docId": 333878,
        "score": 0.5547778556500589,
        "userScore":1
    },
    {
        "title": "Shortest-Path-Preserving Rounding",
        "paperAbstract": "  Various applications of graphs, in particular applications related to finding\nshortest paths, naturally get inputs with real weights on the edges. However,\nfor algorithmic or visualization reasons, inputs with integer weights would\noften be preferable or even required. This raises the following question: given\nan undirected graph with non-negative real weights on the edges and an error\nthreshold $\\varepsilon$, how efficiently can we decide whether we can round all\nweights such that shortest paths are maintained, and the change of weight of\neach shortest path is less than $\\varepsilon$? So far, only for path-shaped\ngraphs a polynomial-time algorithm was known. In this paper we prove, by\nreduction from 3-SAT, that, in general, the problem is NP-hard. However, if the\ngraph is a tree with $n$ vertices, the problem can be solved in $O(n^2)$ time.\n",
        "query": "shortest path negative graph weights",
        "docId": 1127057,
        "score": 0.5490320700548477,
        "userScore":1
    },
    {
        "title": "Minimum weight spanning trees of weighted scale free networks",
        "paperAbstract": "  In this lecture we will consider the minimum weight spanning tree (MST)\nproblem, i.e., one of the simplest and most vital combinatorial optimization\nproblems. We will discuss a particular greedy algorithm that allows to compute\na MST for undirected weighted graphs, namely Kruskal\u0027s algorithm, and we will\nstudy the structure of MSTs obtained for weighted scale free random graphs.\nThis is meant to clarify whether the structure of MSTs is sensitive to\ncorrelations between edge weights and topology of the underlying scale free\ngraphs.\n",
        "query": "shortest path negative graph weights",
        "docId": 371657,
        "score": 0.5444372060992304,
        "userScore":1
    },
    {
        "title": "Optimal Embedding Into Star Metrics",
        "paperAbstract": "  We present an O(n^3 log^2 n)-time algorithm for the following problem: given\na finite metric space X, create a star-topology network with the points of X as\nits leaves, such that the distances in the star are at least as large as in X,\nwith minimum dilation. As part of our algorithm, we solve in the same time\nbound the parametric negative cycle detection problem: given a directed graph\nwith edge weights that are increasing linear functions of a parameter lambda,\nfind the smallest value of lambda such that the graph contains no\nnegative-weight cycles.\n",
        "query": "shortest path negative graph weights",
        "docId": 121879,
        "score": 0.5431328809063416,
        "userScore":0
    },
    {
        "title": "Controllable reset behavior in domain wall-magnetic tunnel junction\n  artificial neurons for task-adaptable computation",
        "paperAbstract": "  Neuromorphic computing with spintronic devices has been of interest due to\nthe limitations of CMOS-driven von Neumann computing. Domain wall-magnetic\ntunnel junction (DW-MTJ) devices have been shown to be able to intrinsically\ncapture biological neuron behavior. Edgy-relaxed behavior, where a frequently\nfiring neuron experiences a lower action potential threshold, may provide\nadditional artificial neuronal functionality when executing repeated tasks. In\nthis study, we demonstrate that this behavior can be implemented in DW-MTJ\nartificial neurons via three alternative mechanisms: shape anisotropy, magnetic\nfield, and current-driven soft reset. Using micromagnetics and analytical\ndevice modeling to classify the Optdigits handwritten digit dataset, we show\nthat edgy-relaxed behavior improves both classification accuracy and\nclassification rate for ordered datasets while sacrificing little to no\naccuracy for a randomized dataset. This work establishes methods by which\nartificial spintronic neurons can be flexibly adapted to datasets.\n",
        "query": "neural network hallucination prevention mitigation",
        "docId": 1406837,
        "score": 0.46407743168717275,
        "userScore":1
    },
    {
        "title": "Maximized Lateral Inhibition in Paired Magnetic Domain Wall Racetracks\n  for Neuromorphic Computing",
        "paperAbstract": "  Lateral inhibition is an important functionality in neuromorphic computing,\nmodeled after the biological neuron behavior that a firing neuron deactivates\nits neighbors belonging to the same layer and prevents them from firing. In\nmost neuromorphic hardware platforms lateral inhibition is implemented by\nexternal circuitry, thereby decreasing the energy efficiency and increasing the\narea overhead of such systems. Recently, the domain wall -- magnetic tunnel\njunction (DW-MTJ) artificial neuron is demonstrated in modeling to be\ninherently inhibitory. Without peripheral circuitry, lateral inhibition in\nDW-MTJ neurons results from magnetostatic interaction between neighboring\nneuron cells. However, the lateral inhibition mechanism in DW-MTJ neurons has\nnot been studied thoroughly, leading to weak inhibition only in very\nclosely-spaced devices. This work approaches these problems by modeling\ncurrent- and field- driven DW motion in a pair of adjacent DW-MTJ neurons. We\nmaximize the magnitude of lateral inhibition by tuning the magnetic interaction\nbetween the neurons. The results are explained by current-driven DW velocity\ncharacteristics in response to external magnetic field and quantified by an\nanalytical model. Finally, the dependence of lateral inhibition strength on\ndevice parameters is investigated. This provides a guideline for the\noptimization of lateral inhibition implementation in DW-MTJ neurons. With\nstrong lateral inhibition achieved, a path towards competitive learning\nalgorithms such as the winner-take-all are made possible on such neuromorphic\ndevices.\n",
        "query": "neural network hallucination prevention mitigation",
        "docId": 1216401,
        "score": 0.45632192253060566,
        "userScore":0
    },
    {
        "title": "Tactile Hallucinations on Artificial Skin Induced by Homeostasis in a\n  Deep Boltzmann Machine",
        "paperAbstract": "  Perceptual hallucinations are present in neurological and psychiatric\ndisorders and amputees. While the hallucinations can be drug-induced, it has\nbeen described that they can even be provoked in healthy subjects.\nUnderstanding their manifestation could thus unveil how the brain processes\nsensory information and might evidence the generative nature of perception. In\nthis work, we investigate the generation of tactile hallucinations on\nbiologically inspired, artificial skin. To model tactile hallucinations, we\napply homeostasis, a change in the excitability of neurons during sensory\ndeprivation, in a Deep Boltzmann Machine (DBM). We find that homeostasis\nprompts hallucinations of previously learned patterns on the artificial skin in\nthe absence of sensory input. Moreover, we show that homeostasis is capable of\ninducing the formation of meaningful latent representations in a DBM and that\nit significantly increases the quality of the reconstruction of these latent\nstates. Through this, our work provides a possible explanation for the nature\nof tactile hallucinations and highlights homeostatic processes as a potential\nunderlying mechanism.\n",
        "query": "neural network hallucination prevention mitigation",
        "docId": 1142778,
        "score": 0.45164666827710676,
        "userScore":0
    },
    {
        "title": "Neural Mesh: Introducing a Notion of Space and Conservation of Energy to\n  Neural Networks",
        "paperAbstract": "  Neural networks are based on a simplified model of the brain. In this\nproject, we wanted to relax the simplifying assumptions of a traditional neural\nnetwork by making a model that more closely emulates the low level interactions\nof neurons. Like in an RNN, our model has a state that persists between time\nsteps, so that the energies of neurons persist. However, unlike an RNN, our\nstate consists of a 2 dimensional matrix, rather than a 1 dimensional vector,\nthereby introducing a concept of distance to other neurons within the state. In\nour model, neurons can only fire to adjacent neurons, as in the brain. Like in\nthe brain, we only allow neurons to fire in a time step if they contain enough\nenergy, or excitement. We also enforce a notion of conservation of energy, so\nthat a neuron cannot excite its neighbors more than the excitement it already\ncontained at that time step. Taken together, these two features allow signals\nin the form of activations to flow around in our network over time, making our\nneural mesh more closely model signals traveling through the brain the brain.\nAlthough our main goal is to design an architecture to more closely emulate the\nbrain in the hope of having a correct internal representation of information by\nthe time we know how to properly train a general intelligence, we did benchmark\nour neural mash on a specific task. We found that by increasing the runtime of\nthe mesh, we were able to increase its accuracy without increasing the number\nof parameters.\n",
        "query": "neural network hallucination prevention mitigation",
        "docId": 1008327,
        "score": 0.440833507112238,
        "userScore":1
    },
    {
        "title": "A wake-sleep algorithm for recurrent, spiking neural networks",
        "paperAbstract": "  We investigate a recently proposed model for cortical computation which\nperforms relational inference. It consists of several interconnected,\nstructurally equivalent populations of leaky integrate-and-fire (LIF) neurons,\nwhich are trained in a self-organized fashion with spike-timing dependent\nplasticity (STDP). Despite its robust learning dynamics, the model is\nsusceptible to a problem typical for recurrent networks which use a correlation\nbased (Hebbian) learning rule: if trained with high learning rates, the\nrecurrent connections can cause strong feedback loops in the network dynamics,\nwhich lead to the emergence of attractor states. This causes a strong reduction\nin the number of representable patterns and a decay in the inference ability of\nthe network. As a solution, we introduce a conceptually very simple\n\"wake-sleep\" algorithm: during the wake phase, training is executed normally,\nwhile during the sleep phase, the network \"dreams\" samples from its generative\nmodel, which are induced by random input. This process allows us to activate\nthe attractor states in the network, which can then be unlearned effectively by\nan anti-Hebbian mechanism. The algorithm allows us to increase learning rates\nup to a factor of ten while avoiding clustering, which allows the network to\nlearn several times faster. Also for low learning rates, where clustering is\nnot an issue, it improves convergence speed and reduces the final inference\nerror.\n",
        "query": "neural network hallucination prevention mitigation",
        "docId": 829794,
        "score": 0.43757945272042775,
        "userScore":1
    },
    {
        "title": "Biologically inspired sleep algorithm for artificial neural networks",
        "paperAbstract": "  Sleep plays an important role in incremental learning and consolidation of\nmemories in biological systems. Motivated by the processes that are known to be\ninvolved in sleep generation in biological networks, we developed an algorithm\nthat implements a sleep-like phase in artificial neural networks (ANNs). After\ninitial training phase, we convert the ANN to a spiking neural network (SNN)\nand simulate an offline sleep-like phase using spike-timing dependent\nplasticity rules to modify synaptic weights. The SNN is then converted back to\nthe ANN and evaluated or trained on new inputs. We demonstrate several\nperformance improvements after applying this processing to ANNs trained on\nMNIST, CUB200 and a motivating toy dataset. First, in an incremental learning\nframework, sleep is able to recover older tasks that were otherwise forgotten\nin the ANN without sleep phase due to catastrophic forgetting. Second, sleep\nresults in forward transfer learning of unseen tasks. Finally, sleep improves\ngeneralization ability of the ANNs to classify images with various types of\nnoise. We provide a theoretical basis for the beneficial role of the\nbrain-inspired sleep-like phase for the ANNs and present an algorithmic way for\nfuture implementations of the various features of sleep in deep learning ANNs.\nOverall, these results suggest that biological sleep can help mitigate a number\nof problems ANNs suffer from, such as poor generalization and catastrophic\nforgetting for incremental learning.\n",
        "query": "neural network hallucination prevention mitigation",
        "docId": 1160410,
        "score": 0.4367188830676412,
        "userScore":0
    },
    {
        "title": "White Noise Analysis of Neural Networks",
        "paperAbstract": "  A white noise analysis of modern deep neural networks is presented to unveil\ntheir biases at the whole network level or the single neuron level. Our\nanalysis is based on two popular and related methods in psychophysics and\nneurophysiology namely classification images and spike triggered analysis.\nThese methods have been widely used to understand the underlying mechanisms of\nsensory systems in humans and monkeys. We leverage them to investigate the\ninherent biases of deep neural networks and to obtain a first-order\napproximation of their functionality. We emphasize on CNNs since they are\ncurrently the state of the art methods in computer vision and are a decent\nmodel of human visual processing. In addition, we study multi-layer\nperceptrons, logistic regression, and recurrent neural networks. Experiments\nover four classic datasets, MNIST, Fashion-MNIST, CIFAR-10, and ImageNet, show\nthat the computed bias maps resemble the target classes and when used for\nclassification lead to an over twofold performance than the chance level.\nFurther, we show that classification images can be used to attack a black-box\nclassifier and to detect adversarial patch attacks. Finally, we utilize spike\ntriggered averaging to derive the filters of CNNs and explore how the behavior\nof a network changes when neurons in different layers are modulated. Our effort\nillustrates a successful example of borrowing from neurosciences to study ANNs\nand highlights the importance of cross-fertilization and synergy across machine\nlearning, deep learning, and computational neuroscience.\n",
        "query": "neural network hallucination prevention mitigation",
        "docId": 1224002,
        "score": 0.43092208140011934,
        "userScore":1
    },
    {
        "title": "Adversarial Defense via Neural Oscillation inspired Gradient Masking",
        "paperAbstract": "  Spiking neural networks (SNNs) attract great attention due to their low power\nconsumption, low latency, and biological plausibility. As they are widely\ndeployed in neuromorphic devices for low-power brain-inspired computing,\nsecurity issues become increasingly important. However, compared to deep neural\nnetworks (DNNs), SNNs currently lack specifically designed defense methods\nagainst adversarial attacks. Inspired by neural membrane potential oscillation,\nwe propose a novel neural model that incorporates the bio-inspired oscillation\nmechanism to enhance the security of SNNs. Our experiments show that SNNs with\nneural oscillation neurons have better resistance to adversarial attacks than\nordinary SNNs with LIF neurons on kinds of architectures and datasets.\nFurthermore, we propose a defense method that changes model\u0027s gradients by\nreplacing the form of oscillation, which hides the original training gradients\nand confuses the attacker into using gradients of \u0027fake\u0027 neurons to generate\ninvalid adversarial samples. Our experiments suggest that the proposed defense\nmethod can effectively resist both single-step and iterative attacks with\ncomparable defense effectiveness and much less computational costs than\nadversarial training methods on DNNs. To the best of our knowledge, this is the\nfirst work that establishes adversarial defense through masking surrogate\ngradients on SNNs.\n",
        "query": "neural network hallucination prevention mitigation",
        "docId": 1741129,
        "score": 0.4307483434042112,
        "userScore":0
    },
    {
        "title": "Plasticity-Enhanced Domain-Wall MTJ Neural Networks for Energy-Efficient\n  Online Learning",
        "paperAbstract": "  Machine learning implements backpropagation via abundant training samples. We\ndemonstrate a multi-stage learning system realized by a promising non-volatile\nmemory device, the domain-wall magnetic tunnel junction (DW-MTJ). The system\nconsists of unsupervised (clustering) as well as supervised sub-systems, and\ngeneralizes quickly (with few samples). We demonstrate interactions between\nphysical properties of this device and optimal implementation of\nneuroscience-inspired plasticity learning rules, and highlight performance on a\nsuite of tasks. Our energy analysis confirms the value of the approach, as the\nlearning budget stays below 20 $\\mu J$ even for large tasks used typically in\nmachine learning.\n",
        "query": "neural network hallucination prevention mitigation",
        "docId": 1252712,
        "score": 0.42194996240765903,
        "userScore":1
    },
    {
        "title": "Controlled hierarchical filtering: Model of neocortical sensory\n  processing",
        "paperAbstract": "  A model of sensory information processing is presented. The model assumes\nthat learning of internal (hidden) generative models, which can predict the\nfuture and evaluate the precision of that prediction, is of central importance\nfor information extraction. Furthermore, the model makes a bridge to\ngoal-oriented systems and builds upon the structural similarity between the\narchitecture of a robust controller and that of the hippocampal entorhinal\nloop. This generative control architecture is mapped to the neocortex and to\nthe hippocampal entorhinal loop. Implicit memory phenomena; priming and\nprototype learning are emerging features of the model. Mathematical theorems\nensure stability and attractive learning properties of the architecture.\nConnections to reinforcement learning are also established: both the control\nnetwork, and the network with a hidden model converge to (near) optimal policy\nunder suitable conditions. Falsifying predictions, including the role of the\nfeedback connections between neocortical areas are made.\n",
        "query": "neural network hallucination prevention mitigation",
        "docId": 1975518,
        "score": 0.4207426807801582,
        "userScore":0
    },
    {
        "title": "Algorithm engineering for a quantum annealing platform",
        "paperAbstract": "  Recent advances bring within reach the viability of solving combinatorial\nproblems using a quantum annealing algorithm implemented on a purpose-built\nplatform that exploits quantum properties. However, the question of how to tune\nthe algorithm for most effective use in this framework is not well understood.\nIn this paper we describe some operational parameters that drive performance,\ndiscuss approaches for mitigating sources of error, and present experimental\nresults from a D-Wave Two quantum annealing processor.\n",
        "query": "performance analysis quantum annealing",
        "docId": 563729,
        "score": 0.7977596717087874,
        "userScore":2
    },
    {
        "title": "Quantum Annealing: from Viewpoints of Statistical Physics, Condensed\n  Matter Physics, and Computational Physics",
        "paperAbstract": "  In this paper, we review some features of quantum annealing and related\ntopics from viewpoints of statistical physics, condensed matter physics, and\ncomputational physics. We can obtain a better solution of optimization problems\nin many cases by using the quantum annealing. Actually the efficiency of the\nquantum annealing has been demonstrated for problems based on statistical\nphysics. Then the quantum annealing has been expected to be an efficient and\ngeneric solver of optimization problems. Since many implementation methods of\nthe quantum annealing have been developed and will be proposed in the future,\ntheoretical frameworks of wide area of science and experimental technologies\nwill be evolved through studies of the quantum annealing.\n",
        "query": "performance analysis quantum annealing",
        "docId": 335169,
        "score": 0.7830843301976245,
        "userScore":2
    },
    {
        "title": "An Overview of Approaches to Modernize Quantum Annealing Using Local\n  Searches",
        "paperAbstract": "  I describe how real quantum annealers may be used to perform local (in state\nspace) searches around specified states, rather than the global searches\ntraditionally implemented in the quantum annealing algorithm. The quantum\nannealing algorithm is an analogue of simulated annealing, a classical\nnumerical technique which is now obsolete. Hence, I explore strategies to use\nan annealer in a way which takes advantage of modern classical optimization\nalgorithms, and additionally should be less sensitive to problem\nmis-specification then the traditional quantum annealing algorithm.\n",
        "query": "performance analysis quantum annealing",
        "docId": 744707,
        "score": 0.7690361909056307,
        "userScore":2
    },
    {
        "title": "Quantum Annealing and Computation: A Brief Documentary Note",
        "paperAbstract": "  Major breakthrough in quantum computation has recently been achieved using\nquantum annealing to develop analog quantum computers instead of gate based\ncomputers. After a short introduction to quantum computation, we retrace very\nbriefly the history of these developments and discuss the Indian researches in\nthis connection and provide some interesting documents (in the Figs.) obtained\nfrom a chosen set of high impact papers (and also some recent news etc. blogs\nappearing in the Internet). This note is also designed to supplement an earlier\nnote by Bose (Science and Culture, 79, pp. 337-378, 2013).\n",
        "query": "performance analysis quantum annealing",
        "docId": 466342,
        "score": 0.7672291448736743,
        "userScore":1
    },
    {
        "title": "Applications of Quantum Annealing in Statistics",
        "paperAbstract": "  Quantum computation offers exciting new possibilities for statistics. This\npaper explores the use of the D-Wave machine, a specialized type of quantum\ncomputer, which performs quantum annealing. A general description of quantum\nannealing through the use of the D-Wave is given, along with technical issues\nto be encountered. Quantum annealing is used to perform maximum likelihood\nestimation, generate an experimental design, and perform matrix inversion.\nThough the results show that quantum computing is still at an early stage which\nis not yet superior to classical computation, there is promise for quantum\ncomputation in the future.\n",
        "query": "performance analysis quantum annealing",
        "docId": 1111859,
        "score": 0.7670977667084742,
        "userScore":2
    },
    {
        "title": "On the Emerging Potential of Quantum Annealing Hardware for\n  Combinatorial Optimization",
        "paperAbstract": "  Over the past decade, the usefulness of quantum annealing hardware for\ncombinatorial optimization has been the subject of much debate. Thus far,\nexperimental benchmarking studies have indicated that quantum annealing\nhardware does not provide an irrefutable performance gain over state-of-the-art\noptimization methods. However, as this hardware continues to evolve, each new\niteration brings improved performance and warrants further benchmarking. To\nthat end, this work conducts an optimization performance assessment of D-Wave\nSystems\u0027 most recent Advantage Performance Update computer, which can natively\nsolve sparse unconstrained quadratic optimization problems with over 5,000\nbinary decision variables and 40,000 quadratic terms. We demonstrate that\nclasses of contrived problems exist where this quantum annealer can provide run\ntime benefits over a collection of established classical solution methods that\nrepresent the current state-of-the-art for benchmarking quantum annealing\nhardware. Although this work does not present strong evidence of an irrefutable\nperformance benefit for this emerging optimization technology, it does exhibit\nencouraging progress, signaling the potential impacts on practical optimization\ntasks in the future.\n",
        "query": "performance analysis quantum annealing",
        "docId": 1725646,
        "score": 0.7625071956002625,
        "userScore":2
    },
    {
        "title": "Quantum Annealing for Industry Applications: Introduction and Review",
        "paperAbstract": "  Quantum annealing is a heuristic quantum optimization algorithm that can be\nused to solve combinatorial optimization problems. In recent years, advances in\nquantum technologies have enabled the development of small- and\nintermediate-scale quantum processors that implement the quantum annealing\nalgorithm for programmable use. Specifically, quantum annealing processors\nproduced by D-Wave Systems have been studied and tested extensively in both\nresearch and industrial settings across different disciplines. In this paper we\nprovide a literature review of the theoretical motivations for quantum\nannealing as a heuristic quantum optimization algorithm, the software and\nhardware that is required to use such quantum processors, and the\nstate-of-the-art applications and proofs-of-concepts that have been\ndemonstrated using them. The goal of our review is to provide a centralized and\ncondensed source regarding applications of quantum annealing technology. We\nidentify the advantages, limitations, and potential of quantum annealing for\nboth researchers and practitioners from various fields.\n",
        "query": "performance analysis quantum annealing",
        "docId": 1577267,
        "score": 0.7592660022834661,
        "userScore":2
    },
    {
        "title": "Perspectives of quantum annealing: Methods and implementations",
        "paperAbstract": "  Quantum annealing is a computing paradigm that has the ambitious goal of\nefficiently solving large-scale combinatorial optimization problems of\npractical importance. However, many challenges have yet to be overcome before\nthis goal can be reached. This perspectives article first gives a brief\nintroduction to the concept of quantum annealing, and then highlights new\npathways that may clear the way towards feasible and large scale quantum\nannealing. Moreover, since this field of research is to a strong degree driven\nby a synergy between experiment and theory, we discuss both in this work. An\nimportant focus in this article is on future perspectives, which complements\nother review articles, and which we hope will motivate further research.\n",
        "query": "performance analysis quantum annealing",
        "docId": 1098945,
        "score": 0.7585429287455678,
        "userScore":3
    },
    {
        "title": "Benchmarking Quantum Annealing Controls with Portfolio Optimization",
        "paperAbstract": "  Quantum annealing offers a novel approach to finding the optimal solutions\nfor a variety of computational problems, where the quantum annealing controls\ninfluence the observed performance and error mechanisms by tuning the\nunderlying quantum dynamics. However, the influence of the available controls\nis often poorly understood, and methods for evaluating the effects of these\ncontrols are necessary to tune quantum computational performance. Here we use\nportfolio optimization as a case study by which to benchmark quantum annealing\ncontrols and their relative effects on computational accuracy. We compare\nempirical results from the D-Wave 2000Q quantum annealer to the computational\nground truth for a variety of portfolio optimization instances. We evaluate\nboth forward and reverse annealing methods and we identify control variations\nthat yield optimal performance in terms of probability of success and\nprobability of chain breaks.\n",
        "query": "performance analysis quantum annealing",
        "docId": 1314519,
        "score": 0.7573382479465476,
        "userScore":2
    },
    {
        "title": "A cross-disciplinary introduction to quantum annealing-based algorithms",
        "paperAbstract": "  A central goal in quantum computing is the development of quantum hardware\nand quantum algorithms in order to analyse challenging scientific and\nengineering problems. Research in quantum computation involves contributions\nfrom both physics and computer science, hence this article presents a concise\nintroduction to basic concepts from both fields that are used in\nannealing-based quantum computation, an alternative to the more familiar\nquantum gate model.\n  We introduce some concepts from computer science required to define difficult\ncomputational problems and to realise the potential relevance of quantum\nalgorithms to find novel solutions to those problems. We introduce the\nstructure of quantum annealing-based algorithms as well as two examples of this\nkind of algorithms for solving instances of the max-SAT and Minimum Multicut\nproblems. An overview of the quantum annealing systems manufactured by D-Wave\nSystems is also presented.\n",
        "query": "performance analysis quantum annealing",
        "docId": 953505,
        "score": 0.7519947855766258,
        "userScore":1
    },
    {
        "title": "Enhanced Circuit Densities in Epitaxially Defined FinFETs (EDFinFETs)\n  over FinFETs",
        "paperAbstract": "  FinFET technology is prone to suffer from Line Edge Roughness (LER) based VT\nvariation with scaling. To address this, we proposed an Epitaxially Defined\n(ED) FinFET (EDFinFET) as an alternate to FinFET architecture for 10 nm node\nand beyond. We showed by statistical simulations that EDFinFET reduces LER\nbased VT variability by 90% and overall variability by 59%. However, EDFinFET\nconsists of wider fins as the fin widths are not constrained by electrostatics\nand variability (cf. FinFETs have fin width ~ LG/3 where LG is gate-length).\nThis indicates that EDFinFET based circuits may be less dense. In this study we\nshow that wide fins enable taller fin heights. The ability to engineer multiple\nSTI levels on tall fins enables different transistor widths (i.e. various W/Ls\ne.g. 1-10) in a single fin. This capability ensures that even though individual\nEDFinFET devices have ~2x larger footprints than FinFETs, EDFinFET may produce\nequal or higher circuit density for basic building blocks like inverters or\nNAND gates for W/Ls of 2 and higher.\n",
        "query": "gate transistors vs FinFET",
        "docId": 723621,
        "score": 0.5982748960164983,
        "userScore":3
    },
    {
        "title": "HFinFET: A Scalable, High Performance, Low Leakage Hybrid N-Channel FET",
        "paperAbstract": "  In this letter we propose the design and simulation study of a novel\ntransistor, called HFinFET, which is a hybrid of a HEMT and a FinFET, to obtain\nexcellent performance and good off state control. Followed by the description\nof the design, 3D device simulation has been performed to predict the\ncharacteristics of the device. The device has been benchmarked against\npublished state of the art HEMT as well as planar and non-planar Si NMOSFET\ndata of comparable gate length using standard benchmarking techniques.\n",
        "query": "gate transistors vs FinFET",
        "docId": 181098,
        "score": 0.5441123382947808,
        "userScore":2
    },
    {
        "title": "Analytical Modelling of Ferroelectricity Instigated Enhanced\n  Electrostatic Control in Short-Channel FinFETs",
        "paperAbstract": "  This study simulated negative-capacitance double gate FinFETs with channel\nlengths ranging from 25nm to 100nm using TCAD. The results show that negative\ncapacitance significantly reduces subthreshold swing as well as drain induced\nbarrier lowering effects. The improvement is found to be significantly more\nprominent for short channel devices than long ones, which demonstrates the\ntremendous advantage of negative capacitance gate stack for scaled MOSFETs. A\ncompact analytical formulation is developed to quantify sub-threshold swing\nimprovement for short channel devices.\n",
        "query": "gate transistors vs FinFET",
        "docId": 1324682,
        "score": 0.5080263734974562,
        "userScore":2
    },
    {
        "title": "A Dynamically Configurable Silicon Nanowire Field Effect Transistor\n  based on Electrically Doped Source/Drain",
        "paperAbstract": "  In this article, we present a configurable field-effect transistor (FET),\nwhere not only polarity (n- and p-type), but the conduction mechanism of a FET\ncan also be configured dynamically. As a result, we can have both types of\ndevices, high-performance MOSFET and low-power TFET, for computational and\npower efficient system on chip (SoC) products. The calibrated 3D-TCAD\nsimulation results validate characteristics and functionalities of the\nconfigurable FET, and showed good consistency with the static conventional\nMOSFET (or TFET).\n",
        "query": "gate transistors vs FinFET",
        "docId": 582953,
        "score": 0.5026660095206523,
        "userScore":3
    },
    {
        "title": "Circuit-aware Device Modeling of Energy-efficient Monolayer WS$_2$\n  Trench-FinFETs",
        "paperAbstract": "  The continuous scaling of semiconductor technology has pushed the footprint\nof logic devices below 50 nm. Currently, logic standard cells with one single\nfin are being investigated to increase the integration density, although such\noptions could severely limit the performance of individual devices. In this\nletter, we present a novel Trench (T-) FinFET device, composed of a monolayer\ntwo-dimensional (2D) channel material. The device characteristics of a\nmonolayer WS$_2$-based T-FinFET are studied by combining the first-principles\ncalculations and quantum transport (QT) simulations. These results serve as\ninputs to a predictive analytical model. The latter allows to benchmark the\nT-FinFET with strained (s)-Si FinFETs in both quasi-ballistic and diffusive\ntransport regimes. The circuit-level evaluation highlights that WS$_2$\nT-FinFETs exhibit a competitive energy-delay performance compared to s-Si\nFinFET and WS$_2$ double-gate transistors, assuming the same mobility and\ncontact resistivity at small footprints.\n",
        "query": "gate transistors vs FinFET",
        "docId": 1455309,
        "score": 0.5024853976183792,
        "userScore":1
    },
    {
        "title": "Metal-Gated Junctionless Nanowire Transistors",
        "paperAbstract": "  Junctionless Nanowire Field-Effect Transistors (JNFETs), where the channel\nregion is uniformly doped without the need for source-channel and drain-channel\njunctions or lateral doping abruptness, are considered an attractive\nalternative to conventional CMOS FETs. Previous theoretical and experimental\nworks [1][2] on JNFETs have considered polysilicon gates and silicon-dioxide\ndielectric. However, with further scaling, JNFETs will suffer from deleterious\neffects of doped polysilicon such as high resistance, additional capacitance\ndue to gate-oxide interface depletion, and incompatibility with high-k\ndielectrics[3][4]. In this paper, novel metal- gated high-k JNFETs are\ninvestigated through detailed process and device simulations. These MJNFETs are\nalso ideally suited for new types of nano-architectures such as N3ASICs [5]\nwhich utilize regular nanowire arrays with limited customization. In such nano-\nsystems, the simplified device geometry in conjunction with a single-type FET\ncircuit style [6] would imply that logic arrays could be patterned out of\npre-doped SOI wafers without the need for any additional ion implantation.\n",
        "query": "gate transistors vs FinFET",
        "docId": 513349,
        "score": 0.49464863847736495,
        "userScore":1
    },
    {
        "title": "Compact spin qubits using the common gate structure of fin field-effect\n  transistors",
        "paperAbstract": "  The sizes of commercial transistors are of nanometer order, and there have\nalready been many proposals of spin qubits using conventional complementary\nmetal oxide semiconductor (CMOS) transistors. However, the previously proposed\nspin qubits require many wires to control a small number of qubits. This causes\na significant \u0027jungle of wires\u0027 problem when the qubits are integrated into a\nchip. Herein, to reduce the complicated wiring, we theoretically consider spin\nqubits embedded into fin field-effect transistor (FinFET) devices such that the\nspin qubits share the common gate electrode of the FinFET. The interactions\nbetween qubits occur via the Ruderman Kittel Kasuya Yosida (RKKY) interaction\nvia the channel of the FinFET. The compensation for the compact implementation\nrequires high-density current lines in a small space. The possibility of a\nquantum annealing machine is discussed in addition to the quantum computers of\nthe current proposals.\n",
        "query": "gate transistors vs FinFET",
        "docId": 1346123,
        "score": 0.4891901144966866,
        "userScore":1
    },
    {
        "title": "Negative Capacitance Enables FinFET Scaling Beyond 3nm Node",
        "paperAbstract": "  A comprehensive study of the scaling of negative capacitance FinFET\n(NC-FinFET) is conducted with TCAD. We show that the NC-FinFET can be scaled to\n\"2.1nm node\" and almost \"1.5nm node\" that comes two nodes after the industry\n\"3nm node,\" which has 16nm Lg and is the last FinFET node according to the\nInternational Roadmap for Devices and Systems (IRDS). In addition, for the\nintervening nodes, NC-FinFET can meet IRDS Ion and Ioff target at\ntarget-beating VDD. The benefits of negative capacitance (NC) include improved\nsubthreshold slope (SS), drain-induced barrier lowering (DIBL), Vt roll-off,\ntransconductance over Id (Gm/Id), output conductance over Id (Gd/Id), and lower\nVDD. Further scaling may be achieved by improving capacitance matching between\nferroelectric (FE) and dielectric (DE).\n",
        "query": "gate transistors vs FinFET",
        "docId": 1325962,
        "score": 0.48892765131090954,
        "userScore":2
    },
    {
        "title": "Performance Considerations of Thin Ferroelectrics (~10 nm HfO2, ~20 nm\n  PZT) FDSOI NCFETs for Digital Circuits at Reduced Power Consumption",
        "paperAbstract": "  The paper presents simulation study of thin ferroelectrics (Si doped HfO2,\nPZT) PGP FDSOI NCFETs at circuit level for high performance, low VDD low-power\ndigital circuits. The baseline PGP FDSOI MOSFET has 20 nm metal gate length\nwith supply voltage varying from 0.5 V to 0.9 V. The circuits studied were\n3-stage CMOS ring oscillator, NAND-2 and NOR-2 gates at a frequency of 20 GHz.\nThe paper shows that HfO2 FDSOI NCFET based NAND-2 gates can provide\nsignificant reduction in average power consumption, which was ~66% that of\nbaseline FDSOI MOSFET based NAND-2 gates for comparable performance. For the\nsame performance, the average power consumption for PZT FDSOI NCFET based\nNAND-2 gate was ~86% that of baseline FDSOI MOSFET based NAND-2 gate. The\npower-delay product of HfO2 FDSOI NCFET based gates was found to be ~24% lower\nthan baseline FDSOI MOSFET based gates and that of PZT FDSOI NCFET based gates\nwas found to be ~21% less than that of baseline FDSOI MOSFET based gates. The\nperformance of HfO2 FDSOI NCFET based gates with increased fan-in and fan-out\nwas also found to be superior to PZT FDSOI NCFET based gates and baseline FDSOI\nMOSFET based gates.\n",
        "query": "gate transistors vs FinFET",
        "docId": 1137217,
        "score": 0.4875685866979609,
        "userScore":1
    },
    {
        "title": "On the Performance of Dual-Gate Reconfigurable Nanowire Transistors",
        "paperAbstract": "  We investigate the operation of dual-gate reconfigurable field-effect\ntransistor (RFET) in the programgate at drain (PGAD) and program-gate at source\n(PGAS) configurations. To this end, dual-gate silicon nanowire (SiNW) FETs are\nfabricated based on anisotropic wet chemical silicon etching and nickel\nsilicidation yielding silicide-SiNW Schottky junctions at source and drain.\nWhereas in PGAD-configuration ambipolar operation is suppressed, switching is\ndeteriorated due to the injection through a Schottky-barrier. Operating the\nRFET in PGAS configuration yields a switching behavior close to a conventional\nMOSFET. This, howewer, needs to be traded off against strongly non-linear\noutput characteristics for small bias.\n",
        "query": "gate transistors vs FinFET",
        "docId": 1430241,
        "score": 0.48321348821792043,
        "userScore":1
    },
    {
        "title": "The number of non-zero coefficients of modular forms (mod p)",
        "paperAbstract": "  We give an asymptotic formula for the number of non-zero coefficients of\nmodular forms (mod p).\n",
        "query": "number theory modular form functions noncongruence congruence",
        "docId": 648751,
        "score": 0.7225200446957967,
        "userScore":0
    },
    {
        "title": "Experimental finding of modular forms for noncongruence subgroups",
        "paperAbstract": "  In this paper we will use experimental and computational methods to find\nmodular forms for non-congruence subgroups, and the modular forms for\ncongruence subgroups that they are associated with via the\nAtkin--Swinnerton-Dyer correspondence. We also prove a generalization of a\ncriterion due to Ligozat for an eta-quotient to be a modular function.\n",
        "query": "number theory modular form functions noncongruence congruence",
        "docId": 148803,
        "score": 0.7224846988964924,
        "userScore":2
    },
    {
        "title": "Congruences Among Power Series Coefficients of Modular Forms",
        "paperAbstract": "  Many authors have investigated the congruence relations amongst the\ncoefficients of power series expansions of modular forms $f$ in modular\nfunctions $t$. In a recent paper, R. Osburn and B. Sahu examine several power\nseries expansions and prove that the coefficients exhibit congruence relations\nsimilar to the congruences satisfied by the Ap\\\u0027ery numbers associated with the\nirrationality of $\\zeta(3)$. We show that many of the examples of Osburn and\nSahu are members of infinite families.\n",
        "query": "number theory modular form functions noncongruence congruence",
        "docId": 461328,
        "score": 0.7147760473161497,
        "userScore":2
    },
    {
        "title": "Congruences via modular forms",
        "paperAbstract": "  We prove two congruences for the coefficients of power series expansions in t\nof modular forms where t is a modular function. As a result, we settle two\nrecent conjectures of Chan, Cooper and Sica. Additionally, we provide a table\nof congruences for numbers which appear in similar power series expansions and\nin the study of integral solutions of Apery-like differential equations.\n",
        "query": "number theory modular form functions noncongruence congruence",
        "docId": 159924,
        "score": 0.7147207172687455,
        "userScore":2
    },
    {
        "title": "Incongruences for modular forms and applications to partition functions",
        "paperAbstract": "  The study of arithmetic properties of coefficients of modular forms $f(\\tau)\n\u003d \\sum a(n)q^n$ has a rich history, including deep results regarding\ncongruences in arithmetic progressions. Recently, work of C.-S. Radu, S.\nAhlgren, B. Kim, N. Andersen, and S. L\\\"{o}brich have employed the\n$q$-expansion principle of P. Deligne and M. Rapoport in order to determine\nmore about where these congruences can occur. Here, we extend the method to\ngive additional results for a large class of modular forms. We also give\nanalogous results for generalized Frobenius partitions and the two mock theta\nfunctions $f(q)$ and $\\omega(q).$\n",
        "query": "number theory modular form functions noncongruence congruence",
        "docId": 1190970,
        "score": 0.6989156824838574,
        "userScore":3
    },
    {
        "title": "Congruences for Andrews\u0027 spt-function modulo 32760 and extension of\n  Atkin\u0027s Hecke-type partition congruences",
        "paperAbstract": "  New congruences are found for Andrews\u0027 smallest parts partition function\nspt(n). The generating function for spt(n) is related to the holomorphic part\nalpha(24z) of a certain weak Maass form M(z) of weight 3/2. We show that a\nnormalized form of the generating function for spt(n) is an eigenform modulo 72\nfor the Hecke operators T(p^2) for primes p \u003e 3, and an eigenform modulo t for\nt \u003d 5, 7 or 13 provided that (t, 6p) \u003d 1. The result for the modulus 3 was\nobserved earlier by the author and considered by Ono and Folsom. Similar\ncongruences for higher powers of t (namely 5^6, 7^4 and 13^2) occur for the\ncoefficients of the function alpha(z). Analogous results for the partition\nfunction were found by Atkin in 1966. Our results depend on the recent result\nof Ono that M[p](z/24) is a weakly holomorphic modular form of weight 3/2 for\nthe full modular group where\n  M[p](z) \u003d M(z)|T(p^2) - chi(p)(1 + p)M(z).\n",
        "query": "number theory modular form functions noncongruence congruence",
        "docId": 224649,
        "score": 0.6893318385053746,
        "userScore":2
    },
    {
        "title": "Congruences for modular forms and generalized Frobenius partitions",
        "paperAbstract": "  The partition function is known to exhibit beautiful congruences that are\noften proved using the theory of modular forms. In this paper, we study the\nextent to which these congruence results apply to the generalized Frobenius\npartitions defined by Andrews. In particular, we prove that there are\ninfinitely many congruences for $c\\phi_k(n)$ modulo $\\ell,$ where\n$\\gcd(\\ell,6k)\u003d1,$ and we also prove results on the parity of $c\\phi_k(n).$\nAlong the way, we prove results regarding the parity of coefficients of weakly\nholomorphic modular forms which generalize work of Ono.\n",
        "query": "number theory modular form functions noncongruence congruence",
        "docId": 1020682,
        "score": 0.6838962626893306,
        "userScore":2
    },
    {
        "title": "Divisibility Arising From Addition: The Application of Modular Functions\n  to Infinite Partition Congruence Families",
        "paperAbstract": "  The theory of partition congruences has been a fascinating and difficult\nsubject for over a century now. In attempting to prove a given congruence\nfamily, multiple possible complications include the genus of the underlying\nmodular curve, representation difficulties of the associated sequences of\nmodular functions, and difficulties regarding the piecewise $\\ell$-adic\nconvergence of elements of the associated space of modular functions. However,\nour knowledge of the subject has developed substantially and continues to\ndevelop. In this very brief survey, we will discuss the utility of modular\nfunctions in proving partition congruences, both theoretical and computational,\nand many of the problems in the subject that are yet to be overcome.\n",
        "query": "number theory modular form functions noncongruence congruence",
        "docId": 1711611,
        "score": 0.6830079773854933,
        "userScore":1
    },
    {
        "title": "Explicit congruences for mock modular forms",
        "paperAbstract": "  In recent work of Bringmann, Guerzhoy, and the first author, p-adic modular\nforms were constructed from mock modular forms. This paper proves explicit\ncongruences for these p-adic modular forms.\n",
        "query": "number theory modular form functions noncongruence congruence",
        "docId": 666875,
        "score": 0.6812369005765504,
        "userScore":1
    },
    {
        "title": "Congruence classes for modular forms over small sets",
        "paperAbstract": "  J.P. Serre showed that for any integer $m,~a(n)\\equiv 0 \\pmod m$ for almost\nall $n,$ where $a(n)$ is the $n^{\\text{th}}$ Fourier coefficient of any modular\nform with rational coefficients. In this article, we consider a certain class\nof cuspforms and study $\\#\\{a(n) \\pmod m\\}_{n\\leq x}$ over the set of integers\nwith $O(1)$ many prime factors. Moreover, we show that any residue class $a\\in\n\\mathbb{Z}/m\\mathbb{Z}$ can be written as the sum of at most thirteen Fourier\ncoefficients, which are polynomially bounded as a function of $m.$\n",
        "query": "number theory modular form functions noncongruence congruence",
        "docId": 1787659,
        "score": 0.6715185248891071,
        "userScore":2
    },
    {
        "title": "Benford\u0027s Law and Distractors in Multiple Choice Exams",
        "paperAbstract": "  Suppose that in a multiple choice examination the leading digit of the\ncorrect options follows Benford\u0027s Law, while the the leading digit of the\ndistractors are uniform. Consider a strategy for guessing at answers that\nselects the option with the lowest leading digit with ties broken at random. We\nprovide an expression for the probability that this strategy selects the\ncorrect option.\n",
        "query": "intransitive dice rolling",
        "docId": 481267,
        "score": 0.295647471633508,
        "userScore":0
    },
    {
        "title": "Learning Non-Monotonic Automatic Post-Editing of Translations from Human\n  Orderings",
        "paperAbstract": "  Recent research in neural machine translation has explored flexible\ngeneration orders, as an alternative to left-to-right generation. However,\ntraining non-monotonic models brings a new complication: how to search for a\ngood ordering when there is a combinatorial explosion of orderings arriving at\nthe same final result? Also, how do these automatic orderings compare with the\nactual behaviour of human translators? Current models rely on manually built\nbiases or are left to explore all possibilities on their own. In this paper, we\nanalyze the orderings produced by human post-editors and use them to train an\nautomatic post-editing system. We compare the resulting system with those\ntrained with left-to-right and random post-editing orderings. We observe that\nhumans tend to follow a nearly left-to-right order, but with interesting\ndeviations, such as preferring to start by correcting punctuation or verbs.\n",
        "query": "intransitive dice rolling",
        "docId": 1278890,
        "score": 0.29479814523094205,
        "userScore":0
    },
    {
        "title": "Forbidden Patterns and the Alternating Derangement Sequence",
        "paperAbstract": "  In this note we count linear arrangements that avoid certain patterns and\nshow their connection to the derangement numbers. We discuss the sequence Dn,\nwhich counts linear arrangements that avoid patterns 12, 23, ..., (n-1)n, n1,\nand show that this sequence almost follows the derangement sequence itself\nsince the number of its odd terms is one more than the derangement numbers\nwhile the number of its even terms is one less. We also express the derangement\nnumbers in terms of these and other arrangements.\n",
        "query": "intransitive dice rolling",
        "docId": 777334,
        "score": 0.27247549339278265,
        "userScore":0
    },
    {
        "title": "THEaiTRE: Artificial Intelligence to Write a Theatre Play",
        "paperAbstract": "  We present THEaiTRE, a starting project aimed at automatic generation of\ntheatre play scripts. This paper reviews related work and drafts an approach we\nintend to follow. We plan to adopt generative neural language models and\nhierarchical generation approaches, supported by summarization and machine\ntranslation methods, and complemented with a human-in-the-loop approach.\n",
        "query": "intransitive dice rolling",
        "docId": 1309182,
        "score": 0.26651720655900135,
        "userScore":0
    },
    {
        "title": "Birational Rowmotion and the Octahedron Recurrence",
        "paperAbstract": "  We use the octahedron recurrence to give a simplified statement and proof of\na formula for iterated birational rowmotion on a product of two chains, first\ndescribed by Musiker and Roby. Using this, we show that weights of certain\nchains in rectangles shift in a predictable way under the action of rowmotion.\nWe then define generalized Stanley-Thomas words whose cyclic rotation uniquely\ndetermines birational rowmotion on the product of two chains. We also discuss\nthe relationship between rowmotion and birational RSK and give a birational\nanalogue of Greene\u0027s theorem in this setting.\n",
        "query": "intransitive dice rolling",
        "docId": 1634393,
        "score": 0.2628200745377569,
        "userScore":0
    },
    {
        "title": "Rowmotion on 321-avoiding permutations",
        "paperAbstract": "  We give a natural definition of rowmotion for $321$-avoiding permutations, by\ntranslating, through bijections involving Dyck paths and the Lalanne--Kreweras\ninvolution, the analogous notion for antichains of the positive root poset of\ntype $A$. We prove that some permutation statistics, such as the number of\nfixed points, are homomesic under rowmotion, meaning that they have a constant\naverage over its orbits.\n  Our setting also provides a more natural description of the celebrated\nArmstrong--Stump--Thomas equivariant bijection between antichains and\nnon-crossing matchings in types $A$ and $B$, by showing that it is equivalent\nto the Robinson--Schensted--Knuth correspondence on $321$-avoiding permutations\npermutations.\n",
        "query": "intransitive dice rolling",
        "docId": 1767524,
        "score": 0.2550957461130494,
        "userScore":0
    },
    {
        "title": "Likelihood-based solution to the Monty Hall puzzle and a related\n  3-prisoner paradox",
        "paperAbstract": "  The Monty Hall puzzle has been solved and dissected in many ways, but always\nusing probabilistic arguments, so it is considered a probability puzzle. In\nthis paper the puzzle is set up as an orthodox statistical problem involving an\nunknown parameter, a probability model and an observation. This means we can\ncompute a likelihood function, and the decision to switch corresponds to\nchoosing the maximum likelihood solution. One advantage of the likelihood-based\nsolution is that the reasoning applies to a single game, unaffected by the\nfuture plan of the host. I also describe an earlier version of the puzzle in\nterms of three prisoners: two to be executed and one released. Unlike the goats\nand the car, these prisoners have consciousness, so they can think about\nexchanging punishments. When two of them do that, however, we have a paradox,\nwhere it is advantageous for both to exchange their punishment with each other.\nOverall, the puzzle and the paradox are useful examples of statistical\nthinking, so they are excellent teaching topics.\n",
        "query": "intransitive dice rolling",
        "docId": 1358541,
        "score": 0.25405561074909855,
        "userScore":0
    },
    {
        "title": "Fast Cross-domain Data Augmentation through Neural Sentence Editing",
        "paperAbstract": "  Data augmentation promises to alleviate data scarcity. This is most important\nin cases where the initial data is in short supply. This is, for existing\nmethods, also where augmenting is the most difficult, as learning the full data\ndistribution is impossible. For natural language, sentence editing offers a\nsolution - relying on small but meaningful changes to the original ones.\nLearning which changes are meaningful also requires large amounts of training\ndata. We thus aim to learn this in a source domain where data is abundant and\napply it in a different, target domain, where data is scarce - cross-domain\naugmentation.\n  We create the Edit-transformer, a Transformer-based sentence editor that is\nsignificantly faster than the state of the art and also works cross-domain. We\nargue that, due to its structure, the Edit-transformer is better suited for\ncross-domain environments than its edit-based predecessors. We show this\nperformance gap on the Yelp-Wikipedia domain pairs. Finally, we show that due\nto this cross-domain performance advantage, the Edit-transformer leads to\nmeaningful performance gains in several downstream tasks.\n",
        "query": "intransitive dice rolling",
        "docId": 1260609,
        "score": 0.24306250845984323,
        "userScore":0
    },
    {
        "title": "A Note on the Structure of Roller Coaster Permutations",
        "paperAbstract": "  We consider the structure of roller coaster permutations as introduced by\nAhmed \u0026 Snevily[1]. A roller coaster permutation is described as a permuta-\ntion that maximizes the total switches from ascending to descending or visa\nversa for the permutation and all of its subpermutations simultaneously. This\npaper looks at the alternating structure of these permutations and then we\nintroduce a notion of a condition stronger than alternating for a permutation\nthat we shall refer to as recursively alternating. We also examine the behav-\nior of what entries can show up in even, odd, and end positions within the\npermutations.\n",
        "query": "intransitive dice rolling",
        "docId": 730530,
        "score": 0.24250049353383218,
        "userScore":0
    },
    {
        "title": "Simulating Action Dynamics with Neural Process Networks",
        "paperAbstract": "  Understanding procedural language requires anticipating the causal effects of\nactions, even when they are not explicitly stated. In this work, we introduce\nNeural Process Networks to understand procedural text through (neural)\nsimulation of action dynamics. Our model complements existing memory\narchitectures with dynamic entity tracking by explicitly modeling actions as\nstate transformers. The model updates the states of the entities by executing\nlearned action operators. Empirical results demonstrate that our proposed model\ncan reason about the unstated causal effects of actions, allowing it to provide\nmore accurate contextual information for understanding and generating\nprocedural text, all while offering more interpretable internal representations\nthan existing alternatives.\n",
        "query": "intransitive dice rolling",
        "docId": 912323,
        "score": 0.23844594603308877,
        "userScore":0
    },
    {
        "title": "Recent Trends in Deep Learning Based Natural Language Processing",
        "paperAbstract": "  Deep learning methods employ multiple processing layers to learn hierarchical\nrepresentations of data and have produced state-of-the-art results in many\ndomains. Recently, a variety of model designs and methods have blossomed in the\ncontext of natural language processing (NLP). In this paper, we review\nsignificant deep learning related models and methods that have been employed\nfor numerous NLP tasks and provide a walk-through of their evolution. We also\nsummarize, compare and contrast the various models and put forward a detailed\nunderstanding of the past, present and future of deep learning in NLP.\n",
        "query": "natural language processing deep learning",
        "docId": 877721,
        "score": 0.721123553926077,
        "userScore":2
    },
    {
        "title": "Natural Language Processing Advancements By Deep Learning: A Survey",
        "paperAbstract": "  Natural Language Processing (NLP) helps empower intelligent machines by\nenhancing a better understanding of the human language for linguistic-based\nhuman-computer communication. Recent developments in computational power and\nthe advent of large amounts of linguistic data have heightened the need and\ndemand for automating semantic analysis using data-driven approaches. The\nutilization of data-driven strategies is pervasive now due to the significant\nimprovements demonstrated through the usage of deep learning methods in areas\nsuch as Computer Vision, Automatic Speech Recognition, and in particular, NLP.\nThis survey categorizes and addresses the different aspects and applications of\nNLP that have benefited from deep learning. It covers core NLP tasks and\napplications and describes how deep learning methods and models advance these\nareas. We further analyze and compare different approaches and state-of-the-art\nmodels.\n",
        "query": "natural language processing deep learning",
        "docId": 1251555,
        "score": 0.7174118673795462,
        "userScore":3
    },
    {
        "title": "A Survey of the Usages of Deep Learning in Natural Language Processing",
        "paperAbstract": "  Over the last several years, the field of natural language processing has\nbeen propelled forward by an explosion in the use of deep learning models. This\nsurvey provides a brief introduction to the field and a quick overview of deep\nlearning architectures and methods. It then sifts through the plethora of\nrecent studies and summarizes a large assortment of relevant contributions.\nAnalyzed research areas include several core linguistic processing issues in\naddition to a number of applications of computational linguistics. A discussion\nof the current state of the art is then provided along with recommendations for\nfuture research in the field.\n",
        "query": "natural language processing deep learning",
        "docId": 1008060,
        "score": 0.7016551808837619,
        "userScore":2
    },
    {
        "title": "Comparative Study of CNN and RNN for Natural Language Processing",
        "paperAbstract": "  Deep neural networks (DNN) have revolutionized the field of natural language\nprocessing (NLP). Convolutional neural network (CNN) and recurrent neural\nnetwork (RNN), the two main types of DNN architectures, are widely explored to\nhandle various NLP tasks. CNN is supposed to be good at extracting\nposition-invariant features and RNN at modeling units in sequence. The state of\nthe art on many NLP tasks often switches due to the battle between CNNs and\nRNNs. This work is the first systematic comparison of CNN and RNN on a wide\nrange of representative NLP tasks, aiming to give basic guidance for DNN\nselection.\n",
        "query": "natural language processing deep learning",
        "docId": 816517,
        "score": 0.6573233692757139,
        "userScore":2
    },
    {
        "title": "Natural Language Processing (almost) from Scratch",
        "paperAbstract": "  We propose a unified neural network architecture and learning algorithm that\ncan be applied to various natural language processing tasks including:\npart-of-speech tagging, chunking, named entity recognition, and semantic role\nlabeling. This versatility is achieved by trying to avoid task-specific\nengineering and therefore disregarding a lot of prior knowledge. Instead of\nexploiting man-made input features carefully optimized for each task, our\nsystem learns internal representations on the basis of vast amounts of mostly\nunlabeled training data. This work is then used as a basis for building a\nfreely available tagging system with good performance and minimal computational\nrequirements.\n",
        "query": "natural language processing deep learning",
        "docId": 247666,
        "score": 0.6565140889512975,
        "userScore":1
    },
    {
        "title": "Empirical Evaluation of Multi-task Learning in Deep Neural Networks for\n  Natural Language Processing",
        "paperAbstract": "  Multi-Task Learning (MTL) aims at boosting the overall performance of each\nindividual task by leveraging useful information contained in multiple related\ntasks. It has shown great success in natural language processing (NLP).\nCurrently, a number of MLT architectures and learning mechanisms have been\nproposed for various NLP tasks. However, there is no systematic exploration and\ncomparison of different MLT architectures and learning mechanisms for their\nstrong performance in-depth. In this paper, we conduct a thorough examination\nof typical MTL methods on a broad range of representative NLP tasks. Our\nprimary goal is to understand the merits and demerits of existing MTL methods\nin NLP tasks, thus devising new hybrid architectures intended to combine their\nstrengths.\n",
        "query": "natural language processing deep learning",
        "docId": 1165990,
        "score": 0.6522237967258135,
        "userScore":2
    },
    {
        "title": "Experimental Standards for Deep Learning in Natural Language Processing\n  Research",
        "paperAbstract": "  The field of Deep Learning (DL) has undergone explosive growth during the\nlast decade, with a substantial impact on Natural Language Processing (NLP) as\nwell. Yet, compared to more established disciplines, a lack of common\nexperimental standards remains an open challenge to the field at large.\nStarting from fundamental scientific principles, we distill ongoing discussions\non experimental standards in NLP into a single, widely-applicable methodology.\nFollowing these best practices is crucial to strengthen experimental evidence,\nimprove reproducibility and support scientific progress. These standards are\nfurther collected in a public repository to help them transparently adapt to\nfuture needs.\n",
        "query": "natural language processing deep learning",
        "docId": 1636389,
        "score": 0.649964741042238,
        "userScore":2
    },
    {
        "title": "Multi-Task Learning in Natural Language Processing: An Overview",
        "paperAbstract": "  Deep learning approaches have achieved great success in the field of Natural\nLanguage Processing (NLP). However, deep neural models often suffer from\noverfitting and data scarcity problems that are pervasive in NLP tasks. In\nrecent years, Multi-Task Learning (MTL), which can leverage useful information\nof related tasks to achieve simultaneous performance improvement on multiple\nrelated tasks, has been used to handle these problems. In this paper, we give\nan overview of the use of MTL in NLP tasks. We first review MTL architectures\nused in NLP tasks and categorize them into four classes, including the parallel\narchitecture, hierarchical architecture, modular architecture, and generative\nadversarial architecture. Then we present optimization techniques on loss\nconstruction, data sampling, and task scheduling to properly train a multi-task\nmodel. After presenting applications of MTL in a variety of NLP tasks, we\nintroduce some benchmark datasets. Finally, we make a conclusion and discuss\nseveral possible research directions in this field.\n",
        "query": "natural language processing deep learning",
        "docId": 1531959,
        "score": 0.6458839638597667,
        "userScore":2
    },
    {
        "title": "Notes on Deep Learning for NLP",
        "paperAbstract": "  My notes on Deep Learning for NLP.\n",
        "query": "natural language processing deep learning",
        "docId": 1018918,
        "score": 0.6456571487764915,
        "userScore":0
    },
    {
        "title": "Deep Learning applied to NLP",
        "paperAbstract": "  Convolutional Neural Network (CNNs) are typically associated with Computer\nVision. CNNs are responsible for major breakthroughs in Image Classification\nand are the core of most Computer Vision systems today. More recently CNNs have\nbeen applied to problems in Natural Language Processing and gotten some\ninteresting results. In this paper, we will try to explain the basics of CNNs,\nits different variations and how they have been applied to NLP.\n",
        "query": "natural language processing deep learning",
        "docId": 826595,
        "score": 0.6428150524443605,
        "userScore":2
    },
    {
        "title": "ptype-cat: Inferring the Type and Values of Categorical Variables",
        "paperAbstract": "  Type inference is the task of identifying the type of values in a data column\nand has been studied extensively in the literature. Most existing type\ninference methods support data types such as Boolean, date, float, integer and\nstring. However, these methods do not consider non-Boolean categorical\nvariables, where there are more than two possible values encoded by integers or\nstrings. Therefore, such columns are annotated either as integer or string\nrather than categorical, and need to be transformed into categorical manually\nby the user. In this paper, we propose a probabilistic type inference method\nthat can identify the general categorical data type (including non-Boolean\nvariables). Additionally, we identify the possible values of each categorical\nvariable by adapting the existing type inference method ptype. Combining these\nmethods, we present ptype-cat which achieves better results than existing\napplicable solutions.\n",
        "query": "types data mining",
        "docId": 1566059,
        "score": 0.6402752258237552,
        "userScore":1
    },
    {
        "title": "Graded Modal Dependent Type Theory",
        "paperAbstract": "  Graded type theories are an emerging paradigm for augmenting the reasoning\npower of types with parameterizable, fine-grained analyses of program\nproperties. There have been many such theories in recent years which equip a\ntype theory with quantitative dataflow tracking, usually via a semiring-like\nstructure which provides analysis on variables (often called `quantitative\u0027 or\n`coeffect\u0027 theories). We present Graded Modal Dependent Type Theory (GrTT for\nshort), which equips a dependent type theory with a general, parameterizable\nanalysis of the flow of data, both in and between computational terms and\ntypes. In this theory, it is possible to study, restrict, and reason about data\nuse in programs and types, enabling, for example, parametric quantifiers and\nlinearity to be captured in a dependent setting. We propose GrTT, study its\nmetatheory, and explore various case studies of its use in reasoning about\nprograms and studying other type theories. We have implemented the theory and\nhighlight the interesting details, including showing an application of grading\nto optimising the type checking procedure itself.\n",
        "query": "types data mining",
        "docId": 1369493,
        "score": 0.6123921343166501,
        "userScore":0
    },
    {
        "title": "Improving Precision of Type Analysis Using Non-Discriminative Union",
        "paperAbstract": "  This paper presents a new type analysis for logic programs. The analysis is\nperformed with a priori type definitions; and type expressions are formed from\na fixed alphabet of type constructors. Non-discriminative union is used to join\ntype information from different sources without loss of precision. An operation\nthat is performed repeatedly during an analysis is to detect if a fixpoint has\nbeen reached. This is reduced to checking the emptiness of types. Due to the\nuse of non-discriminative union, the fundamental problem of checking the\nemptiness of types is more complex in the proposed type analysis than in other\ntype analyses with a priori type definitions. The experimental results,\nhowever, show that use of tabling reduces the effect to a small fraction of\nanalysis time on a set of benchmarks.\n  Keywords: Type analysis, Non-discriminative union, Abstract interpretation,\nTabling\n",
        "query": "types data mining",
        "docId": 1979424,
        "score": 0.6077857151675516,
        "userScore":0
    },
    {
        "title": "Data Type Inference for Logic Programming",
        "paperAbstract": "  In this paper we present a new static data type inference algorithm for logic\nprogramming. Without the need of declaring types for predicates, our algorithm\nis able to automatically assign types to predicates which, in most cases,\ncorrespond to the data types processed by their intended meaning. The algorithm\nis also able to infer types given data type definitions similar to data\ndefinitions in Haskell and, in this case, the inferred types are more\ninformative in general. We present the type inference algorithm, prove some\nproperties and finally, we evaluate our approach on example programs that deal\nwith different data structures.\n",
        "query": "types data mining",
        "docId": 1515377,
        "score": 0.602627375221509,
        "userScore":0
    },
    {
        "title": "Type Theory based on Dependent Inductive and Coinductive Types",
        "paperAbstract": "  We develop a dependent type theory that is based purely on inductive and\ncoinductive types, and the corresponding recursion and corecursion principles.\nThis results in a type theory with a small set of rules, while still being\nfairly expressive. For example, all well-known basic types and type formers\nthat are needed for using this type theory as a logic are definable:\npropositional connectives, like falsity, conjunction, disjunction, and function\nspace, dependent function space, existential quantification, equality, natural\nnumbers, vectors etc. The reduction relation on terms consists solely of a rule\nfor recursion and a rule for corecursion. The reduction relations for\nwell-known types arise from that. To further support the introduction of this\nnew type theory, we also prove fundamental properties of its term calculus.\nMost importantly, we prove subject reduction and strong normalisation of the\nreduction relation, which gives computational meaning to the terms.\n  The presented type theory is based on ideas from categorical logic that have\nbeen investigated before by the first author, and it extends Hagino\u0027s\ncategorical data types to a dependently typed setting. By basing the type\ntheory on concepts from category theory we maintain the duality between\ninductive and coinductive types, and it allows us to describe, for example, the\nfunction space as a coinductive type.\n",
        "query": "types data mining",
        "docId": 730321,
        "score": 0.5971935578106411,
        "userScore":0
    },
    {
        "title": "Type Soundness for Path Polymorphism",
        "paperAbstract": "  Path polymorphism is the ability to define functions that can operate\nuniformly over arbitrary recursively specified data structures. Its essence is\ncaptured by patterns of the form $x\\,y$ which decompose a compound data\nstructure into its parts. Typing these kinds of patterns is challenging since\nthe type of a compound should determine the type of its components. We propose\na static type system (i.e. no run-time analysis) for a pattern calculus that\ncaptures this feature. Our solution combines type application, constants as\ntypes, union types and recursive types. We address the fundamental properties\nof Subject Reduction and Progress that guarantee a well-behaved dynamics. Both\nthese results rely crucially on a notion of pattern compatibility and also on a\ncoinductive characterisation of subtyping.\n",
        "query": "types data mining",
        "docId": 695299,
        "score": 0.5875961006594252,
        "userScore":0
    },
    {
        "title": "Type theoretical databases",
        "paperAbstract": "  We present a soundness theorem for a dependent type theory with context\nconstants with respect to an indexed category of (finite, abstract) simplical\ncomplexes. The point of interest for computer science is that this category can\nbe seen to represent tables in a natural way. Thus the category is a model for\ndatabases, a single mathematical structure in which all database schemas and\ninstances (of a suitable, but sufficiently general form) are represented. The\ntype theory then allows for the specification of database schemas and\ninstances, the manipulation of the same with the usual type-theoretic\noperations, and the posing of queries.\n",
        "query": "types data mining",
        "docId": 535151,
        "score": 0.587195532680191,
        "userScore":0
    },
    {
        "title": "Practical Datatype Specializations with Phantom Types and Recursion\n  Schemes",
        "paperAbstract": "  Datatype specialization is a form of subtyping that captures program\ninvariants on data structures that are expressed using the convenient and\nintuitive datatype notation. Of particular interest are structural invariants\nsuch as well-formedness. We investigate the use of phantom types for describing\ndatatype specializations. We show that it is possible to express\nstatically-checked specializations within the type system of Standard ML. We\nalso show that this can be done in a way that does not lose useful programming\nfacilities such as pattern matching in case expressions.\n",
        "query": "types data mining",
        "docId": 1977593,
        "score": 0.5849840458564284,
        "userScore":0
    },
    {
        "title": "Normalisation by Evaluation for Type Theory, in Type Theory",
        "paperAbstract": "  We develop normalisation by evaluation (NBE) for dependent types based on\npresheaf categories. Our construction is formulated in the metalanguage of type\ntheory using quotient inductive types. We use a typed presentation hence there\nare no preterms or realizers in our construction, and every construction\nrespects the conversion relation. NBE for simple types uses a logical relation\nbetween the syntax and the presheaf interpretation. In our construction, we\nmerge the presheaf interpretation and the logical relation into a\nproof-relevant logical predicate. We prove normalisation, completeness,\nstability and decidability of definitional equality. Most of the constructions\nwere formalized in Agda.\n",
        "query": "types data mining",
        "docId": 798271,
        "score": 0.581034161592008,
        "userScore":0
    },
    {
        "title": "Typage fort et typage souple des collections topologiques et des\n  transformations",
        "paperAbstract": "  Topological collections allow to consider uniformly many data structures in\nprogramming languages and are handled by functions defined by pattern matching\ncalled transformations. We present two type systems for languages with\ntopological collections and transformations. The first one is a strong type\nsystem \\`a la Hindley/Milner which can be entirely typed at compile time. The\nsecond one is a mixed static and dynamic type system allowing to handle\nheterogeneous collections, that is collections which contain values with\ndifferent types. In the two cases, automatic type inference is possible.\n",
        "query": "types data mining",
        "docId": 164629,
        "score": 0.5672602555292325,
        "userScore":0
    },
    {
        "title": "Mirror version of similar triangles method for constrained optimization\n  problems",
        "paperAbstract": "  Science about optimization methods is rapidly developing today. In machine\nlearning, computer vision, biology, medicine, construction and in many other\ndifferent areas optimization methods have vast popularity and they appear as\nimportant tool. One of the most important goals in optimization: create some\n\"universal\" method, which will have good performance in all problems regardless\nsmoothness of a task, computation precision of gradient and other parameters\nwhich characterize a problem. In this thesis we propose a method which is\n\"universal\" for different problems and, at the same time, is simple for\nunderstanding.\n",
        "query": "optimization algorithms",
        "docId": 853350,
        "score": 0.6418262028083106,
        "userScore":0
    },
    {
        "title": "Extremal Optimization: Heuristics via Co-Evolutionary Avalanches",
        "paperAbstract": "  An introduction to Extremal Optimization written for the Computer Simulation\nColumn in ``Computing in Science and Engineering\u0027\u0027 (CISE).\n",
        "query": "optimization algorithms",
        "docId": 1897201,
        "score": 0.641348303083646,
        "userScore":1
    },
    {
        "title": "Mathematics for Machine Learning and Data Science: Optimization with\n  Mathematica Applications",
        "paperAbstract": "  The field of optimization has gotten a lot of interest in recent years owing\nto significant advances in computer technology. Numerous issues in machine\nlearning, economics, finance, geophysics, molecular modeling, computational\nsystems biology, operations research, and all areas of engineering are now\nbeing resolved owing to the rapid growth of optimization methods and\nalgorithms. This monograph presents the main theorems in linear algebra, convex\nsets, convex functions, single variable optimization, multivariable\noptimization, and their corresponding algorithms. We also briefly touch upon\nthe constrained nonlinear optimization. We have found the Wolfram language to\nbe ideal for specifying algorithms in human readable form. To minimize\nnonlinear objective functions, we have created 27 Mathematica functions that\nfollow the principles of 18 algorithms. The code examples were carefully\ndesigned to demonstrate the purpose of given algorithm. The code for each\nalgorithm will run as is with no code from prior algorithms or third parties\nrequired beyond the installation of Mathematica.\n",
        "query": "optimization algorithms",
        "docId": 1790898,
        "score": 0.6208226249919377,
        "userScore":3
    },
    {
        "title": "DEFT-FUNNEL: an open-source global optimization solver for constrained\n  grey-box and black-box problems",
        "paperAbstract": "  The fast-growing need for grey-box and black-box optimization methods for\nconstrained global optimization problems in fields such as medicine, chemistry,\nengineering and artificial intelligence, has contributed for the design of new\nefficient algorithms for finding the best possible solution. In this work, we\npresent DEFT-FUNNEL, an open-source global optimization algorithm for general\nconstrained grey-box and black-box problems that belongs to the class of\ntrust-region sequential quadratic optimization algorithms. It extends the\nprevious works by Sampaio and Toint (2015, 2016) to a global optimization\nsolver that is able to exploit information from closed-form functions.\nPolynomial interpolation models are used as surrogates for the black-box\nfunctions and a clustering-based multistart strategy is applied for searching\nfor the global minima. Numerical experiments show that DEFT-FUNNEL compares\nfavorably with other state-of-the-art methods on two sets of benchmark\nproblems: one set containing problems where every function is a black box and\nanother set with problems where some of the functions and their derivatives are\nknown to the solver. The code as well as the test sets used for experiments are\navailable at the Github repository http://github.com/phrsampaio/deft-funnel.\n",
        "query": "optimization algorithms",
        "docId": 1224533,
        "score": 0.5963218066784677,
        "userScore":2
    },
    {
        "title": "Opytimizer: A Nature-Inspired Python Optimizer",
        "paperAbstract": "  Optimization aims at selecting a feasible set of parameters in an attempt to\nsolve a particular problem, being applied in a wide range of applications, such\nas operations research, machine learning fine-tuning, and control engineering,\namong others. Nevertheless, traditional iterative optimization methods use the\nevaluation of gradients and Hessians to find their solutions, not being\npractical due to their computational burden and when working with non-convex\nfunctions. Recent biological-inspired methods, known as meta-heuristics, have\narisen in an attempt to fulfill these problems. Even though they do not\nguarantee to find optimal solutions, they usually find a suitable solution. In\nthis paper, we proposed a Python-based meta-heuristic optimization framework\ndenoted as Opytimizer. Several methods and classes are implemented to provide a\nuser-friendly workspace among diverse meta-heuristics, ranging from\nevolutionary- to swarm-based techniques.\n",
        "query": "optimization algorithms",
        "docId": 1224898,
        "score": 0.586809887698946,
        "userScore":2
    },
    {
        "title": "Optimization Strategies in Complex Systems",
        "paperAbstract": "  We consider a class of combinatorial optimization problems that emerge in a\nvariety of domains among which: condensed matter physics, theory of financial\nrisks, error correcting codes in information transmissions, molecular and\nprotein conformation, image restoration. We show the performances of two\nalgorithms, the``greedy\u0027\u0027 (quick decrease along the gradient) and\nthe``reluctant\u0027\u0027 (slow decrease close to the level curves) as well as those of\na``stochastic convex interpolation\u0027\u0027of the two. Concepts like the average\nrelaxation time and the wideness of the attraction basin are analyzed and their\nsystem size dependence illustrated.\n",
        "query": "optimization algorithms",
        "docId": 2129414,
        "score": 0.5785654030880334,
        "userScore":2
    },
    {
        "title": "Optimization on Spheres: Models and Proximal Algorithms with\n  Computational Performance Comparisons",
        "paperAbstract": "  We present a unified treatment of the abstract problem of finding the best\napproximation between a cone and spheres in the image of affine\ntransformations. Prominent instances of this problem are phase retrieval and\nsource localization. The common geometry binding these problems permits a\ngeneric application of algorithmic ideas and abstract convergence results for\nnonconvex optimization. We organize variational models for this problem into\nthree different classes and derive the main algorithmic approaches within these\nclasses (13 in all). We identify the central ideas underlying these methods and\nprovide thorough numerical benchmarks comparing their performance on synthetic\nand laboratory data. The software and data of our experiments are all publicly\naccessible. We also introduce one new algorithm, a cyclic relaxed\nDouglas-Rachford algorithm, which outperforms all other algorithms by every\nmeasure: speed, stability and accuracy. The analysis of this algorithm remains\nopen.\n",
        "query": "optimization algorithms",
        "docId": 1034080,
        "score": 0.5767717983316043,
        "userScore":2
    },
    {
        "title": "Das Optimierungslabor -- ein Erfahrungsbericht (Experiencing\n  optimization with students)",
        "paperAbstract": "  For several years, students visit us on different occasions at the\nuniversity. But how to bridge from the school curriculum to the contents of the\nuniversity mathematics? And how to find a focal point at which an active\ncontribute, despite the lack of knowledge, in view of limited time is possible?\nOur approach: Translate,under guidance, everyday life optimization problems\ninto the language of mathematics, i.e. using variables, target functions,\nequations and inequalities. These so-called integer linear programming models\nare then solved by standard software. In this report we wnat to tell about the\nlessons we have learned.\n",
        "query": "optimization algorithms",
        "docId": 497392,
        "score": 0.5707543427758974,
        "userScore":1
    },
    {
        "title": "An algorithm for computing Fr\\\u0027echet means on the sphere",
        "paperAbstract": "  For most optimisation methods an essential assumption is the vector space\nstructure of the feasible set. This condition is not fulfilled if we consider\noptimisation problems over the sphere. We present an algorithm for solving a\nspecial global problem over the sphere, namely the determination of Fr\\\u0027echet\nmeans, which are points minimising the mean distance to a given set of points.\nThe Branch and Bound method derived needs no further assumptions on the input\ndata, but is able to cope with this objective function which is neither convex\nnor differentiable. The algorithm\u0027s performance is tested on simulated and real\ndata.\n",
        "query": "optimization algorithms",
        "docId": 948783,
        "score": 0.5672056901369,
        "userScore":1
    },
    {
        "title": "Porcellio scaber algorithm (PSA) for solving constrained optimization\n  problems",
        "paperAbstract": "  In this paper, we extend a bio-inspired algorithm called the porcellio scaber\nalgorithm (PSA) to solve constrained optimization problems, including a\nconstrained mixed discrete-continuous nonlinear optimization problem. Our\nextensive experiment results based on benchmark optimization problems show that\nthe PSA has a better performance than many existing methods or algorithms. The\nresults indicate that the PSA is a promising algorithm for constrained\noptimization.\n",
        "query": "optimization algorithms",
        "docId": 899419,
        "score": 0.5640212085698533,
        "userScore":2
    },
    {
        "title": "On the nature of monetary and price inflation and hyperinflation",
        "paperAbstract": "  Monetary inflation is a sustained increase in the money supply than can\nresult in price inflation, which is a rise in the general level of prices of\ngoods and services. The objectives of this paper were to develop economic\nmodels to (1) predict the annual rate of growth in the US consumer price index\n(CPI), based on the annual growth in the US broad money supply (BMS), the\nannual growth in US real GDP, and the annual growth in US savings, over the\ntime period 2001 to 2019; (2) investigate the means by which monetary and price\ninflation can develop into monetary and price hyperinflation. The hypothesis\nthat the annual rate of growth in the US CPI is a function of the annual growth\nin the US BMS minus the annual growth in US real GDP minus the annual growth in\nUS savings, over the time period investigated, has been shown to be the case.\nHowever, an exact relationship required the use of a non-zero residual term. A\nmathematical statistical formulation of a hyperinflationary process has been\nprovided and used to quantify the period of hyperinflation in the Weimar\nRepublic, from July 1922 until the end of November 1923.\n",
        "query": "GDP growth inflation rate",
        "docId": 1535801,
        "score": 0.6737205802267496,
        "userScore":2
    },
    {
        "title": "Interest Rates and Inflation",
        "paperAbstract": "  A relation between interest rates and inflation is presented using a two\ncomponent economic model and a simple general principle. Preliminary results\nindicate a remarkable similarity to classical economic theories, in particular\nthat of Wicksell.\n",
        "query": "GDP growth inflation rate",
        "docId": 255898,
        "score": 0.6349578641521401,
        "userScore":2
    },
    {
        "title": "Do Mature Economies Grow Exponentially?",
        "paperAbstract": "  Most models that try to explain economic growth indicate exponential growth\npaths. In recent years, however, a lively discussion has emerged considering\nthe validity of this notion. In the empirical literature dealing with drivers\nof economic growth, the majority of articles is based upon an implicit\nassumption of exponential growth. Few scholarly articles have addressed this\nissue so far. In order to shed light on this issue, we estimate autoregressive\nintegrated moving average time series models based on Gross Domestic Product\nPer Capita data for 18 mature economies from 1960 to 2013. We compare the\nadequacy of linear and exponential growth models and conduct several robustness\nchecks. Our fndings cast doubts on the widespread belief of exponential growth\nand suggest a deeper discussion on alternative economic grow theories.\n",
        "query": "GDP growth inflation rate",
        "docId": 696056,
        "score": 0.5935524832651069,
        "userScore":1
    },
    {
        "title": "What Drives Inflation and How: Evidence from Additive Mixed Models\n  Selected by cAIC",
        "paperAbstract": "  We analyze the forces that explain inflation using a panel of 122 countries\nfrom 1997 to 2015 with 37 regressors. 98 models motivated by economic theory\nare compared to a gradient boosting algorithm, non-linearities and structural\nbreaks are considered. We show that the typical estimation methods are likely\nto lead to fallacious policy conclusions which motivates the use of a new\napproach that we propose in this paper. The boosting algorithm outperforms\ntheory-based models. We confirm that energy prices are important but what\nreally matters for inflation is their non-linear interplay with energy rents.\nDemographic developments also make a difference. Globalization and technology,\npublic debt, central bank independence and political characteristics are less\nrelevant. GDP per capita is more relevant than the output gap, credit growth\nmore than M2 growth.\n",
        "query": "GDP growth inflation rate",
        "docId": 1300788,
        "score": 0.5928841135377567,
        "userScore":1
    },
    {
        "title": "Why price inflation in developed countries is systematically\n  underestimated",
        "paperAbstract": "  There is an extensive historical dataset on real GDP per capita prepared by\nAngus Maddison. This dataset covers the period since 1870 with continuous\nannual estimates in developed countries. All time series for individual\neconomies have a clear structural break between 1940 and 1950. The behavior\nbefore 1940 and after 1950 can be accurately (R2 from 0.7 to 0.99) approximated\nby linear time trends. The corresponding slopes of regressions lines before and\nafter the break differ by a factor of 4 (Switzerland) to 19 (Spain). We have\nextrapolated the early trends into the second interval and obtained much lower\nestimates of real GDP per capita in 2011: from 2.4 (Switzerland) to 5.0 (Japan)\ntimes smaller than the current levels. When the current linear trends are\nextrapolated into the past, they intercept the zero line between 1908\n(Switzerland) and 1944 (Japan). There is likely an internal conflict between\nthe estimating procedures before 1940 and after 1950. A reasonable explanation\nof the discrepancy is that the GDP deflator in developed countries has been\nhighly underestimated since 1950. In the USA, the GDP deflator is\nunderestimated by a factor of 1.4. This is exactly the ratio of the interest\nrate controlled by the Federal Reserve and the rate of inflation. Hence, the\nFederal Reserve actually retains its interest rate at the level of true price\ninflation when corrected for the bias in the GDP deflator.\n",
        "query": "GDP growth inflation rate",
        "docId": 346543,
        "score": 0.5899312469505542,
        "userScore":1
    },
    {
        "title": "Interest Rates and Inflation",
        "paperAbstract": "  This article is an extension of the work of one of us (Coopersmith, 2011) in\nderiving the relationship between certain interest rates and the inflation rate\nof a two component economic system. We use the well-known Fisher relation\nbetween the difference of the nominal interest rate and its inflation adjusted\nvalue to eliminate the inflation rate and obtain a delay differential equation.\nWe provide computer simulated solutions for this equation over regimes of\ninterest. This paper could be of interest to three audiences: those in\nEconomics who are interested in interest and inflation; those in Mathematics\nwho are interested in examining a detailed analysis of a delay differential\nequation, which includes a summary of existing results, simulations, and an\nexact solution; and those in Physics who are interested in non-traditional\napplications of traditional methods of modeling.\n",
        "query": "GDP growth inflation rate",
        "docId": 717732,
        "score": 0.5884614382659876,
        "userScore":2
    },
    {
        "title": "Does GDP measure growth in the economy or simply growth in the money\n  supply?",
        "paperAbstract": "  Gross Domestic Product(GDP) is a widely used measurement of economic growth\nrepresenting the market value of all final goods and services produced by a\ncountry within a given time. In this paper we question the assumption that GDP\nmeasures production, and suggest that in reality it merely captures changes in\nthe rate of expansion of the money supply used to measure the price data it is\nderived from. We first review the Quantity Theory of Money $MV\u003dPT$, and show\nthat the Velocity of Circulation of Money(V) does not affect the price level as\nclaimed, as it is also a factor of the quantity of transactions(T). It then\nfollows directly that attempts to measure total production from any form of\nprice data as the GDP measurement does, will necessarily be confounded by the\ninverse relationship between prices and the quantity of production, which\nrequires that as the total quantity of production increases, prices will drop.\nFinally, in support of this claim we present an empirical analysis of the GDP\nof nine countries and one currency union, showing that when normalized for\nmoney supply growth GDP measures have been uniformly shrinking over the last 20\nyears, and discuss the possible reasons for this behaviour.\n",
        "query": "GDP growth inflation rate",
        "docId": 361214,
        "score": 0.5832781996957621,
        "userScore":2
    },
    {
        "title": "The role of the \"Maximizing Output Growth Inflation Rate\" in monetary\n  policy",
        "paperAbstract": "  The paper discusses the role of monetary policy when potential output depends\non the inflation rate. If the intention of the central bank is to maximize\nactual output growth, then it has to be credibly committed to a strict\ninflation targeting rule, and to take the MOGIR (the Maximizing Output Growth\nInflation Rate) as the target.\n",
        "query": "GDP growth inflation rate",
        "docId": 511011,
        "score": 0.5775763292626959,
        "userScore":1
    },
    {
        "title": "Inflation and unemployment in Japan: from 1980 to 2050",
        "paperAbstract": "  The evolution of inflation, p(t), and unemployment, UE(t), in Japan has been\nmodeled. Both variables were represented as linear functions of the change rate\nof labor force, dLF/LF. These models provide an accurate description of\ndisinflation in the 1990s and a deflationary period in the 2000s. In Japan,\nthere exists a statistically reliable (R2\u003d0.68) Phillips curve, which is\ncharacterized by a negative relation between inflation and unemployment and\ntheir synchronous evolution: UE(t) \u003d -0.94p(t) + 0.045. Effectively, growing\nunemployment has resulted in decreasing inflation since 1982. A linear and\nlagged generalized relationship between inflation, unemployment and labor force\nhas been also obtained for Japan: p(t) \u003d 2.8*dLF(t)/LF(t) + 0.9*UE(t) - 0.0392.\nLabor force projections allow a prediction of inflation and unemployment in\nJapan: CPI inflation will be negative (between -0.5% and -1% per year) during\nthe next 40 years. Unemployment will increase from ~4.0% in 2010 to 5.3% in\n2050.\n",
        "query": "GDP growth inflation rate",
        "docId": 171036,
        "score": 0.575850181581238,
        "userScore":1
    },
    {
        "title": "Can we predict long-run economic growth?",
        "paperAbstract": "  For those concerned with the long-term value of their accounts, it can be a\nchallenge to plan in the present for inflation-adjusted economic growth over\ncoming decades. Here, I argue that there exists an economic constant that\ncarries through time, and that this can help us to anticipate the more distant\nfuture: global economic wealth has a fixed link to civilization\u0027s overall rate\nof energy consumption from all sources; the ratio of these two quantities has\nnot changed over the past 40 years that statistics are available. Power\nproduction and wealth rise equally quickly because civilization, like any other\nsystem in the universe, must consume and dissipate its energy reserves in order\nto sustain its current size. One perspective might be that financial wealth\nmust ultimately collapse as we deplete our energy reserves. However, we can\nalso expect that highly aggregated quantities like global wealth have inertia,\nand that growth rates must persist. Exceptionally rapid innovation in the two\ndecades following 1950 allowed for unprecedented acceleration of\ninflation-adjusted rates of return. But today, real innovation rates are more\nstagnant. This means that, over the coming decade or so, global GDP and wealth\nshould rise fairly steadily at an inflation-adjusted rate of about 2.2% per\nyear.\n",
        "query": "GDP growth inflation rate",
        "docId": 385348,
        "score": 0.5745918710729754,
        "userScore":1
    },
    {
        "title": "A Survey of Impedance Measurement Methods in Power Electronics",
        "paperAbstract": "  Impedance is one of the vital parameters that provides useful information for\nmany power electronics related applications. A lot of impedance measurement\nmethods in power electronics have been reported. However, a comprehensive\ninvestigation among these methods in terms of their characteristics,\nadvantages, and limitations has not been found in the literature. In order to\nbridge this gap, a survey of the impedance measurement methods is conducted in\nthis paper. These methods are introduced, discussed, and then classified into\ndifferent categories depending on the measurement modes, principles, and\ninstruments. Moreover, recommendations for the future research on the impedance\nmeasurement are also presented.\n",
        "query": "power electronics applications",
        "docId": 1636233,
        "score": 0.5132286119021341,
        "userScore":3
    },
    {
        "title": "Review of Solid-State Modulators",
        "paperAbstract": "  Solid-state modulators for pulsed power applications have been a goal since\nthe first fast high-power semiconductor devices became available. Recent\nimprovements in both the speed and peak power capabilities of semiconductor\ndevices developed for the power conditioning and traction industries have led\nto a new generation of solid-state switched high power modulators with\nperformance rivaling that of hard tube modulators and thyratron switched\nline-type modulators. These new solid-state devices offer the promise of higher\nefficiency and longer lifetimes at a time when availability of standard\ntechnologies is becoming questionable. A brief discussion of circuit topologies\nand solid-state devices is followed by examples of modulators currently in use\nor in test. This presentation is intended to give an overview of the current\ncapabilities of solid-state modulators in various applications.\n",
        "query": "power electronics applications",
        "docId": 2187093,
        "score": 0.48363333969598443,
        "userScore":1
    },
    {
        "title": "Analyses of klystron modulator approaches for NLC",
        "paperAbstract": "  Analyze of experimental results shows that the energy discharge efficiency of\nthe modern modulators is 0.92-0.93. The energy transfer efficiency can be\nreached up to 0.7 for 1.5 usec pulse width and up to 0.8 for 3.0 usec\naccordingly. The geometry and core material of the transformer is one element\nof modulator, which limits the efficiency due to limitation if transmission\nband of the pulse power. Ways of the rise of energy transfer efficiency had\nbeen discussed. The use of features of coupled transmission lines lies in the\nbase of an idea how the energy transfer efficiency can be improved up to 0.85\nin the frame of the conventional approach. In the second part of the paper the\ntransformerless modulator approaches are presented upon a modified Marx method\nof voltage multiplication using on/off IGBT instead of on switches.\n",
        "query": "power electronics applications",
        "docId": 2187013,
        "score": 0.44339549096832087,
        "userScore":1
    },
    {
        "title": "Low-noise high-voltage DC power supply for nanopositioning applications",
        "paperAbstract": "  Nanopositioning techniques currently applied to characterize physical\nproperties of materials interesting for applications at the microscopic scale\nrely on high-voltage electronic control circuits that should have the lowest\npossible noise level. Here we introduce a simple, flexible, and custom-built\npower supply circuit that can provide +375\\,V with a noise level below 10\\,ppm.\nThe flexibility of the circuit comes from its topology based on discrete MOSFET\ncomponents that can be suitable replaced in order to change the polarity as\nwell as the output voltage and current.\n",
        "query": "power electronics applications",
        "docId": 555896,
        "score": 0.43340911600441245,
        "userScore":2
    },
    {
        "title": "The Electrodynamics of Free and Bound Charge Electricity Generators\n  using Impressed Sources and the Modification to Maxwell\u0027s Equations",
        "paperAbstract": "  The conversion of external energy into electricity is the foundation of power\nstation and energy harvesting operation. The external source supplies an\nimpressed force per unit charge to free or bound charge to produces AC\nelectricity. We analyze the electrodynamics of ideal electricity generators\nthrough a time dependent permanent polarization without any applied electric\nfield, which modifies the constitutive relations and is essential to oscillate\ncharge in a lossless way. For both cases, we show that Maxwell\u0027s equations, and\nin particular Faraday\u0027s law are modified, along with the required boundary\nconditions through the addition of an effective impressed magnetic current\nboundary source. For the free charge case, we highlight the example of an\nelectromagnetic generator based on Lorentz force, where the impressed force per\nunit charge that polarizes the conductor comes from mechanical motion of free\ncharge with an impressed velocity of a conductor relative to a stationary DC\nmagnetic field. The bound charge generator is an idealized permanently\npolarized bar electret with a time dependent permanent polarization, the\nunderlying principle behind piezoelectric nano-generators. In the open circuit\nstate, both electricity generators are equivalent to idealized Hertzian\ndipoles, with the open circuit voltage equal to the induced emf. Analyzing the\nshort circuit responses, we show that the bound charge electricity generator\nhas a capacitive source impedance. In contrast, we show for the ideal free\ncharge AC electricity generator, the back emf from the inductance of the loop\nthat defines the short circuit, directly cancels the source emf, so the voltage\nacross the inductor is solely determined by the magnetic current boundary\nsource. Thus, we determine the magnetic current boundary source is the\ntopological invariant that best describes the output voltage of an AC\ngenerator.\n",
        "query": "power electronics applications",
        "docId": 1110814,
        "score": 0.42732325807708094,
        "userScore":2
    },
    {
        "title": "Power Delivery of the Future",
        "paperAbstract": "  This paper is written to provide an insight into the physics and engineering\nthat go into power delivery of the future. Topics covered are Fault Current\nLimiters (FCL) including Superconducting FCL and Emission Limited FCL;\nLightning and Restoration Preparedness; Compressed-Gas-Insulated Delivery;\nEvaporative Cooling Delivery; Advanced Delivery Technologies Requiring Big\nBreakthroughs such as Conducting Polymers, Electron-Beam Delivery, Microwave\nDelivery, and Laser-Beam Delivery.\n",
        "query": "power electronics applications",
        "docId": 2190325,
        "score": 0.4262664151746094,
        "userScore":1
    },
    {
        "title": "Reference Ultra Low DC Current Source (ULCS) Between 1 fA and 100 pA at\n  TUBITAK UME",
        "paperAbstract": "  In this paper, we present a programmable Ultra Low DC Current Source (ULCS)\ndeveloped at T\\\"UB\\.ITAK UME. The output current range is from 1 fA up to 100\npA with 100 aA resolution and is directly traceable to DC voltage, capacitance\nand time units. The principle of the device is based on applying a linear ramp\nvoltage on standard capacitors with values between 1 pF and 1000 pF. These\nstandard capacitors are commercial standard capacitors which are kept in a\ntemperature controlled insulated box. Linear ramp voltage is generated using a\ncommercial DAQ card. Current source device is fully automated with computer\ncontrol and can be used to generate any currents with 100 aA resolution in its\nfull range. The uncertainty is 2.5 mA/A at 1 fA current.\n",
        "query": "power electronics applications",
        "docId": 1145925,
        "score": 0.4237452139024924,
        "userScore":1
    },
    {
        "title": "Novel depletion mode JFET based low static power complementary circuit\n  technology",
        "paperAbstract": "  The lack of an easily realizable complementary circuit technology offering\nlow static power consumption has been limiting the utilization of other\nsemiconductor materials than silicon. In this publication, a novel depletion\nmode JFET based complementary circuit technology is presented and herein after\nreferred to as Complementary Semiconductor (CS) circuit technology. The fact\nthat JFETs are pure semiconductor devices, i.e. a carefully optimized Metal\nOxide Semiconductor (MOS) gate stack is not required, facilitates the\nimplementation of CS circuit technology to many semiconductor materials, like\ne.g. germanium and silicon carbide. Furthermore, when the CS circuit technology\nis idle there are neither conductive paths between nodes that are biased at\ndifferent potentials nor forward biased p-n junctions and thus it enables low\nstatic power consumption. Moreover, the fact that the operation of depletion\nmode JFETs does not necessitate the incorporation of forward biased p-n\njunctions means that CS circuit technology is not limited to wide band-gap\nsemiconductor materials, low temperatures, and/or low voltage spans. In this\npaper the operation of the CS logic is described and proven via simulations.\n",
        "query": "power electronics applications",
        "docId": 1416708,
        "score": 0.42365946603501925,
        "userScore":2
    },
    {
        "title": "Transistor as a Rectifier",
        "paperAbstract": "  Transistor is a three terminal semiconductor device normally used as an\namplifier or as a switch. Here the alternating current (a.c) rectifying\nproperty of the transistor is considered. The ordinary silicon diode exhibits a\nvoltage drop of ~0.6V across its terminals. In this article it is shown that\nthe transistor can be used to build a diode or rectify low current a.c (~mA)\nwith a voltage drop of ~0.03V. This voltage is ~20 times smaller than the\nsilicon diode. This article gives the half-wave and full-wave transistor\nrectifier configurations along with some applications to justify their\nusefulness.\n",
        "query": "power electronics applications",
        "docId": 343605,
        "score": 0.41243311257676396,
        "userScore":1
    },
    {
        "title": "Output stages inside a negative feedback loop: application to a\n  low-voltage three-phase DC-AC converter for educational purposes",
        "paperAbstract": "  The circuit presented in this paper aims at providing three 40 Vpp 50Hz AC\nvoltages sources with 120-degree phase separation between them. This is a fully\nanalogue circuit that uses standard, low-cost electronic components without\nresorting to a microcontroller as previously proposed by Shirvasar et al [1].\nThis circuit may serve as a basis for a low-voltage 3P-AC power supply that\nstudents may safely use to realize experiments, i.e. about the principles and\napplications of three-phase AC power lines, without the risk of electric\nshocks.\n",
        "query": "power electronics applications",
        "docId": 478008,
        "score": 0.4122585137332564,
        "userScore":1
    },
    {
        "title": "Fiscal Stimulus of Last Resort",
        "paperAbstract": "  I examine global dynamics in a monetary model with overlapping generations of\nfinite-horizon agents and a binding lower bound on nominal interest rates. Debt\ntargeting rules exacerbate the possibility of self-fulfilling liquidity traps,\nfor agents expect austerity following deflationary slumps. Conversely, activist\nbut sustainable fiscal policy regimes - implementing intertemporally balanced\ntax cuts and/or transfer increases in response to disinflationary trajectories\n- are capable of escaping liquidity traps and embarking inflation into a\nglobally stable path that converges to the target. Should fiscal stimulus of\nlast resort be overly aggressive, however, spiral dynamics around the\nliquidity-trap steady state exist, causing global indeterminacy.\n",
        "query": "fiscal policy",
        "docId": 1450171,
        "score": 0.5807589093916405,
        "userScore":1
    },
    {
        "title": "Optimal Taxation with Endogenous Default under Incomplete Markets",
        "paperAbstract": "  In a dynamic economy, we characterize the fiscal policy of the government\nwhen it levies distortionary taxes and issues defaultable bonds to finance its\nstochastic expenditure. Default may occur in equilibrium as it prevents the\ngovernment from incurring in future tax distortions that would come along with\nthe service of the debt. Households anticipate the possibility of default\ngenerating endogenous credit limits. These limits hinder the government\u0027s\nability to smooth taxes using debt, implying more volatile and less serially\ncorrelated fiscal policies, higher borrowing costs and lower levels of\nindebtedness. In order to exit temporary financial autarky following a default\nevent, the government has to repay a random fraction of the defaulted debt. We\nshow that the optimal fiscal and renegotiation policies have implications\naligned with the data.\n",
        "query": "fiscal policy",
        "docId": 650580,
        "score": 0.5468064418343985,
        "userScore":1
    },
    {
        "title": "Government Solvency, Austerity and Fiscal Consolidation in the OECD: A\n  Keynesian Appraisal of Transversality and No Ponzi Game Conditions",
        "paperAbstract": "  This paper investigates the relevance of the No-Ponzi game condition for\npublic debt (i.e. the public debt growth rate has to be lower than the real\ninterest rate, a necessary assumption for Ricardian equivalence) and of the\ntransversality condition for the GDP growth rate (i.e. the GDP growth rate has\nto be lower than the real interest rate). First, on the unbalanced panel of 21\ncountries from 1961 to 2010 available in OECD database, those two conditions\nwere simultaneously validated only for 29% of the cases under examination.\nSecond, those two conditions were more frequent in the 1980s and the 1990s when\nmonetary policies were more restrictive. Third, in tune with the Keynesian\nview, when the real interest rate is higher than the GDP growth, it corresponds\nto 75% of the cases of the increases of the debt/GDP ratio but to only 43% of\nthe cases of the decreases of the debt/GDP ratio (fiscal consolidations).\n",
        "query": "fiscal policy",
        "docId": 426463,
        "score": 0.5312091125546701,
        "userScore":1
    },
    {
        "title": "Ramsey Optimal Policy versus Multiple Equilibria with Fiscal and\n  Monetary Interactions",
        "paperAbstract": "  We consider a frictionless constant endowment economy based on Leeper (1991).\nIn this economy, it is shown that, under an ad-hoc monetary rule and an ad-hoc\nfiscal rule, there are two equilibria. One has active monetary policy and\npassive fiscal policy, while the other has passive monetary policy and active\nfiscal policy. We consider an extended setup in which the policy maker\nminimizes a loss function under quasi-commitment, as in Schaumburg and\nTambalotti (2007). Under this formulation there exists a unique Ramsey\nequilibrium, with an interest rate peg and a passive fiscal policy. We thank\nJohn P. Conley, Luis de Araujo and one referree for their very helpful\ncomments.\n",
        "query": "fiscal policy",
        "docId": 1241931,
        "score": 0.49863006343900906,
        "userScore":1
    },
    {
        "title": "Shifting Policy Strategy in Keynesianism",
        "paperAbstract": "  This paper analyzes the evolution of Keynesianism making use of concepts\noffered by Imre Lakatos. The Keynesian \"hard core\" lies in its views regarding\nthe instability of the market economy, its \"protective belt\" in the policy\nstrategy for macroeconomic stabilization using fiscal policy and monetary\npolicy. Keynesianism developed as a policy program to counter classical\nliberalism, which attributes priority to the autonomy of the market economy and\ntries to limit the role of government. In general, the core of every policy\nprogram consists in an unfalsifiable worldview and a value judgment that remain\nunchanged. On the other hand, a policy strategy with a protective belt\ninevitably evolves owing to changes in reality and advances in scientific\nknowledge. This is why the Keynesian policy strategy has shifted from being\nfiscal-led to one that is monetary-led because of the influence of monetarism;\nfurther, the Great Recession has even led to their integration.\n",
        "query": "fiscal policy",
        "docId": 1306263,
        "score": 0.48433465000601217,
        "userScore":1
    },
    {
        "title": "Fiscal policy and inequality in a model with endogenous positional\n  concerns",
        "paperAbstract": "  We investigate the dynamics of wealth inequality in an economy where\nhouseholds have positional preferences, with the strength of the positional\nconcern determined endogenously by inequality of wealth distribution in the\nsociety. We demonstrate that in the long run such an economy converges to a\nunique egalitarian steady-state equilibrium, with all households holding equal\npositive wealth, when the initial inequality is sufficiently low. Otherwise,\nthe steady state is characterised by polarisation of households into rich, who\nown all the wealth, and poor, whose wealth is zero. A fiscal policy with\ngovernment consumption funded by taxes on labour income and wealth can move the\neconomy from any initial state towards an egalitarian equilibrium with a higher\naggregate wealth.\n",
        "query": "fiscal policy",
        "docId": 1494422,
        "score": 0.4685962940302204,
        "userScore":2
    },
    {
        "title": "Social Discounting and the Long Rate of Interest",
        "paperAbstract": "  The well-known theorem of Dybvig, Ingersoll and Ross shows that the long\nzero-coupon rate can never fall. This result, which, although undoubtedly\ncorrect, has been regarded by many as surprising, stems from the implicit\nassumption that the long-term discount function has an exponential tail. We\nrevisit the problem in the setting of modern interest rate theory, and show\nthat if the long \"simple\" interest rate (or Libor rate) is finite, then this\nrate (unlike the zero-coupon rate) acts viably as a state variable, the value\nof which can fluctuate randomly in line with other economic indicators. New\ninterest rate models are constructed, under this hypothesis and certain\ngeneralizations thereof, that illustrate explicitly the good asymptotic\nbehaviour of the resulting discount bond systems. The conditions necessary for\nthe existence of such \"hyperbolic\" and \"generalized hyperbolic\" long rates are\nthose of so-called social discounting, which allow for long-term cash flows to\nbe treated as broadly \"just as important\" as those of the short or medium term.\nAs a consequence, we are able to provide a consistent arbitrage-free valuation\nframework for the cost-benefit analysis and risk management of long-term social\nprojects, such as those associated with sustainable energy, resource\nconservation, and climate change.\n",
        "query": "fiscal policy",
        "docId": 439929,
        "score": 0.4269883137194981,
        "userScore":2
    },
    {
        "title": "Policy Maker\u0027s Credibility with Predetermined Instruments for\n  Forward-Looking Targets",
        "paperAbstract": "  The aim of the present paper is to provide criteria for a central bank of how\nto choose among different monetary-policy rules when caring about a number of\npolicy targets such as the output gap and expected inflation. Special attention\nis given to the question if policy instruments are predetermined or only\nforward looking. Using the new-Keynesian Phillips curve with a cost-push-shock\npolicy-transmission mechanism, the forward-looking case implies an extreme lack\nof robustness and of credibility of stabilization policy. The backward-looking\ncase is such that the simple-rule parameters can be the solution of Ramsey\noptimal policy under limited commitment. As a consequence, we suggest to model\nexplicitly the rational behavior of the policy maker with Ramsey optimal\npolicy, rather than to use simple rules with an ambiguous assumption leading to\npolicy advice that is neither robust nor credible.\n",
        "query": "fiscal policy",
        "docId": 1390684,
        "score": 0.4240164934858228,
        "userScore":0
    },
    {
        "title": "Government Guarantees and Banks\u0027 Income Smoothing",
        "paperAbstract": "  We propose four channels through which government guarantees affect banks\u0027\nincentives to smooth income. Empirically, we exploit two complementary settings\nthat represent plausible exogenous changes in government guarantees: the\nincrease in implicit guarantees following the creation of the Eurozone and the\nremoval of explicit guarantees granted to the Landesbanken. We show that\nincreases (decreases) in government guarantees are associated with significant\ndecreases (increases) in banks\u0027 income smoothing. Taken together, our results\nlargely corroborate the predominance of a tail-risk channel, wherein government\nguarantees reduce banks\u0027 tail risk, thereby reducing managers\u0027 incentives to\nengage in income smoothing.\n",
        "query": "fiscal policy",
        "docId": 1803458,
        "score": 0.4172923032892868,
        "userScore":1
    },
    {
        "title": "Behavioural Macroeconomic Policy: New perspectives on time inconsistency",
        "paperAbstract": "  This paper brings together divergent approaches to time inconsistency from\nmacroeconomic policy and behavioural economics. Behavioural discount functions\nfrom behavioural microeconomics are embedded into a game-theoretic analysis of\ntemptation versus enforcement to construct an encompassing model, nesting\ncombinations of time consistent and time inconsistent preferences. The analysis\npresented in this paper shows that, with hyperbolic/quasihyperbolic\ndiscounting, the enforceable range of inflation targets is narrowed. This\nsuggests limits to the effectiveness of monetary targets, under certain\nconditions. The paper concludes with a discussion of monetary policy\nimplications, explored specifically in the light of current macroeconomic\npolicy debates.\n",
        "query": "fiscal policy",
        "docId": 1152394,
        "score": 0.414893471744044,
        "userScore":1
    },
    {
        "title": "Techniques for Feature Extraction In Speech Recognition System : A\n  Comparative Study",
        "paperAbstract": "  The time domain waveform of a speech signal carries all of the auditory\ninformation. From the phonological point of view, it little can be said on the\nbasis of the waveform itself. However, past research in mathematics, acoustics,\nand speech technology have provided many methods for converting data that can\nbe considered as information if interpreted correctly. In order to find some\nstatistically relevant information from incoming data, it is important to have\nmechanisms for reducing the information of each segment in the audio signal\ninto a relatively small number of parameters, or features. These features\nshould describe each segment in such a characteristic way that other similar\nsegments can be grouped together by comparing their features. There are\nenormous interesting and exceptional ways to describe the speech signal in\nterms of parameters. Though, they all have their strengths and weaknesses, we\nhave presented some of the most used methods with their importance.\n",
        "query": "speech recognition audio signal processing",
        "docId": 428413,
        "score": 0.6523005449554873,
        "userScore":2
    },
    {
        "title": "Speech Recognition by Machine, A Review",
        "paperAbstract": "  This paper presents a brief survey on Automatic Speech Recognition and\ndiscusses the major themes and advances made in the past 60 years of research,\nso as to provide a technological perspective and an appreciation of the\nfundamental progress that has been accomplished in this important area of\nspeech communication. After years of research and development the accuracy of\nautomatic speech recognition remains one of the important research challenges\n(e.g., variations of the context, speakers, and environment).The design of\nSpeech Recognition system requires careful attentions to the following issues:\nDefinition of various types of speech classes, speech representation, feature\nextraction techniques, speech classifiers, database and performance evaluation.\nThe problems that are existing in ASR and the various techniques to solve these\nproblems constructed by various research workers have been presented in a\nchronological order. Hence authors hope that this work shall be a contribution\nin the area of speech recognition. The objective of this review paper is to\nsummarize and compare some of the well known methods used in various stages of\nspeech recognition system and identify research topic and applications which\nare at the forefront of this exciting and challenging field.\n",
        "query": "speech recognition audio signal processing",
        "docId": 167555,
        "score": 0.6449246341989507,
        "userScore":3
    },
    {
        "title": "Conversion of Acoustic Signal (Speech) Into Text By Digital Filter using\n  Natural Language Processing",
        "paperAbstract": "  One of the most crucial aspects of communication in daily life is speech\nrecognition. Speech recognition that is based on natural language processing is\none of the essential elements in the conversion of one system to another. In\nthis paper, we created an interface that transforms speech and other auditory\ninputs into text using a digital filter. Contrary to the many methods for this\nconversion, it is also possible for linguistic faults to appear occasionally,\ngender recognition, speech recognition that is unsuccessful (cannot recognize\nvoice), and gender recognition to fail. Since technical problems are involved,\nwe developed a program that acts as a mediator to prevent initiating software\nissues in order to eliminate even this little deviation. Its planned MFCC and\nHMM are in sync with its AI system. As a result, technical errors have been\navoided.\n",
        "query": "speech recognition audio signal processing",
        "docId": 1709904,
        "score": 0.6123352442281202,
        "userScore":2
    },
    {
        "title": "Voice Recognition Algorithms using Mel Frequency Cepstral Coefficient\n  (MFCC) and Dynamic Time Warping (DTW) Techniques",
        "paperAbstract": "  Digital processing of speech signal and voice recognition algorithm is very\nimportant for fast and accurate automatic voice recognition technology. The\nvoice is a signal of infinite information. A direct analysis and synthesizing\nthe complex voice signal is due to too much information contained in the\nsignal. Therefore the digital signal processes such as Feature Extraction and\nFeature Matching are introduced to represent the voice signal. Several methods\nsuch as Liner Predictive Predictive Coding (LPC), Hidden Markov Model (HMM),\nArtificial Neural Network (ANN) and etc are evaluated with a view to identify a\nstraight forward and effective method for voice signal. The extraction and\nmatching process is implemented right after the Pre Processing or filtering\nsignal is performed. The non-parametric method for modelling the human auditory\nperception system, Mel Frequency Cepstral Coefficients (MFCCs) are utilize as\nextraction techniques. The non linear sequence alignment known as Dynamic Time\nWarping (DTW) introduced by Sakoe Chiba has been used as features matching\ntechniques. Since it\u0027s obvious that the voice signal tends to have different\ntemporal rate, the alignment is important to produce the better\nperformance.This paper present the viability of MFCC to extract features and\nDTW to compare the test patterns.\n",
        "query": "speech recognition audio signal processing",
        "docId": 179890,
        "score": 0.6025929950729942,
        "userScore":2
    },
    {
        "title": "A Wavelet Transform Based Scheme to Extract Speech Pitch and Formant\n  Frequencies",
        "paperAbstract": "  Pitch and Formant frequencies are important features in speech processing\napplications. The period of the vocal cord\u0027s output for vowels is known as the\npitch or the fundamental frequency, and formant frequencies are essentially\nresonance frequencies of the vocal tract. These features vary among different\npersons and even words, but they are within a certain frequency range. In\npractice, just the first three formants are enough for the most of speech\nprocessing. Feature extraction and classification are the main components of\neach speech recognition system. In this article, two wavelet based approaches\nare proposed to extract the mentioned features with help of the filter bank\nidea. By comparing the results of the presented feature extraction methods on\nseveral speech signals, it was found out that the wavelet transform has a good\naccuracy compared to the cepstrum method and it has no sensitivity to noise. In\naddition, several fuzzy based classification techniques for speech processing\nare reviewed.\n",
        "query": "speech recognition audio signal processing",
        "docId": 1706448,
        "score": 0.5972625681898123,
        "userScore":1
    },
    {
        "title": "Application of Kullback-Leibler Metric to Speech Recognition",
        "paperAbstract": "  Article discusses the application of Kullback-Leibler divergence to the\nrecognition of speech signals and suggests three algorithms implementing this\ndivergence criterion: correlation algorithm, spectral algorithm and filter\nalgorithm. Discussion covers an approach to the problem of speech variability\nand is illustrated with the results of experimental modeling of speech signals.\nThe article gives a number of recommendations on the choice of appropriate\nmodel parameters and provides a comparison to some other methods of speech\nrecognition.\n",
        "query": "speech recognition audio signal processing",
        "docId": 1975615,
        "score": 0.5916882609158165,
        "userScore":1
    },
    {
        "title": "An elitist approach for extracting automatically well-realized speech\n  sounds with high confidence",
        "paperAbstract": "  This paper presents an \"elitist approach\" for extracting automatically\nwell-realized speech sounds with high confidence. The elitist approach uses a\nspeech recognition system based on Hidden Markov Models (HMM). The HMM are\ntrained on speech sounds which are systematically well-detected in an iterative\nprocedure. The results show that, by using the HMM models defined in the\ntraining phase, the speech recognizer detects reliably specific speech sounds\nwith a small rate of errors.\n",
        "query": "speech recognition audio signal processing",
        "docId": 1977693,
        "score": 0.585543671437307,
        "userScore":2
    },
    {
        "title": "Algorithm of Segment-Syllabic Synthesis in Speech Recognition Problem",
        "paperAbstract": "  Speech recognition based on the syllable segment is discussed in this paper.\nThe principal search methods in space of states for the speech recognition\nproblem by segment-syllabic parameters trajectory synthesis are investigated.\nRecognition as comparison the parameters trajectories in chosen speech units on\nthe sections of the segmented speech is realized. Some experimental results are\ngiven and discussed.\n",
        "query": "speech recognition audio signal processing",
        "docId": 1979925,
        "score": 0.5849137198973722,
        "userScore":2
    },
    {
        "title": "Speech Recognition of the letter \u0027zha\u0027 in Tamil Language using HMM",
        "paperAbstract": "  Speech signals of the letter \u0027zha\u0027 in Tamil language of 3 males and 3 females\nwere coded using an improved version of Linear Predictive Coding (LPC). The\nsampling frequency was at 16 kHz and the bit rate was at 15450 bits per second,\nwhere the original bit rate was at 128000 bits per second with the help of wave\nsurfer audio tool. The output LPC cepstrum is implemented in first order three\nstate Hidden Markov Model(HMM) chain.\n",
        "query": "speech recognition audio signal processing",
        "docId": 169478,
        "score": 0.583479619485729,
        "userScore":1
    },
    {
        "title": "Minimal Feature Analysis for Isolated Digit Recognition for varying\n  encoding rates in noisy environments",
        "paperAbstract": "  This research work is about recent development made in speech recognition. In\nthis research work, analysis of isolated digit recognition in the presence of\ndifferent bit rates and at different noise levels has been performed. This\nresearch work has been carried using audacity and HTK toolkit. Hidden Markov\nModel (HMM) is the recognition model which was used to perform this experiment.\nThe feature extraction techniques used are Mel Frequency Cepstrum coefficient\n(MFCC), Linear Predictive Coding (LPC), perceptual linear predictive (PLP), mel\nspectrum (MELSPEC), filter bank (FBANK). There were three types of different\nnoise levels which have been considered for testing of data. These include\nrandom noise, fan noise and random noise in real time environment. This was\ndone to analyse the best environment which can used for real time applications.\nFurther, five different types of commonly used bit rates at different sampling\nrates were considered to find out the most optimum bit rate.\n",
        "query": "speech recognition audio signal processing",
        "docId": 1703812,
        "score": 0.5829337907780925,
        "userScore":1
    },
    {
        "title": "COVID-19 Evolves in Human Hosts",
        "paperAbstract": "  Today, we are all threatened by an unprecedented pandemic: COVID-19. How\ndifferent is it from other coronaviruses? Will it be attenuated or become more\nvirulent? Which animals may be its original host? In this study, we collected\nand analyzed nearly thirty thousand publicly available complete genome\nsequences for COVID-19 virus from 79 different countries, the previously known\nflu-causing coronaviruses (HCov-229E, HCov-OC43, HCov-NL63 and HCov-HKU1) and\nthe lethal, pathogenic viruses, SARS, MERS, Victoria, Lassa, Yamagata, Ebola,\nand Dengue. We found strong similarities between the current circulating\nCOVID-19 and SARS and MERS, as well as COVID-19 in rhinolophines and pangolins.\nOn the contrary, COVID-19 shares little similarity with the flu-causing\ncoronaviruses and the other known viruses. Strikingly, we observed that the\ndivergence of COVID-19 strains isolated from human hosts has steadily increased\nfrom December 2019 to May 2020, suggesting COVID-19 is actively evolving in\nhuman hosts. In this paper, we first propose a novel MLCS algorithm NP-MLCS1\nfor the big sequence analysis, which can calculate the common model for\nCOVID-19 complete genome sequences to provide important information for vaccine\nand antibody development. Geographic and time-course analysis of the evolution\ntrees of the human COVID-19 reveals possible evolutional paths among strains\nfrom 79 countries. This finding has important implications to the management of\nCOVID-19 and the development of vaccines and medications.\n",
        "query": "genome sequencing covid-19",
        "docId": 1255935,
        "score": 0.6986154262030411,
        "userScore":3
    },
    {
        "title": "Unlocking capacities of viral genomics for the COVID-19 pandemic\n  response",
        "paperAbstract": "  More than any other infectious disease epidemic, the COVID-19 pandemic has\nbeen characterized by the generation of large volumes of viral genomic data at\nan incredible pace due to recent advances in high-throughput sequencing\ntechnologies, the rapid global spread of SARS-CoV-2, and its persistent threat\nto public health. However, distinguishing the most epidemiologically relevant\ninformation encoded in these vast amounts of data requires substantial effort\nacross the research and public health communities. Studies of SARS-CoV-2\ngenomes have been critical in tracking the spread of variants and understanding\nits epidemic dynamics, and may prove crucial for controlling future epidemics\nand alleviating significant public health burdens. Together, genomic data and\nbioinformatics methods enable broad-scale investigations of the spread of\nSARS-CoV-2 at the local, national, and global scales and allow researchers the\nability to efficiently track the emergence of novel variants, reconstruct\nepidemic dynamics, and provide important insights into drug and vaccine\ndevelopment and disease control. Here, we discuss the tremendous opportunities\nthat genomics offers to unlock the effective use of SARS-CoV-2 genomic data for\nefficient public health surveillance and guiding timely responses to COVID-19.\n",
        "query": "genome sequencing covid-19",
        "docId": 1461423,
        "score": 0.6489428807534523,
        "userScore":2
    },
    {
        "title": "Genotyping coronavirus SARS-CoV-2: methods and implications",
        "paperAbstract": "  The emerging global infectious COVID-19 coronavirus disease by novel Severe\nAcute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) presents critical threats\nto global public health and the economy since it was identified in late\nDecember 2019 in China. The virus has gone through various pathways of\nevolution. For understanding the evolution and transmission of SARS-CoV-2,\ngenotyping of virus isolates is of great importance. We present an accurate\nmethod for effectively genotyping SARS-CoV-2 viruses using complete genomes.\nThe method employs the multiple sequence alignments of the genome isolates with\nthe SARS-CoV-2 reference genome. The SNP genotypes are then measured by Jaccard\ndistances to track the relationship of virus isolates. The genotyping analysis\nof SARS-CoV-2 isolates from the globe reveals that specific multiple mutations\nare the predominated mutation type during the current epidemic. Our method\nserves a promising tool for monitoring and tracking the epidemic of pathogenic\nviruses in their gradual and local genetic variations. The genotyping analysis\nshows that the genes encoding the S proteins and RNA polymerase, RNA primase,\nand nucleoprotein, undergo frequent mutations. These mutations are critical for\nvaccine development in disease control.\n",
        "query": "genome sequencing covid-19",
        "docId": 1261320,
        "score": 0.6080489378371521,
        "userScore":2
    },
    {
        "title": "Interactive SARS-CoV-2 mutation timemaps",
        "paperAbstract": "  As the year 2020 draws to an end, several new strains have been reported for\nthe SARS-CoV-2 coronavirus, the agent responsible for the COVID-19 pandemic\nthat has afflicted us all this past year. However, it is difficult to\ncomprehend the scale, in sequence space, geographical location and time, at\nwhich SARS-CoV-2 mutates and evolves in its human hosts. To get an appreciation\nfor the rapid evolution of the coronavirus, we built interactive scalable\nvector graphics maps that show daily nucleotide variations in genomes from the\nsix most populated continents compared to that of the initial, ground-zero\nSARS-CoV-2 isolate sequenced at the beginning of the year. Availability:\nMutation time maps are available from https://bcgsc.github.io/SARS2/\n",
        "query": "genome sequencing covid-19",
        "docId": 1403575,
        "score": 0.6044193495798709,
        "userScore":2
    },
    {
        "title": "COVID-Datathon: Biomarker identification for COVID-19 severity based on\n  BALF scRNA-seq data",
        "paperAbstract": "  The severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) emergence\nbegan in late 2019 and has since spread rapidly worldwide. The characteristics\nof respiratory immune response to this emerging virus is not clear. Recently,\nSingle-cell RNA sequencing (scRNA-seq) transcriptome profiling of\nBronchoalveolar lavage fluid (BALF) cells has been done to elucidate the\npotential mechanisms underlying in COVID-19. With the aim of better utilizing\nthis atlas of BALF cells in response to the virus, here we propose a\nbioinformatics pipeline to identify candidate biomarkers of COVID-19 severity,\nwhich may help characterize BALF cells to have better mechanistic understanding\nof SARS-CoV-2 infection. The proposed pipeline is implemented in R and is\navailable at https://github.com/namini94/scBALF_Hackathon.\n",
        "query": "genome sequencing covid-19",
        "docId": 1543129,
        "score": 0.5869837996344653,
        "userScore":3
    },
    {
        "title": "Topological data analysis identifies emerging adaptive mutations in\n  SARS-CoV-2",
        "paperAbstract": "  The COVID-19 pandemic has initiated an unprecedented worldwide effort to\ncharacterize its evolution through the mapping of mutations of the coronavirus\nSARS-CoV-2. The early identification of mutations that could confer adaptive\nadvantages to the virus, such as higher infectivity or immune evasion, is of\nparamount importance. However, the large number of currently available genomes\nprecludes the efficient use of phylogeny-based methods. Here we establish a\nfast and scalable early warning system based on Topological Data Analysis for\nthe identification and surveillance of emerging adaptive mutations in large\ngenomic datasets. Analyzing millions of SARS-CoV-2 genomes from GISAID, we\ndemonstrate that topologically salient mutations are linked with an increase in\ninfectivity or immune escape. We report on emerging potentially adaptive\nmutations as of January 2022, and pinpoint mutations in Variants of Concern\nthat are likely due to convergent evolution. Our approach can improve the\nsurveillance of mutations of concern, guide experimental studies, and aid\nvaccine development.\n",
        "query": "genome sequencing covid-19",
        "docId": 1485056,
        "score": 0.5854416435099381,
        "userScore":2
    },
    {
        "title": "Unexpected novel Merbecovirus discoveries in agricultural sequencing\n  datasets from Wuhan, China",
        "paperAbstract": "  In this study we document the unexpected discovery of multiple coronaviruses\nand a BSL-3 pathogen in agricultural cotton and rice sequencing datasets. In\nparticular, we have identified a novel HKU5-related Merbecovirus in a cotton\ndataset sequenced by the Huazhong Agricultural University in 2017. We have also\nfound an infectious clone sequence containing a novel HKU4-related Merbecovirus\nrelated to MERS coronavirus in a rice dataset sequenced by the Huazhong\nAgricultural University in early 2020. Another HKU5-related Merbecovirus, as\nwell as Japanese encephalitis virus, were identified in a cotton dataset\nsequenced by the Huazhong Agricultural University in 2018. An HKU3-related\nBetacoronavirus was found in a Mus musculus sequencing dataset from the Wuhan\nInstitute of Virology in 2017. Finally, a SARS-WIV1-like Betacoronavirus was\nfound in a rice dataset sequenced by the Fujian Agriculture and Forestry\nUniversity in 2017. Using the contaminating reads we have extracted from the\nabove datasets, we were able to assemble complete genomes of two novel\ncoronaviruses which we disclose herein. In light of our findings, we raise\nconcerns about biosafety protocol breaches, as indicated by our discovery of\nmultiple dangerous human pathogens in agricultural sequencing laboratories in\nWuhan and Fouzou City, China.\n",
        "query": "genome sequencing covid-19",
        "docId": 1448951,
        "score": 0.5740314491194738,
        "userScore":1
    },
    {
        "title": "COVID-19 Docking Server: A meta server for docking small molecules,\n  peptides and antibodies against potential targets of COVID-19",
        "paperAbstract": "  Motivation: The coronavirus disease 2019 (COVID-19) caused by a new type of\ncoronavirus has been emerging from China and led to thousands of death globally\nsince December 2019. Despite many groups have engaged in studying the newly\nemerged virus and searching for the treatment of COVID-19, the understanding of\nthe COVID-19 target-ligand interactions represents a key chal-lenge. Herein, we\nintroduce COVID-19 Docking Server, a web server that predicts the binding modes\nbetween COVID-19 targets and the ligands including small molecules, peptides\nand anti-bodies. Results: Structures of proteins involved in the virus life\ncycle were collected or constructed based on the homologs of coronavirus, and\nprepared ready for docking. The meta platform provides a free and interactive\ntool for the prediction of COVID-19 target-ligand interactions and following\ndrug discovery for COVID-19.\n",
        "query": "genome sequencing covid-19",
        "docId": 1250518,
        "score": 0.5732636393213326,
        "userScore":1
    },
    {
        "title": "Reads2Vec: Efficient Embedding of Raw High-Throughput Sequencing Reads\n  Data",
        "paperAbstract": "  The massive amount of genomic data appearing for SARS-CoV-2 since the\nbeginning of the COVID-19 pandemic has challenged traditional methods for\nstudying its dynamics. As a result, new methods such as Pangolin, which can\nscale to the millions of samples of SARS-CoV-2 currently available, have\nappeared. Such a tool is tailored to take as input assembled, aligned and\ncurated full-length sequences, such as those found in the GISAID database. As\nhigh-throughput sequencing technologies continue to advance, such assembly,\nalignment and curation may become a bottleneck, creating a need for methods\nwhich can process raw sequencing reads directly.\n  In this paper, we propose Reads2Vec, an alignment-free embedding approach\nthat can generate a fixed-length feature vector representation directly from\nthe raw sequencing reads without requiring assembly. Furthermore, since such an\nembedding is a numerical representation, it may be applied to highly optimized\nclassification and clustering algorithms. Experiments on simulated data show\nthat our proposed embedding obtains better classification results and better\nclustering properties contrary to existing alignment-free baselines. In a study\non real data, we show that alignment-free embeddings have better clustering\nproperties than the Pangolin tool and that the spike region of the SARS-CoV-2\ngenome heavily informs the alignment-free clusterings, which is consistent with\ncurrent biological knowledge of SARS-CoV-2.\n",
        "query": "genome sequencing covid-19",
        "docId": 1747173,
        "score": 0.5549845318245001,
        "userScore":2
    },
    {
        "title": "COVID-19 Diagnostics: Past, Present, and Future",
        "paperAbstract": "  In winter of 2020, SARS-CoV-2 emerged as a global threat, impacting not only\nhealth but also financial and political stability. To address the societal need\nfor monitoring the spread of SARS-CoV-2, many existing diagnostic technologies\nwere quickly adapted to detect SARS-CoV-2 RNA and antigens as well as the\nimmune response and new testing strategies were developed to accelerate\ntime-to-decision. In parallel, the infusion of research support accelerated the\ndevelopment of new spectroscopic methods. While these methods have\nsignificantly reduced the impact of SARS-CoV-2 on society when coupled with\nbehavioral changes, they also lay the groundwork for a new generation of\nplatform technologies. With several epidemics on the horizon, such as the rise\nof antibiotic-resistant bacteria, the ability to quickly pivot the target\npathogen of this diagnostic toolset will continue to have an impact.\n",
        "query": "genome sequencing covid-19",
        "docId": 1526153,
        "score": 0.5541027168271437,
        "userScore":2
    },
    {
        "title": "Bootstrap inference for the finite population total under complex\n  sampling designs",
        "paperAbstract": "  Bootstrap is a useful tool for making statistical inference, but it may\nprovide erroneous results under complex survey sampling. Most studies about\nbootstrap-based inference are developed under simple random sampling and\nstratified random sampling. In this paper, we propose a unified bootstrap\nmethod applicable to some complex sampling designs, including Poisson sampling\nand probability-proportional-to-size sampling. Two main features of the\nproposed bootstrap method are that studentization is used to make inference,\nand the finite population is bootstrapped based on a multinomial distribution\nby incorporating the sampling information. We show that the proposed bootstrap\nmethod is second-order accurate using the Edgeworth expansion. Two simulation\nstudies are conducted to compare the proposed bootstrap method with the\nWald-type method, which is widely used in survey sampling. Results show that\nthe proposed bootstrap method is better in terms of coverage rate especially\nwhen sample size is limited.\n",
        "query": "different sampling methods",
        "docId": 1071192,
        "score": 0.6095643746424013,
        "userScore":3
    },
    {
        "title": "Coupling methods for multistage sampling",
        "paperAbstract": "  Multistage sampling is commonly used for household surveys when there exists\nno sampling frame, or when the population is scattered over a wide area.\nMultistage sampling usually introduces a complex dependence in the selection of\nthe final units, which makes asymptotic results quite difficult to prove. In\nthis work, we consider multistage sampling with simple random without\nreplacement sampling at the first stage, and with an arbitrary sampling design\nfor further stages. We consider coupling methods to link this sampling design\nto sampling designs where the primary sampling units are selected\nindependently. We first generalize a method introduced by [Magyar Tud. Akad.\nMat. Kutat\\\u0027{o} Int. K\\\"{o}zl. 5 (1960) 361-374] to get a coupling with\nmultistage sampling and Bernoulli sampling at the first stage, which leads to a\ncentral limit theorem for the Horvitz--Thompson estimator. We then introduce a\nnew coupling method with multistage sampling and simple random with replacement\nsampling at the first stage. When the first-stage sampling fraction tends to\nzero, this method is used to prove consistency of a with-replacement bootstrap\nfor simple random without replacement sampling at the first stage, and\nconsistency of bootstrap variance estimators for smooth functions of totals.\n",
        "query": "different sampling methods",
        "docId": 678517,
        "score": 0.6043811236218897,
        "userScore":2
    },
    {
        "title": "Consistent Sampling with Replacement",
        "paperAbstract": "  We describe a very simple method for `consistent sampling\u0027 that allows for\nsampling with replacement. The method extends previous approaches to consistent\nsampling, which assign a pseudorandom real number to each element, and sample\nthose with the smallest associated numbers. When sampling with replacement, our\nextension gives the item sampled a new, larger, associated pseudorandom number,\nand returns it to the pool of items being sampled.\n",
        "query": "different sampling methods",
        "docId": 1019162,
        "score": 0.6027005410041095,
        "userScore":2
    },
    {
        "title": "Sharp exponential inequalities in survey sampling: conditional Poisson\n  sampling schemes",
        "paperAbstract": "  This paper is devoted to establishing exponential bounds for the\nprobabilities of deviation of a sample sum from its expectation, when the\nvariables involved in the summation are obtained by sampling in a finite\npopulation according to a rejective scheme, generalizing sampling without\nreplacement, and by using an appropriate normalization. In contrast to Poisson\nsampling, classical deviation inequalities in the i.i.d. setting do not\nstraightforwardly apply to sample sums related to rejective schemes, due to the\ninherent dependence structure of the sampled points. We show here how to\novercome this difficulty, by combining the formulation of rejective sampling as\nPoisson sampling conditioned upon the sample size with the Escher\ntransformation. In particular, the Bennett/Bernstein type bounds established\nhighlight the effect of the asymptotic variance of the (properly standardized)\nsample weighted sum and are shown to be much more accurate than those based on\nthe negative association property shared by the terms involved in the\nsummation. Beyond its interest in itself, such a result for rejective sampling\nis crucial, insofar as it can be extended to many other sampling schemes,\nnamely those that can be accurately approximated by rejective plans in the\nsense of the total variation distance.\n",
        "query": "different sampling methods",
        "docId": 779123,
        "score": 0.5933409890168502,
        "userScore":2
    },
    {
        "title": "Determinantal Sampling Designs",
        "paperAbstract": "  In this article, recent results about point processes are used in sampling\ntheory. Precisely, we define and study a new class of sampling designs:\ndeterminantal sampling designs. The law of such designs is known, and there\nexists a simple selection algorithm. We compute exactly the variance of linear\nestimators constructed upon these designs by using the first and second order\ninclusion probabilities. Moreover, we obtain asymptotic and finite sample\ntheorems. We construct explicitly fixed size determinantal sampling designs\nwith given first order inclusion probabilities. We also address the search of\noptimal determinantal sampling designs.\n",
        "query": "different sampling methods",
        "docId": 670575,
        "score": 0.5908117082838356,
        "userScore":2
    },
    {
        "title": "Properties of Chromy\u0027s sampling procedure",
        "paperAbstract": "  Chromy (1979) proposed a unequal probability sampling algorithm, which\nenables to select a sample in one pass of the sampling frame only. This is the\ndefault sequential method used in the SURVEYSELECT procedure of the SAS\nsoftware. In this article, we study the properties of Chromy sampling. We prove\nthat the Horvitz-Thompson is asymptotically normally distributed, and give an\nexplicit expression for the second-order inclusion probabilities. This makes it\npossible to estimate the variance unbiasedly for the randomized version of the\nmethod programmed in the SURVEYSELECT procedure.\n",
        "query": "different sampling methods",
        "docId": 1222792,
        "score": 0.5870949540651491,
        "userScore":1
    },
    {
        "title": "Fundamentals of Partial Rejection Sampling",
        "paperAbstract": "  Partial Rejection Sampling is an algorithmic approach to obtaining a perfect\nsample from a specified distribution. The objects to be sampled are assumed to\nbe represented by a number of random variables. In contrast to classical\nrejection sampling, in which all variables are resampled until a feasible\nsolution is found, partial rejection sampling aims at greater efficiency by\nresampling only a subset of variables that `go wrong\u0027. Partial rejection\nsampling is closely related to Moser and Tardos\u0027 algorithmic version of the\nLov\\\u0027asz Local Lemma, but with the additional requirement that a specified\noutput distribution should be met. This article provides a largely\nself-contained account of the basic form of the algorithm and its analysis.\n",
        "query": "different sampling methods",
        "docId": 1485508,
        "score": 0.5826815984954621,
        "userScore":1
    },
    {
        "title": "Probability Sampling Designs: Principles for Choice of Design and\n  Balancing",
        "paperAbstract": "  The aim of this paper is twofold. First, three theoretical principles are\nformalized: randomization, overrepresentation and restriction. We develop these\nprinciples and give a rationale for their use in choosing the sampling design\nin a systematic way. In the model-assisted framework, knowledge of the\npopulation is formalized by modelling the population and the sampling design is\nchosen accordingly. We show how the principles of overrepresentation and of\nrestriction naturally arise from the modelling of the population. The balanced\nsampling then appears as a consequence of the modelling. Second, a review of\nprobability balanced sampling is presented through the model-assisted\nframework. For some basic models, balanced sampling can be shown to be an\noptimal sampling design. Emphasis is placed on new spatial sampling methods and\ntheir related models. An illustrative example shows the advantages of the\ndifferent methods. Throughout the paper, various examples illustrate how the\nthree principles can be applied in order to improve inference.\n",
        "query": "different sampling methods",
        "docId": 800774,
        "score": 0.581820347583168,
        "userScore":3
    },
    {
        "title": "Inference for two-stage sampling designs with application to a panel for\n  urban policy",
        "paperAbstract": "  Two-stage sampling designs are commonly used for household and health\nsurveys. To produce reliable estimators with assorted confidence intervals,\nsome basic statistical properties like consistency and asymptotic normality of\nthe Horvitz-Thompson estimator are desirable, along with the consistency of\nassorted variance estimators. These properties have been mainly studied for\nsingle-stage sampling designs. In this work, we prove the consistency of the\nHorvitz-Thompson estimator and of associated variance estimators for a general\nclass of two-stage sampling designs, under mild assumptions. We also study\ntwo-stage sampling with a large entropy sampling design at the first stage, and\nprove that the Horvitz-Thompson estimator is asymptotically normally\ndistributed through a coupling argument. When the first-stage sampling fraction\nis negligible, simplified variance estimators which do not require estimating\nthe variance within the Primary Sampling Units are proposed, and shown to be\nconsistent. An application to a panel for urban policy, which is the initial\nmotivation for this work, is also presented.\n",
        "query": "different sampling methods",
        "docId": 1018904,
        "score": 0.5744822905780875,
        "userScore":1
    },
    {
        "title": "A Proximal Algorithm for Sampling",
        "paperAbstract": "  We study sampling problems associated with potentials that lack smoothness.\nThe potentials can be either convex or non-convex. Departing from the standard\nsmooth setting, the potentials are only assumed to be weakly smooth or\nnon-smooth, or the summation of multiple such functions. We develop a sampling\nalgorithm that resembles proximal algorithms in optimization for this\nchallenging sampling task. Our algorithm is based on a special case of Gibbs\nsampling known as the alternating sampling framework (ASF). The key\ncontribution of this work is a practical realization of the ASF based on\nrejection sampling for both non-convex and convex potentials that are not\nnecessarily smooth. In almost all the cases of sampling considered in this\nwork, our proximal sampling algorithm achieves better complexity than all\nexisting methods.\n",
        "query": "different sampling methods",
        "docId": 1612799,
        "score": 0.5732375381553254,
        "userScore":2
    },
    {
        "title": "The Enigma of Entropy",
        "paperAbstract": "  This is a light-hearted take at the the second law of thermodynamics.\n",
        "query": "entropy thermodynamics",
        "docId": 398482,
        "score": 0.7018182394264656,
        "userScore":0
    },
    {
        "title": "The different paths to entropy",
        "paperAbstract": "  In order to undestand how the complex concept of entropy emerged,we propose a\ntrip towards the past reviewing the works of Clausius, Boltzmann, Gibbs and\nPlanck. In particular, since the Gibbs\u0027s work is not very well known, we\npresent a detailed analysis, recalling the three definitions of the entropy\nthat Gibbs gives. May be one of the most important aspect of the entropy is to\nsee it as a thermodynamic potential like the other thermodynamic potentials as\nproposed by Callen. We close with some remarks on entropy and irreversibility.\n",
        "query": "entropy thermodynamics",
        "docId": 369432,
        "score": 0.6956268163788746,
        "userScore":2
    },
    {
        "title": "Entropy is a Mathematical Formula",
        "paperAbstract": "  The microscopic explanation of entropy has been challenged from both\nexperimental and theoretical point of view. The expression of entropy is\nderived from the first law of thermodynamics indicating that entropy or the\nsecond law of thermodynamics is not an independent law.\n",
        "query": "entropy thermodynamics",
        "docId": 2189965,
        "score": 0.6890232696043483,
        "userScore":1
    },
    {
        "title": "Possible extended forms of thermodynamic entropy",
        "paperAbstract": "  Thermodynamic entropy is determined by a heat measurement through the\nClausius equality. The entropy then formalizes a fundamental limitation of\noperations by the second law of thermodynamics. The entropy is also expressed\nas the Shannon entropy of the microscopic degrees of freedom. Whenever an\nextension of thermodynamic entropy is attempted, we must pay special attention\nto how its three different aspects just mentioned are altered. In this paper,\nwe discuss possible extensions of the thermodynamic entropy.\n",
        "query": "entropy thermodynamics",
        "docId": 464139,
        "score": 0.6844728140656837,
        "userScore":2
    },
    {
        "title": "Second law, entropy production, and reversibility in thermodynamics of\n  information",
        "paperAbstract": "  We present a pedagogical review of the fundamental concepts in thermodynamics\nof information, by focusing on the second law of thermodynamics and the entropy\nproduction. Especially, we discuss the relationship among thermodynamic\nreversibility, logical reversibility, and heat emission in the context of the\nLandauer principle and clarify that these three concepts are fundamentally\ndistinct to each other. We also discuss thermodynamics of measurement and\nfeedback control by Maxwell\u0027s demon. We clarify that the demon and the second\nlaw are indeed consistent in the measurement and the feedback processes\nindividually, by including the mutual information to the entropy production.\n",
        "query": "entropy thermodynamics",
        "docId": 925457,
        "score": 0.6617341852964937,
        "userScore":3
    },
    {
        "title": "On entropy in eulerian thermodynamics",
        "paperAbstract": "  To the student of thermodynamics the most difficult subject is entropy. In\nthis paper we examine the actual, practical application of entropy to two\nsimple systems, the homogeneous slab with fixed boundary values of the\ntemperature, and an isolated atmosphere in the presence of the static\ngravitational field. The first gives valuable insight into the nature of\nentropy that is subsequently applied to the second system.\n  It is a basic tenet of thermodynamics that the equilibrium of an extended,\nhomogeneous and isolated system is characterized by a uniform temperature\ndistribution and it is a strongly held belief that this remains true in the\npresence of gravity. We find that this is consistent with the equations of\nextended thermodynamics but that entropy enters in an essential way. The\nprinciple of equivalence takes on a new aspect.\n",
        "query": "entropy thermodynamics",
        "docId": 267908,
        "score": 0.6604547296100236,
        "userScore":3
    },
    {
        "title": "Generalization of Gibbs Entropy and Thermodynamic Relation",
        "paperAbstract": "  In this paper, we extend Gibbs\u0027s approach of quasi-equilibrium thermodynamic\nprocesses, and calculate the microscopic expression of entropy for general\nnon-equilibrium thermodynamic processes. Also, we analyze the formal structure\nof thermodynamic relation in non-equilibrium thermodynamic processes.\n",
        "query": "entropy thermodynamics",
        "docId": 202922,
        "score": 0.6449025330522247,
        "userScore":3
    },
    {
        "title": "A Fresh Look at Entropy and the Second Law of Thermodynamics",
        "paperAbstract": "  This paper is a non-technical, informal presentation of our theory of the\nsecond law of thermodynamics as a law that is independent of statistical\nmechanics and that is derivable solely from certain simple assumptions about\nadiabatic processes for macroscopic systems. It is not necessary to assume\na-priori concepts such as \"heat\", \"hot and cold\", \"temperature\". These are\nderivable from entropy, whose existence we derive from the basic assumptions.\nSee cond-mat/9708200 and math-ph/9805005.\n",
        "query": "entropy thermodynamics",
        "docId": 2109561,
        "score": 0.6425060168185155,
        "userScore":3
    },
    {
        "title": "Quantitative Calculations of Decrease of Entropy in Thermodynamics of\n  Microstructure and Sufficient-Necessary Condition of Decrease of Entropy in\n  Isolated System",
        "paperAbstract": "  Firstly, we calculate quantitatively decrease of entropy by the known\nformulas in the ordering phenomena and nucleation of thermodynamics of\nmicrostructure. They show again that a necessary condition of decrease of\nentropy in isolated system is existence of internal interactions. Further,\nsufficient and necessary condition of decrease of entropy is also discussed\nquantitatively. Then some possible decreases of entropy are researched. A\ncomplete symmetrical structure on change of entropy is obtained. The analysis\nfor many experiments and theories shows that the second law of the\nthermodynamics should be developed.\n",
        "query": "entropy thermodynamics",
        "docId": 121649,
        "score": 0.6412774741573752,
        "userScore":1
    },
    {
        "title": "Heat and Entropy in nonextensive thermodynamics",
        "paperAbstract": "  The concepts of quantity of heat and work are deduced in the non-extensive\nstatistical mechanics context, following steps in parallel to those employed in\nthe extensive statistical mechanics.\n",
        "query": "entropy thermodynamics",
        "docId": 1915775,
        "score": 0.6385653367147412,
        "userScore":2
    },
    {
        "title": "New high-tech flexible networks for the monitoring of deep-sea\n  ecosystems",
        "paperAbstract": "  Increasing interest in the acquisition of biotic and abiotic resources from\nwithin the deep sea (e.g. fisheries, oil-gas extraction, and mining) urgently\nimposes the development of novel monitoring technologies, beyond the\ntraditional vessel-assisted, time-consuming, high-cost sampling surveys. The\nimplementation of permanent networks of seabed and water-column cabled (fixed)\nand docked mobile platforms is presently enforced, to cooperatively measure\nbiological features and environmental (physico-chemical) parameters. Video and\nacoustic (i.e. optoacoustic) imaging are becoming central approaches for\nstudying benthic fauna (e.g. quantifying species presence, behaviour, and\ntrophic interactions) in a remote, continuous, and prolonged fashion. Imaging\nis also being complemented by in situ environmental-DNA sequencing\ntechnologies, allowing the traceability of a wide range of organisms (including\nprokaryotes) beyond the reach of optoacoustic tools. Here, we describe the\ndifferent fixed and mobile platforms of those benthic and pelagic monitoring\nnetworks, proposing at the same time an innovative roadmap for the automated\ncomputing of hierarchical ecological information of deep-sea ecosystems (i.e.\nfrom single species abundance and life traits, to community composition, and\noverall biodiversity)\n",
        "query": "great barrier reef",
        "docId": 1591016,
        "score": 0.34378015240974236,
        "userScore":2
    },
    {
        "title": "FGSD: A Dataset for Fine-Grained Ship Detection in High Resolution\n  Satellite Images",
        "paperAbstract": "  Ship detection using high-resolution remote sensing images is an important\ntask, which contribute to sea surface regulation. The complex background and\nspecial visual angle make ship detection relies in high quality datasets to a\ncertain extent. However, there is few works on giving both precise\nclassification and accurate location of ships in existing ship detection\ndatasets. To further promote the research of ship detection, we introduced a\nnew fine-grained ship detection datasets, which is named as FGSD. The dataset\ncollects high-resolution remote sensing images that containing ship samples\nfrom multiple large ports around the world. Ship samples were fine categorized\nand annotated with both horizontal and rotating bounding boxes. To further\ndetailed the information of the dataset, we put forward a new representation\nmethod of ships\u0027 orientation. For future research, the dock as a new class was\nannotated in the dataset. Besides, rich information of images were provided in\nFGSD, including the source port, resolution and corresponding GoogleEarth\u0027 s\nresolution level of each image. As far as we know, FGSD is the most\ncomprehensive ship detection dataset currently and it\u0027ll be available soon.\nSome baselines for FGSD are also provided in this paper.\n",
        "query": "great barrier reef",
        "docId": 1257187,
        "score": 0.33145130484000873,
        "userScore":1
    },
    {
        "title": "Big Plastic Masses Detection using Sentinel 2 Images",
        "paperAbstract": "  This communication describes a preliminary research on detection of big\nmasses of plastic (marine litter) on the oceans and seas using EO (Earth\nObservation) satellite systems. Free images from the Sentinel 2 (Copernicus\nProject) platform are used. To develop a plastic recognizer, we start with an\nimage where we can find a big accumulation of \"nonfloating\" plastic: Almer\\\u0027ia\ngreenhouses. We made a test using remote sensing differential indexes, but we\ngot much better results using all available wavelengths (thirteen frequency\nbands) and applying Neural Networks to that feature vector.\n",
        "query": "great barrier reef",
        "docId": 1439705,
        "score": 0.32225206732282885,
        "userScore":1
    },
    {
        "title": "Feature Space Exploration For Planning Initial Benthic AUV Surveys",
        "paperAbstract": "  Special-purpose Autonomous Underwater Vehicles (AUVs) are utilised for\nbenthic (seafloor) surveys, where the vehicle collects optical imagery of near\nthe seafloor. Due to the small-sensor footprint of the cameras and the vast\nareas to be surveyed, these AUVs can not feasibly full coverage of areas larger\nthan a few tens of thousands of square meters. Therefore AUV paths which sample\nsparsely, yet effectively, the survey areas are necessary. Broad scale acoustic\nbathymetric data is ready available over large areas, and often is a useful\nprior of seafloor cover. As such, prior bathymetry can be used to guide AUV\ndata collection. This research proposes methods for planning initial AUV\nsurveys that efficiently explore a feature space representation of the\nbathymetry, in order to sample from a diverse set of bathymetric terrain. This\nwill enable the AUV to visit areas that likely contain unique habitats and are\nrepresentative of the entire survey site. The suitability of these methods to\nplan AUV surveys is evaluated based on the coverage of the feature space and\nalso the ability to visit all classes of benthic habitat on the initial dive.\nThis is a valuable tool for AUV surveys as it increases the utility of initial\ndives. It also delivers a comprehensive training set to learn a relationship\nbetween acoustic bathymetry and visually-derived seafloor classifications.\n",
        "query": "great barrier reef",
        "docId": 1474157,
        "score": 0.3113813737281532,
        "userScore":1
    },
    {
        "title": "A shark in the stars: astronomy and culture in the Torres Strait",
        "paperAbstract": "  Technology has, without doubt, expanded our understanding of space. The\nVoyager 1 space probe is on the brink of leaving our solar system. Massive\ntelescopes have discovered blasts of fast radio bursts from 10 billion light\nyears away. And after a decade on Mars, a Rover recently found evidence for an\nearly ocean on the Red Planet.\n  But with every new advance, it\u0027s also important to remember the science of\nastronomy has existed for thousands of years and forms a vital part of\nIndigenous Australian culture, even today. As an example, let\u0027s explore the\nastronomy of the Torres Strait Islanders, an Indigenous Australian people\nliving between the tip of Cape York and Papua New Guinea.\n",
        "query": "great barrier reef",
        "docId": 579763,
        "score": 0.2994530183967754,
        "userScore":0
    },
    {
        "title": "Towards More Efficient EfficientDets and Low-Light Real-Time Marine\n  Debris Detection",
        "paperAbstract": "  Marine debris is a problem both for the health of marine environments and for\nthe human health since tiny pieces of plastic called \"microplastics\" resulting\nfrom the debris decomposition over the time are entering the food chain at any\nlevels. For marine debris detection and removal, autonomous underwater vehicles\n(AUVs) are a potential solution. In this letter, we focus on the efficiency of\nAUV vision for real-time and low-light object detection. First, we improved the\nefficiency of a class of state-of-the-art object detectors, namely\nEfficientDets, by 1.5% AP on D0, 2.6% AP on D1, 1.2% AP on D2 and 1.3% AP on D3\nwithout increasing the GPU latency. Subsequently, we created and made publicly\navailable a dataset for the detection of in-water plastic bags and bottles and\ntrained our improved EfficientDets on this and another dataset for marine\ndebris detection. Finally, we investigated how the detector performance is\naffected by low-light conditions and compared two low-light underwater image\nenhancement strategies both in terms of accuracy and latency. Source code and\ndataset are publicly available.\n",
        "query": "great barrier reef",
        "docId": 1620017,
        "score": 0.2873892191641687,
        "userScore":1
    },
    {
        "title": "Robustly Removing Deep Sea Lighting Effects for Visual Mapping of\n  Abyssal Plains",
        "paperAbstract": "  The majority of Earth\u0027s surface lies deep in the oceans, where no surface\nlight reaches. Robots diving down to great depths must bring light sources that\ncreate moving illumination patterns in the darkness, such that the same 3D\npoint appears with different color in each image. On top, scattering and\nattenuation of light in the water makes images appear foggy and typically\nblueish, the degradation depending on each pixel\u0027s distance to its observed\nseafloor patch, on the local composition of the water and the relative poses\nand cones of the light sources. Consequently, visual mapping, including image\nmatching and surface albedo estimation, severely suffers from the effects that\nco-moving light sources produce, and larger mosaic maps from photos are often\ndominated by lighting effects that obscure the actual seafloor structure. In\nthis contribution a practical approach to estimating and compensating these\nlighting effects on predominantly homogeneous, flat seafloor regions, as can be\nfound in the Abyssal plains of our oceans, is presented. The method is\nessentially parameter-free and intended as a preprocessing step to facilitate\nvisual mapping, but already produces convincing lighting artefact compensation\nup to a global white balance factor. It does not require to be trained\nbeforehand on huge sets of annotated images, which are not available for the\ndeep sea. Rather, we motivate our work by physical models of light propagation,\nperform robust statistics-based estimates of additive and multiplicative\nnuisances that avoid explicit parameters for light, camera, water or scene,\ndiscuss the breakdown point of the algorithms and show results on imagery\ncaptured by robots in several kilometer water depth.\n",
        "query": "great barrier reef",
        "docId": 1538623,
        "score": 0.2640464460680221,
        "userScore":1
    },
    {
        "title": "Exploring the Universe via the Wide, Deep Near-infrared Imaging ESO\n  Public Survey SHARKS",
        "paperAbstract": "  The ESO Public Survey Southern H-ATLAS Regions Ks-band Survey (SHARKS)\ncomprises 300 square degrees of deep imaging at 2.2 microns (the Ks band) with\nthe VISTA InfraRed CAMera (VIRCAM) at the 4-metre Visible and Infrared Survey\nTelescope for Astronomy (VISTA). The first data release of the survey,\ncomprising 5% of the data, was published via the ESO database on 31 January\n2022. We describe the strategy and status of the first data release and present\nthe data products. We discuss briefly different scientific areas being explored\nwith the SHARKS data and conclude with an outline of planned data releases.\n",
        "query": "great barrier reef",
        "docId": 1765648,
        "score": 0.2591313516577145,
        "userScore":0
    },
    {
        "title": "The Effelsberg-Bonn HI Survey (EBHIS)",
        "paperAbstract": "  The Effelsberg-Bonn HI survey (EBHIS) comprises an all-sky survey north of\nDec \u003d -5 degrees of the Milky Way and the local volume out to a red-shift of z\n~ 0.07. Using state of the art Field Programmable Gate Array (FPGA)\nspectrometers it is feasible to cover the 100 MHz bandwidth with 16.384\nspectral channels. High speed storage of HI spectra allows us to minimize the\ndegradation by Radio Frequency Interference (RFI) signals. Regular EBHIS survey\nobservations started during the winter season 2008/2009 after extensive system\nevaluation and verification tests. Until today, we surveyed about 8000 square\ndegrees, focusing during the first all-sky coverage of the Sloan-Digital Sky\nSurvey (SDSS) area and the northern extension of the Magellanic stream. The\nfirst whole sky coverage will be finished in 2011. Already this first coverage\nwill reach the same sensitivity level as the Parkes Milky Way (GASS) and\nextragalactic surveys (HIPASS). EBHIS data will be calibrated, stray-radiation\ncorrected and freely accessible for the scientific community via a\nweb-interface. In this paper we demonstrate the scientific data quality and\nexplore the expected harvest of this new all-sky survey.\n",
        "query": "great barrier reef",
        "docId": 254739,
        "score": 0.2582591323892345,
        "userScore":1
    },
    {
        "title": "Epidemic clones, oceanic gene pools and eco-LD in the free living marine\n  pathogen Vibrio parahaemolyticus",
        "paperAbstract": "  We investigated global patterns of variation in 157 whole genome sequences of\nVibrio parahaemolyticus, a free-living and seafood associated marine bacterium.\nPandemic clones, responsible for recent outbreaks of gastroenteritis in humans\nhave spread globally. However, there are oceanic gene pools, one located in the\noceans surrounding Asia and another in the Mexican Gulf. Frequent recombination\nmeans that most isolates have acquired the genetic profile of their current\nlocation. We investigated the genetic structure in the Asian gene pool by\ncalculating the effective population size in two different ways. Under standard\nneutral models, the two estimates should give similar answers but we found a\nthirty fold difference. We propose that this discrepancy is caused by the\nsubdivision of the species into a hundred or more ecotypes which are maintained\nstably in the population. To investigate the genetic factors involved, we used\n51 unrelated isolates to conduct a genome-wide scan for epistatically\ninteracting loci. We found a single example of strong epistasis between distant\ngenome regions. A majority of strains had a type VI secretion system associated\nwith bacterial killing. The remaining strains had genes associated with biofilm\nformation and regulated by c-di-GMP signaling. All strains had one or other of\nthe two systems and none of isolate had complete complements of both systems,\nalthough several strains had remnants. Further top-down analysis of patterns of\nlinkage disequilibrium within frequently recombining species will allow a\ndetailed understanding of how selection acts to structure the pattern of\nvariation within natural bacterial populations.\n",
        "query": "great barrier reef",
        "docId": 536633,
        "score": 0.2580844527060151,
        "userScore":2
    },
    {
        "title": "Bias in Machine Learning -- What is it Good for?",
        "paperAbstract": "  In public media as well as in scientific publications, the term \\emph{bias}\nis used in conjunction with machine learning in many different contexts, and\nwith many different meanings. This paper proposes a taxonomy of these different\nmeanings, terminology, and definitions by surveying the, primarily scientific,\nliterature on machine learning. In some cases, we suggest extensions and\nmodifications to promote a clear terminology and completeness. The survey is\nfollowed by an analysis and discussion on how different types of biases are\nconnected and depend on each other. We conclude that there is a complex\nrelation between bias occurring in the machine learning pipeline that leads to\na model, and the eventual bias of the model (which is typically related to\nsocial discrimination). The former bias may or may not influence the latter, in\na sometimes bad, and sometime good way.\n",
        "query": "bias machine learning",
        "docId": 1265456,
        "score": 0.723857486141128,
        "userScore":1
    },
    {
        "title": "Algorithmic Factors Influencing Bias in Machine Learning",
        "paperAbstract": "  It is fair to say that many of the prominent examples of bias in Machine\nLearning (ML) arise from bias that is there in the training data. In fact, some\nwould argue that supervised ML algorithms cannot be biased, they reflect the\ndata on which they are trained. In this paper we demonstrate how ML algorithms\ncan misrepresent the training data through underestimation. We show how\nirreducible error, regularization and feature and class imbalance can\ncontribute to this underestimation. The paper concludes with a demonstration of\nhow the careful management of synthetic counterfactuals can ameliorate the\nimpact of this underestimation bias.\n",
        "query": "bias machine learning",
        "docId": 1461432,
        "score": 0.6905869954588297,
        "userScore":3
    },
    {
        "title": "Understanding Bias in Machine Learning",
        "paperAbstract": "  Bias is known to be an impediment to fair decisions in many domains such as\nhuman resources, the public sector, health care etc. Recently, hope has been\nexpressed that the use of machine learning methods for taking such decisions\nwould diminish or even resolve the problem. At the same time, machine learning\nexperts warn that machine learning models can be biased as well. In this\narticle, our goal is to explain the issue of bias in machine learning from a\ntechnical perspective and to illustrate the impact that biased data can have on\na machine learning model. To reach such a goal, we develop interactive plots to\nvisualizing the bias learned from synthetic data.\n",
        "query": "bias machine learning",
        "docId": 1171904,
        "score": 0.685822479734945,
        "userScore":3
    },
    {
        "title": "Underestimation Bias and Underfitting in Machine Learning",
        "paperAbstract": "  Often, what is termed algorithmic bias in machine learning will be due to\nhistoric bias in the training data. But sometimes the bias may be introduced\n(or at least exacerbated) by the algorithm itself. The ways in which algorithms\ncan actually accentuate bias has not received a lot of attention with\nresearchers focusing directly on methods to eliminate bias - no matter the\nsource. In this paper we report on initial research to understand the factors\nthat contribute to bias in classification algorithms. We believe this is\nimportant because underestimation bias is inextricably tied to regularization,\ni.e. measures to address overfitting can accentuate bias.\n",
        "query": "bias machine learning",
        "docId": 1288847,
        "score": 0.6499209526402989,
        "userScore":2
    },
    {
        "title": "A survey of bias in Machine Learning through the prism of Statistical\n  Parity for the Adult Data Set",
        "paperAbstract": "  Applications based on Machine Learning models have now become an\nindispensable part of the everyday life and the professional world. A critical\nquestion then recently arised among the population: Do algorithmic decisions\nconvey any type of discrimination against specific groups of population or\nminorities? In this paper, we show the importance of understanding how a bias\ncan be introduced into automatic decisions. We first present a mathematical\nframework for the fair learning problem, specifically in the binary\nclassification setting. We then propose to quantify the presence of bias by\nusing the standard Disparate Impact index on the real and well-known Adult\nincome data set. Finally, we check the performance of different approaches\naiming to reduce the bias in binary classification outcomes. Importantly, we\nshow that some intuitive methods are ineffective. This sheds light on the fact\ntrying to make fair machine learning models may be a particularly challenging\ntask, in particular when the training observations contain a bias.\n",
        "query": "bias machine learning",
        "docId": 1264618,
        "score": 0.637734214474774,
        "userScore":3
    },
    {
        "title": "The Bias-Expressivity Trade-off",
        "paperAbstract": "  Learning algorithms need bias to generalize and perform better than random\nguessing. We examine the flexibility (expressivity) of biased algorithms. An\nexpressive algorithm can adapt to changing training data, altering its outcome\nbased on changes in its input. We measure expressivity by using an\ninformation-theoretic notion of entropy on algorithm outcome distributions,\ndemonstrating a trade-off between bias and expressivity. To the degree an\nalgorithm is biased is the degree to which it can outperform uniform random\nsampling, but is also the degree to which is becomes inflexible. We derive\nbounds relating bias to expressivity, proving the necessary trade-offs inherent\nin trying to create strongly performing yet flexible algorithms.\n",
        "query": "bias machine learning",
        "docId": 1203558,
        "score": 0.6357328490080292,
        "userScore":1
    },
    {
        "title": "Identifying and Correcting Label Bias in Machine Learning",
        "paperAbstract": "  Datasets often contain biases which unfairly disadvantage certain groups, and\nclassifiers trained on such datasets can inherit these biases. In this paper,\nwe provide a mathematical formulation of how this bias can arise. We do so by\nassuming the existence of underlying, unknown, and unbiased labels which are\noverwritten by an agent who intends to provide accurate labels but may have\nbiases against certain groups. Despite the fact that we only observe the biased\nlabels, we are able to show that the bias may nevertheless be corrected by\nre-weighting the data points without changing the labels. We show, with\ntheoretical guarantees, that training on the re-weighted dataset corresponds to\ntraining on the unobserved but unbiased labels, thus leading to an unbiased\nmachine learning classifier. Our procedure is fast and robust and can be used\nwith virtually any learning algorithm. We evaluate on a number of standard\nmachine learning fairness datasets and a variety of fairness notions, finding\nthat our method outperforms standard approaches in achieving fair\nclassification.\n",
        "query": "bias machine learning",
        "docId": 1074513,
        "score": 0.6314838058194834,
        "userScore":3
    },
    {
        "title": "A Mathematical Foundation for Robust Machine Learning based on\n  Bias-Variance Trade-off",
        "paperAbstract": "  A common assumption in machine learning is that samples are independently and\nidentically distributed (i.i.d). However, the contributions of different\nsamples are not identical in training. Some samples are difficult to learn and\nsome samples are noisy. The unequal contributions of samples has a considerable\neffect on training performances. Studies focusing on unequal sample\ncontributions (e.g., easy, hard, noisy) in learning usually refer to these\ncontributions as robust machine learning (RML). Weighing and regularization are\ntwo common techniques in RML. Numerous learning algorithms have been proposed\nbut the strategies for dealing with easy/hard/noisy samples differ or even\ncontradict with different learning algorithms. For example, some strategies\ntake the hard samples first, whereas some strategies take easy first.\nConducting a clear comparison for existing RML algorithms in dealing with\ndifferent samples is difficult due to lack of a unified theoretical framework\nfor RML. This study attempts to construct a mathematical foundation for RML\nbased on the bias-variance trade-off theory. A series of definitions and\nproperties are presented and proved. Several classical learning algorithms are\nalso explained and compared. Improvements of existing methods are obtained\nbased on the comparison. A unified method that combines two classical learning\nstrategies is proposed.\n",
        "query": "bias machine learning",
        "docId": 1483286,
        "score": 0.6243950514978494,
        "userScore":2
    },
    {
        "title": "Learning De-biased Representations with Biased Representations",
        "paperAbstract": "  Many machine learning algorithms are trained and evaluated by splitting data\nfrom a single source into training and test sets. While such focus on\nin-distribution learning scenarios has led to interesting advancement, it has\nnot been able to tell if models are relying on dataset biases as shortcuts for\nsuccessful prediction (e.g., using snow cues for recognising snowmobiles),\nresulting in biased models that fail to generalise when the bias shifts to a\ndifferent class. The cross-bias generalisation problem has been addressed by\nde-biasing training data through augmentation or re-sampling, which are often\nprohibitive due to the data collection cost (e.g., collecting images of a\nsnowmobile on a desert) and the difficulty of quantifying or expressing biases\nin the first place. In this work, we propose a novel framework to train a\nde-biased representation by encouraging it to be different from a set of\nrepresentations that are biased by design. This tactic is feasible in many\nscenarios where it is much easier to define a set of biased representations\nthan to define and quantify bias. We demonstrate the efficacy of our method\nacross a variety of synthetic and real-world biases; our experiments show that\nthe method discourages models from taking bias shortcuts, resulting in improved\ngeneralisation. Source code is available at https://github.com/clovaai/rebias.\n",
        "query": "bias machine learning",
        "docId": 1186725,
        "score": 0.6152342964420097,
        "userScore":3
    },
    {
        "title": "Removing biased data to improve fairness and accuracy",
        "paperAbstract": "  Machine learning systems are often trained using data collected from\nhistorical decisions. If past decisions were biased, then automated systems\nthat learn from historical data will also be biased. We propose a black-box\napproach to identify and remove biased training data. Machine learning models\ntrained on such debiased data (a subset of the original training data) have low\nindividual discrimination, often 0%. These models also have greater accuracy\nand lower statistical disparity than models trained on the full historical\ndata. We evaluated our methodology in experiments using 6 real-world datasets.\nOur approach outperformed seven previous approaches in terms of individual\ndiscrimination and accuracy.\n",
        "query": "bias machine learning",
        "docId": 1419543,
        "score": 0.6106692044642656,
        "userScore":3
    },
    {
        "title": "A New Desalination Pump Help Define the pH of Ocean Worlds",
        "paperAbstract": "  We study ocean exoplanets, for which the global surface ocean is separated\nfrom the rocky interior by a high-pressure ice mantle. We describe a mechanism\nthat can pump salts out of the ocean, resulting in oceans of very low salinity.\nHere we focus on the H2O-NaCl system, though we discuss the application of this\npump to other salts as well. We find our ocean worlds to be acidic, with a pH\nin the range of 2-4. We discuss and compare between the conditions found within\nour studied oceans and the conditions in which polyextremophiles were\ndiscovered. This work focuses on exoplanets in the super-Earth mass range (2\nM_Earth), with water composing at least a few percent of their mass. Although,\nthe principal of the desalination pump may extend beyond this mass range.\n",
        "query": "Ocean acidification",
        "docId": 959070,
        "score": 0.5723472532476546,
        "userScore":2
    },
    {
        "title": "Modeling the seasonal variability and the governing factors of Ocean\n  Acidification over the Bay of Bengal region",
        "paperAbstract": "  The Bay of Bengal (BoB) is a high recipient of freshwater flux from rivers\nand precipitation, making the region strongly stratified. The strong\nstratification results in a thick barrier layer formation, which inhibits\nvertical mixing making this region a low-productive zone. In the present study,\nwe attempt to model the pH of the BoB region and understand the role of\ndifferent governing factors such as sea-surface temperature (SST), sea-surface\nsalinity (SSS), dissolved inorganic carbon (DIC), and total alkalinity (TALK)\non the seasonality of sea-surface pH. We run a set of sensitivity experiments\nto understand the role of each of the governing factors. The results show that\nthe SST, SSS, and DIC are the principal drivers affecting the sea-surface pH,\nwhile TALK plays a buffering role. The SST and DIC are consistently found to be\nopposite to each other. The pre-monsoon season (MAM) has shown to have an\nalmost equal contribution from all the drivers. In the pre-monsoon season, the\nSST and DIC are balanced by TALK and SSS. The role of SSS is significantly\ndominant in the second half of the year. Both SST and SSS counter the role of\nDIC in the southwest monsoon season. The strong stratification plays an\nessential role in modulating the pH of the BoB region. The thickness of the\nbarrier layer formed in the sub-surface layers positively affects the\nsea-surface pH. The northern BoB is found to be more alkaline than the southern\nBoB. Our study highlights the complexity of ocean acidification in the BoB\nregion compared to the other part of the world ocean.\n",
        "query": "Ocean acidification",
        "docId": 1661826,
        "score": 0.533467465774093,
        "userScore":2
    },
    {
        "title": "The pH of Enceladus\u0027 ocean",
        "paperAbstract": "  Observational data from the Cassini spacecraft are used to obtain a chemical\nmodel of ocean water on Enceladus. The model indicates that Enceladus\u0027 ocean is\na Na-Cl-CO3 solution with an alkaline pH of ~11-12. The dominance of aqueous\nNaCl is a feature that Enceladus\u0027 ocean shares with terrestrial seawater, but\nthe ubiquity of dissolved Na2CO3 suggests that soda lakes are more analogous to\nthe Enceladus ocean. The high pH implies that the hydroxide ion should be\nrelatively abundant, while divalent metals should be present at low\nconcentrations owing to buffering by clays and carbonates on the ocean floor.\nThe high pH is interpreted to be a key consequence of serpentinization of\nchondritic rock, as predicted by prior geochemical reaction path models;\nalthough degassing of CO2 from the ocean may also play a role depending on the\nefficiency of mixing processes in the ocean. Serpentinization leads to the\ngeneration of H2, a geochemical fuel that can support both abiotic and\nbiological synthesis of organic molecules such as those that have been detected\nin Enceladus\u0027 plume. Serpentinization and H2 generation should have occurred on\nEnceladus, like on the parent bodies of aqueously altered meteorites; but it is\nunknown whether these critical processes are still taking place, or if\nEnceladus\u0027 rocky core has been completely altered by past hydrothermal\nactivity. The high pH also suggests that the delivery of oxidants from the\nsurface to the ocean has not been significant, and the rocky core did not\nexperience partial melting and igneous differentiation. On the other hand, the\npH is compatible with life as we know it; life on Earth may have begun under\nsimilar conditions, and serpentinites on Earth support microbial communities\nthat are centered on H2 that is provided by water-rock reactions.\n",
        "query": "Ocean acidification",
        "docId": 596606,
        "score": 0.5218300040244159,
        "userScore":1
    },
    {
        "title": "Diverse Carbonates in Exoplanet Oceans Promote the Carbon Cycle",
        "paperAbstract": "  Carbonate precipitation in oceans is essential for the carbonate-silicate\ncycle (inorganic carbon cycle) to maintain temperate climates. By considering\nthe thermodynamics of carbonate chemistry, we demonstrate that the ocean pH\ndecreases by approximately 0.5 for a factor of 10 increase in the atmospheric\ncarbon dioxide content. The upper and lower limits of ocean pH are within 1-4\nof each other, where the upper limit is buffered by carbonate precipitation and\ndefines the ocean pH when the carbon cycle operates. If the carbonate\ncompensation depth (CCD) resides above the ocean floor, then carbonate\nprecipitation and the carbon cycle cease to operate. The CCD is deep (\u003e40 km)\nfor high ocean temperature and high atmospheric carbon dioxide content. Key\ndivalent carbonates of magnesium, calcium and iron produce an increasingly\nwider parameter space of deep CCDs, suggesting that chemical diversity promotes\nthe carbon cycle. The search for life from exoplanets will benefit by including\nchemically more diverse targets than Earth twins.\n",
        "query": "Ocean acidification",
        "docId": 1773716,
        "score": 0.49436000925566503,
        "userScore":1
    },
    {
        "title": "Anthropogenic Mixing of Seasonally Stratified Shelf Seas by Offshore\n  Wind Farm Infrastructure",
        "paperAbstract": "  The offshore wind energy sector has rapidly expanded over the past two\ndecades, providing a renewable energy solution for coastal nations. Sector\ndevelopment has been led in Europe, but is growing globally. Most developments\nto date have been in well-mixed, i.e. unstratified, shallow-waters near to\nshore. Sector growth is, for the first time, pushing developments to deep\nwater, into a brand new environment: seasonally stratified shelf seas.\nSeasonally stratified shelf seas, where water density varies with depth, have a\ndisproportionately key role in primary production, marine ecosystem and\nbiochemically cycles. Infrastructure will directly mix stratified shelf seas.\nThe magnitude of this mixing, additional to natural background processes, has\nyet to be fully quantified. If large enough it may erode shelf sea\nstratification. Therefore, offshore wind growth may destabilize and\nfundamentally change shelf sea systems. However, enhanced mixing may also\npositively impact some marine ecosystems. This paper sets the scene for sector\ndevelopment into this new environment, reviews the potential physical and\nenvironmental benefits and impacts of large scale industrialization of\nseasonally stratified shelf seas and identifies areas where research is\nrequired to best utilise, manage and mitigate environmental change.\n",
        "query": "Ocean acidification",
        "docId": 1582347,
        "score": 0.4879955917673371,
        "userScore":1
    },
    {
        "title": "Decomposition of Amino Acids in Water with Application to In-Situ\n  Measurements of Enceladus, Europa and Other Hydrothermally Active Icy Ocean\n  Worlds",
        "paperAbstract": "  To test the potential of using amino acid abundances as a biosignature at icy\nocean worlds, we investigate whether primordial amino acids (accreted or formed\nby early aqueous processes) could persist until the present time. By examining\nthe decomposition kinetics of amino acids in aqueous solution based on existing\nlaboratory rate data, we find that all fourteen proteinogenic amino acids\nconsidered in this study decompose to a very large extent (\u003e99.9%) over\nrelatively short lengths of time in hydrothermally active oceans. Therefore, as\na rule of thumb, we suggest that if amino acids are detected at Enceladus,\nEuropa, or other hydrothermally active ocean worlds above a concentration of 1\nnM, they should have been formed recently and not be relicts of early\nprocesses. In particular, the detection of aspartic acid (Asp) and threonine\n(Thr) would strongly suggest active production within the ocean, as these amino\nacids cannot persist beyond 1 billion years even at the freezing point\ntemperature of 273K. Identifying amino acids from the oceans of icy worlds can\nprovide key insight into their history of organic chemistry.\n",
        "query": "Ocean acidification",
        "docId": 1109447,
        "score": 0.48348189779696327,
        "userScore":1
    },
    {
        "title": "The Effect of Ocean Salinity on Climate and Its Implications for Earth\u0027s\n  Habitability",
        "paperAbstract": "  The influence of atmospheric composition on the climates of present-day and\nearly Earth has been studied extensively, but the role of ocean composition has\nreceived less attention. We use the ROCKE-3D ocean-atmosphere general\ncirculation model to investigate the response of Earth\u0027s present-day and\nArchean climate system to low vs. high ocean salinity. We find that saltier\noceans yield warmer climates in large part due to changes in ocean dynamics.\nIncreasing ocean salinity from 20 g/kg to 50 g/kg results in a 71% reduction in\nsea ice cover in our present-day Earth scenario. This same salinity change also\nhalves the pCO$_2$ threshold at which Snowball glaciation occurs in our Archean\nscenarios. In combination with higher levels of greenhouse gases such as CO$_2$\nand CH$_4$, a saltier ocean may allow for a warm Archean Earth with only\nseasonal ice at the poles despite receiving 20% less energy from the Sun.\n",
        "query": "Ocean acidification",
        "docId": 1651197,
        "score": 0.4591800790859608,
        "userScore":3
    },
    {
        "title": "Constraining the climate and ocean pH of the early Earth with a\n  geological carbon cycle model",
        "paperAbstract": "  The early Earth\u0027s environment is controversial. Climatic estimates range from\nhot to glacial, and inferred marine pH spans strongly alkaline to acidic.\nBetter understanding of early climate and ocean chemistry would improve our\nknowledge of the origin of life and its coevolution with the environment. Here,\nwe use a geological carbon cycle model with ocean chemistry to calculate\nself-consistent histories of climate and ocean pH. Our carbon cycle model\nincludes an empirically justified temperature and pH dependence of seafloor\nweathering, allowing the relative importance of continental and seafloor\nweathering to be evaluated. We find that the Archean climate was likely\ntemperate (0-50 {\\deg}C) due to the combined negative feedbacks of continental\nand seafloor weathering. Ocean pH evolves monotonically from 6.6 (+0.6,-0.4)\n(2{\\sigma}) at 4.0 Ga to 7.0 (+0.7,-0.5) (2{\\sigma}) at the Archean-Proterozoic\nboundary, and to 7.9 (+0.1,-0.2) (2{\\sigma}) at the Proterozoic-Phanerozoic\nboundary. This evolution is driven by the secular decline of pCO2, which in\nturn is a consequence of increasing solar luminosity, but is moderated by\ncarbonate alkalinity delivered from continental and seafloor weathering.\nArchean seafloor weathering may have been a comparable carbon sink to\ncontinental weathering, but is less dominant than previously assumed, and would\nnot have induced global glaciation. We show how these conclusions are robust to\na wide range of scenarios for continental growth, internal heat flow evolution\nand outgassing history, greenhouse gas abundances, and changes in the biotic\nenhancement of weathering.\n",
        "query": "Ocean acidification",
        "docId": 962456,
        "score": 0.4570636585771055,
        "userScore":2
    },
    {
        "title": "The development of deep-ocean anoxia in a comprehensive ocean phosphorus\n  model",
        "paperAbstract": "  We analyse a model of the phosphorus cycle in the ocean given by Slomp and\nVan Cappellen (2007, https://doi.org/10.5194/bg-4-155-2007). This model\ncontains four distinct oceanic basins and includes relevant parts of the water,\ncarbon and oxygen cycles. We show that the model can essentially be solved\nanalytically, and its behaviour completely understood without recourse to\nnumerical methods. In particular, we show that, in the model, the carbon and\nphosphorus concentrations in the different ocean reservoirs are all slaved to\nthe concentration of soluble reactive phosphorus in the deep ocean, which\nrelaxes to an equilibrium on a time scale of 180,000 y, and we show that the\ndeep ocean is either oxic or anoxic, depending on a critical parameter which we\ncan determine explicitly. Finally, we examine how the value of this critical\nparameter depends on the physical parameters contained in the model. The\npresented methodology is based on tools from applied mathematics and can be\nused to reduce the complexity of other large, biogeochemical models.\n",
        "query": "Ocean acidification",
        "docId": 1336099,
        "score": 0.4559647295227407,
        "userScore":2
    },
    {
        "title": "On the effects of circulation, sediment resuspension and biological\n  incorporation by diatoms in an ocean model of aluminium",
        "paperAbstract": "  The distribution of dissolved aluminium in the West Atlantic Ocean shows a\nmirror image with that of dissolved silicic acid, hinting at intricate\ninteractions between the ocean cycling of Al and Si. The marine biogeochemistry\nof Al is of interest because of its potential impact on diatom opal\nremineralisation, hence Si availability. Furthermore, the dissolved Al\nconcentration at the surface ocean has been used as a tracer for dust input,\ndust being the most important source of the bio-essential trace element iron to\nthe ocean. Previously, the dissolved concentration of Al was simulated\nreasonably well with only a dust source, and scavenging by adsorption on\nsettling biogenic debris as the only removal process. Here we explore the\nimpacts of (i) a sediment source of Al in the Northern Hemisphere (especially\nnorth of ~40{\\deg}N), (ii) the imposed velocity field, and (iii) biological\nincorporation of Al on the modelled Al distribution in the ocean. The sediment\nsource clearly improves the model results, and using a different velocity field\nshows the importance of advection on the simulated Al distribution. Biological\nincorporation appears to be a potentially important removal process. However,\nconclusive independent data to constrain the Al:Si incorporation ratio by\ngrowing diatoms are missing. Therefore, this study does not provide a\ndefinitive answer to the question of the relative importance of Al removal by\nincorporation compared to removal by adsorptive scavenging.\n",
        "query": "Ocean acidification",
        "docId": 526660,
        "score": 0.4554794156419959,
        "userScore":1
    },
    {
        "title": "Efficacy of Hydroxychloroquine as Prophylaxis for Covid-19",
        "paperAbstract": "  Limitations in the design of the experiment of Boulware et al[1] are\nconsidered in Cohen[2]. They are not subject to correction but they are\nreported for readers\u0027 consideration. However, they made an analysis for the\nincidence based on Fisher\u0027s hypothesis test for means while they published\ndetailed time dependent data which were not analyzed, disregarding an important\ninformation. Here we make the analyses with this time dependent data adopting a\nsimple regression analysis.\n  We conclude their randomized, double-blind, placebo-controlled trial presents\nstatistical evidence, at 99% confidence level, that the treatment of Covid-19\npatients with hydroxychloroquine is effective in reducing the appearance of\nsymptoms if used before or right after exposure to the virus. For 0 to 2 days\nafter exposure to virus, the estimated relative reduction in symptomatic\noutcomes is 72% after 0 days, 48.9% after 1 day and 29.3% after 2 days. For 3\ndays after exposure, the estimated relative reduction is 15.7% but results are\nnot statistically conclusive and for 4 or more days after exposure there is no\nstatistical evidence that hydroxychloroquine is effective in reducing the\nappearance of symptoms.\n  Our results show that the time elapsed between infection and the beginning of\ntreatment is crucial for the efficacy of hydroxychloroquine as a treatment to\nCovid-19.\n",
        "query": "Shark antibodies antiviral therapies",
        "docId": 1320991,
        "score": 0.34600064612413917,
        "userScore":0
    },
    {
        "title": "Protecting Anti-virus Programs From Viral Attacks",
        "paperAbstract": "  During a fight between viruses and anti-viruses it is not always predictable\nthat the anti-virus is going to win. There are many malicious viruses which\ntarget to attack and paralyze the anti-viruses. It is necessary for an\nanti-virus to detect and destroy the malware before its own files are detected\nand destroyed by the malware. The anti-virus may follow thorough testing and\nauditing procedures to fix all its bugs before releasing the software in the\nmarket. Besides the anti-virus may use all the obfuscation techniques like\npolymorphism that the viruses generally use to hide their codes. This article\nalso shows how to use TRIZ Inventive Standards to solve the harmful effects of\nthe viruses on the anti-virus.\n",
        "query": "Shark antibodies antiviral therapies",
        "docId": 448082,
        "score": 0.3349346525228327,
        "userScore":0
    },
    {
        "title": "In silico evaluation of Paxlovid\u0027s pharmacometrics for SARS-CoV-2: a\n  multiscale approach",
        "paperAbstract": "  Paxlovid is a promising, orally bioavailable novel drug for SARS--CoV--2 with\nexcellent safety profiles. Our main goal here is to explore the pharmacometric\nfeatures of this new antiviral. To provide a detailed assessment of Paxlovid,\nwe propose a hybrid multiscale mathematical approach. We demonstrate that the\nresults of the present \\textit{in silico} evaluation match the clinical\nexpectations remarkably well: on the one hand, our computations successfully\nreplicate the outcome of an actual \\textit{in vitro} experiment; on the other\nhand we verify both the sufficiency and the necessity of Paxlovid\u0027s two main\ncomponents (nirmatrelvir and ritonavir) for a simplified \\textit{in vivo} case.\nMoreover, in the simulated context of our computational framework we visualize\nthe importance of early interventions, and identify the time window where a\nunit--length delay causes the highest level of tissue damage. Finally, the\nresults\u0027 sensitivity to the diffusion coefficient of the virus is explored in\ndetails.\n",
        "query": "Shark antibodies antiviral therapies",
        "docId": 1641144,
        "score": 0.32749427228167605,
        "userScore":0
    },
    {
        "title": "Protecting others vs. protecting yourself against ballistic droplets:\n  Quantification by stain patterns",
        "paperAbstract": "  It is often accepted a priori that a face mask worn by an infected subject is\neffective to avoid the spreading of a respiratory disease, while a healthy\nperson is not necessarily well protected when wearing the mask. Using a frugal\nstain technique, we quantify the ballistic droplets reaching a receptor from a\njet-emitting source which mimics a coughing, sneezing or talking human: in real\nlife, such droplets may host active SARS-CoV-2 virus able to replicate in the\nnasopharynx. We demonstrate that materials often used in home-made face masks\nblock most of the droplets. We also show quantitatively that less liquid\ncarried by ballistic droplets reaches a receptor when a blocking material is\ndeployed near the source than when located near the receptor, which supports\nthe paradigm that your face mask does protect you, but protects others even\nbetter than you.\n",
        "query": "Shark antibodies antiviral therapies",
        "docId": 1526527,
        "score": 0.31116409350914154,
        "userScore":0
    },
    {
        "title": "COVID-19 Detection Using Recorded Coughs in the 2021 DiCOVA Challenge",
        "paperAbstract": "  COVID-19 has resulted in over 100 million infections and caused worldwide\nlock downs due to its high transmission rate and limited testing options.\nCurrent diagnostic tests can be expensive, limited in availability,\ntime-intensive and require risky in-person appointments. It has been\nestablished that symptomatic COVID-19 seriously impairs normal functioning of\nthe respiratory system, thus affecting the coughing acoustics. The 2021 DiCOVA\nChallenge @ INTERSPEECH was designed to find scientific and engineering\ninsights to the question by enabling participants to analyze an acoustic\ndataset gathered from COVID-19 positive and non-COVID-19 individuals. In this\nreport we describe our participation in the Challenge (Track 1). We achieved\n82.37% AUC ROC on the blind test outperforming the Challenge\u0027s baseline of\n69.85%.\n",
        "query": "Shark antibodies antiviral therapies",
        "docId": 1473178,
        "score": 0.30664312724371,
        "userScore":0
    },
    {
        "title": "Personal Ultraviolet Respiratory Germ Eliminating Machine\n  (PUR$\\diamond$GEM) for COVID-19",
        "paperAbstract": "  The current COVID-19 pandemic has highlighted the need for cheap reusable\npersonal protective equipment. The disinfection properties of Ultraviolet (UV)\nradiation in the 200-300 nm have been long known and documented. Many solutions\nusing UV radiation, such as cavity disinfection and whole room decontamination\nbetween uses, are in use in various industries, including healthcare. Here we\npropose a portable wearable device which can safely, efficiently and\neconomically, continuously disinfect inhaled/exhaled air using UV radiation\nwith possible 99.99% virus elimination. We utilize UV radiation in the 260 nm\nrange where no ozone is produced, and because of the self-contained UV chamber,\nthere would be no UV exposure to the user. We have optimized the cavity design\nsuch that an amplification of 10-50 times the irradiated UV power may be\nobtained. This is crucial in ensuring enough UV dosage is delivered to the air\nflow during breathing. Further, due to the turbulent nature of airflow, a\nseries of cavities is proposed to ensure efficient actual disinfection. The\nPersonal Ultraviolet Respiratory Germ Eliminating Machine (PUR$\\diamond$GEM)\ncan be worn by people or attached to devices such as ventilator\nexhausts/intakes, or be used free-standing as a portable local air disinfection\nunit, offering modularity with multiple avenues of usage. Patent pending.\n",
        "query": "Shark antibodies antiviral therapies",
        "docId": 1382349,
        "score": 0.2968841009485388,
        "userScore":0
    },
    {
        "title": "A Compressed Sensing Approach to Pooled RT-PCR Testing for COVID-19\n  Detection",
        "paperAbstract": "  We propose `Tapestry\u0027, a novel approach to pooled testing with application to\nCOVID-19 testing with quantitative Reverse Transcription Polymerase Chain\nReaction (RT-PCR) that can result in shorter testing time and conservation of\nreagents and testing kits. Tapestry combines ideas from compressed sensing and\ncombinatorial group testing with a novel noise model for RT-PCR used for\ngeneration of synthetic data. Unlike Boolean group testing algorithms, the\ninput is a quantitative readout from each test and the output is a list of\nviral loads for each sample relative to the pool with the highest viral load.\nWhile other pooling techniques require a second confirmatory assay, Tapestry\nobtains individual sample-level results in a single round of testing, at\nclinically acceptable false positive or false negative rates. We also propose\ndesigns for pooling matrices that facilitate good prediction of the infected\nsamples while remaining practically viable. When testing $n$ samples out of\nwhich $k \\ll n$ are infected, our method needs only $O(k \\log n)$ tests when\nusing random binary pooling matrices, with high probability. However, we also\nuse deterministic binary pooling matrices based on combinatorial design ideas\nof Kirkman Triple Systems to balance between good reconstruction properties and\nmatrix sparsity for ease of pooling. In practice, we have observed the need for\nfewer tests with such matrices than with random pooling matrices. This makes\nTapestry capable of very large savings at low prevalence rates, while\nsimultaneously remaining viable even at prevalence rates as high as 9.5\\%.\nEmpirically we find that single-round Tapestry pooling improves over two-round\nDorfman pooling by almost a factor of 2 in the number of tests required. We\nvalidate Tapestry in simulations and wet lab experiments with oligomers in\nquantitative RT-PCR assays. Lastly, we describe use-case scenarios for\ndeployment.\n",
        "query": "Shark antibodies antiviral therapies",
        "docId": 1287690,
        "score": 0.29101576578653265,
        "userScore":0
    },
    {
        "title": "Tele-operative Robotic Lung Ultrasound Scanning Platform for Triage of\n  COVID-19 Patients",
        "paperAbstract": "  Novel severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has become\na pandemic of epic proportions and a global response to prepare health systems\nworldwide is of utmost importance. In addition to its cost-effectiveness in a\nresources-limited setting, lung ultrasound (LUS) has emerged as a rapid\nnoninvasive imaging tool for the diagnosis of COVID-19 infected patients.\nConcerns surrounding LUS include the disparity of infected patients and\nhealthcare providers, relatively small number of physicians and sonographers\ncapable of performing LUS, and most importantly, the requirement for\nsubstantial physical contact between the patient and operator, increasing the\nrisk of transmission. Mitigation of the spread of the virus is of paramount\nimportance. A 2-dimensional (2D) tele-operative robotic platform capable of\nperforming LUS in for COVID-19 infected patients may be of significant benefit.\nThe authors address the aforementioned issues surrounding the use of LUS in the\napplication of COVID- 19 infected patients. In addition, first time\napplication, feasibility and safety were validated in three healthy subjects,\nalong with 2D image optimization and comparison for overall accuracy.\nPreliminary results demonstrate that the proposed platform allows for\nsuccessful acquisition and application of LUS in humans.\n",
        "query": "Shark antibodies antiviral therapies",
        "docId": 1368665,
        "score": 0.28241619194135836,
        "userScore":0
    },
    {
        "title": "XSS Vulnerabilities in Cloud-Application Add-Ons",
        "paperAbstract": "  Cloud-application add-ons are microservices that extend the functionality of\nthe core applications. Many application vendors have opened their APIs for\nthird-party developers and created marketplaces for add-ons (also add-ins or\napps). This is a relatively new phenomenon, and its effects on the application\nsecurity have not been widely studied. It seems likely that some of the add-ons\nhave lower code quality than the core applications themselves and, thus, may\nbring in security vulnerabilities. We found that many such add-ons are\nvulnerable to cross-site scripting (XSS). The attacker can take advantage of\nthe document-sharing and messaging features of the cloud applications to send\nmalicious input to them. The vulnerable add-ons then execute client-side\nJavaScript from the carefully crafted malicious input. In a major analysis\neffort, we systematically studied 300 add-ons for three popular application\nsuites, namely Microsoft Office Online, G Suite and Shopify, and discovered a\nsignificant percentage of vulnerable add-ons in each marketplace. We present\nthe results of this study, as well as analyze the add-on architectures to\nunderstand how the XSS vulnerabilities can be exploited and how the threat can\nbe mitigated.\n",
        "query": "Shark antibodies antiviral therapies",
        "docId": 1210926,
        "score": 0.27129084573505224,
        "userScore":0
    },
    {
        "title": "From Malware Signatures to Anti-Virus Assisted Attacks",
        "paperAbstract": "  Although anti-virus software has significantly evolved over the last decade,\nclassic signature matching based on byte patterns is still a prevalent concept\nfor identifying security threats. Anti-virus signatures are a simple and fast\ndetection mechanism that can complement more sophisticated analysis strategies.\nHowever, if signatures are not designed with care, they can turn from a\ndefensive mechanism into an instrument of attack. In this paper, we present a\nnovel method for automatically deriving signatures from anti-virus software and\ndemonstrate how the extracted signatures can be used to attack sensible data\nwith the aid of the virus scanner itself. We study the practicability of our\napproach using four commercial products and exemplarily discuss a novel attack\nvector made possible by insufficiently designed signatures. Our research\nindicates that there is an urgent need to improve pattern-based signatures if\nused in anti-virus software and to pursue alternative detection approaches in\nsuch products.\n",
        "query": "Shark antibodies antiviral therapies",
        "docId": 781369,
        "score": 0.27007124271226957,
        "userScore":0
    },
    {
        "title": "On the nature of monetary and price inflation and hyperinflation",
        "paperAbstract": "  Monetary inflation is a sustained increase in the money supply than can\nresult in price inflation, which is a rise in the general level of prices of\ngoods and services. The objectives of this paper were to develop economic\nmodels to (1) predict the annual rate of growth in the US consumer price index\n(CPI), based on the annual growth in the US broad money supply (BMS), the\nannual growth in US real GDP, and the annual growth in US savings, over the\ntime period 2001 to 2019; (2) investigate the means by which monetary and price\ninflation can develop into monetary and price hyperinflation. The hypothesis\nthat the annual rate of growth in the US CPI is a function of the annual growth\nin the US BMS minus the annual growth in US real GDP minus the annual growth in\nUS savings, over the time period investigated, has been shown to be the case.\nHowever, an exact relationship required the use of a non-zero residual term. A\nmathematical statistical formulation of a hyperinflationary process has been\nprovided and used to quantify the period of hyperinflation in the Weimar\nRepublic, from July 1922 until the end of November 1923.\n",
        "query": "CPI GDP",
        "docId": 1535801,
        "score": 0.5592003260949792,
        "userScore":1
    },
    {
        "title": "Why price inflation in developed countries is systematically\n  underestimated",
        "paperAbstract": "  There is an extensive historical dataset on real GDP per capita prepared by\nAngus Maddison. This dataset covers the period since 1870 with continuous\nannual estimates in developed countries. All time series for individual\neconomies have a clear structural break between 1940 and 1950. The behavior\nbefore 1940 and after 1950 can be accurately (R2 from 0.7 to 0.99) approximated\nby linear time trends. The corresponding slopes of regressions lines before and\nafter the break differ by a factor of 4 (Switzerland) to 19 (Spain). We have\nextrapolated the early trends into the second interval and obtained much lower\nestimates of real GDP per capita in 2011: from 2.4 (Switzerland) to 5.0 (Japan)\ntimes smaller than the current levels. When the current linear trends are\nextrapolated into the past, they intercept the zero line between 1908\n(Switzerland) and 1944 (Japan). There is likely an internal conflict between\nthe estimating procedures before 1940 and after 1950. A reasonable explanation\nof the discrepancy is that the GDP deflator in developed countries has been\nhighly underestimated since 1950. In the USA, the GDP deflator is\nunderestimated by a factor of 1.4. This is exactly the ratio of the interest\nrate controlled by the Federal Reserve and the rate of inflation. Hence, the\nFederal Reserve actually retains its interest rate at the level of true price\ninflation when corrected for the bias in the GDP deflator.\n",
        "query": "CPI GDP",
        "docId": 346543,
        "score": 0.5455884748518098,
        "userScore":1
    },
    {
        "title": "The link between unemployment and real economic growth in developed\n  countries",
        "paperAbstract": "  Ten years ago we presented a modified version of Okun law for the biggest\ndeveloped economies and reported its excellent predictive power. In this study,\nwe revisit the original models using the estimates of real GDP per capita and\nunemployment rate between 2010 and 2019. The initial results show that the\nchange in unemployment rate can be accurately predicted by variations in the\nrate of real economic growth. There is a discrete version of the model which is\nrepresented by a piece wise linear dependence of the annual increment in\nunemployment rate on the annual rate of change in real GDP per capita. The\nlengths of the country-dependent time segments are defined by breaks in the GDP\nmeasurement units associated with definitional revisions to the nominal GDP and\nGDP deflator (dGDP). The difference between the CPI and dGDP indices since the\nbeginning of measurements reveals the years of such breaks. Statistically, the\nlink between the studied variables in the revised models is characterized by\nthe coefficient of determination in the range from R2\u003d0.866 (Australia) to\nR2\u003d0.977 (France). The residual errors can be likely associated with the\nmeasurement errors, e.g. the estimates of real GDP per capita from various\nsources differ by tens of percent. The obtained results confirm the original\nfinding on the absence of structural unemployment in the studied developed\ncountries.\n",
        "query": "CPI GDP",
        "docId": 1452013,
        "score": 0.5241679653213236,
        "userScore":2
    },
    {
        "title": "Does GDP measure growth in the economy or simply growth in the money\n  supply?",
        "paperAbstract": "  Gross Domestic Product(GDP) is a widely used measurement of economic growth\nrepresenting the market value of all final goods and services produced by a\ncountry within a given time. In this paper we question the assumption that GDP\nmeasures production, and suggest that in reality it merely captures changes in\nthe rate of expansion of the money supply used to measure the price data it is\nderived from. We first review the Quantity Theory of Money $MV\u003dPT$, and show\nthat the Velocity of Circulation of Money(V) does not affect the price level as\nclaimed, as it is also a factor of the quantity of transactions(T). It then\nfollows directly that attempts to measure total production from any form of\nprice data as the GDP measurement does, will necessarily be confounded by the\ninverse relationship between prices and the quantity of production, which\nrequires that as the total quantity of production increases, prices will drop.\nFinally, in support of this claim we present an empirical analysis of the GDP\nof nine countries and one currency union, showing that when normalized for\nmoney supply growth GDP measures have been uniformly shrinking over the last 20\nyears, and discuss the possible reasons for this behaviour.\n",
        "query": "CPI GDP",
        "docId": 361214,
        "score": 0.5172742395314724,
        "userScore":2
    },
    {
        "title": "Analyzing China\u0027s Consumer Price Index Comparatively with that of United\n  States",
        "paperAbstract": "  This paper provides a thorough analysis on the dynamic structures and\npredictability of China\u0027s Consumer Price Index (CPI-CN), with a comparison to\nthose of the United States. Despite the differences in the two leading\neconomies, both series can be well modeled by a class of Seasonal\nAutoregressive Integrated Moving Average Model with Covariates (S-ARIMAX). The\nCPI-CN series possess regular patterns of dynamics with stable annual cycles\nand strong Spring Festival effects, with fitting and forecasting errors largely\ncomparable to their US counterparts. Finally, for the CPI-CN, the diffusion\nindex (DI) approach offers improved predictions than the S-ARIMAX models.\n",
        "query": "CPI GDP",
        "docId": 1197220,
        "score": 0.506868275463189,
        "userScore":3
    },
    {
        "title": "Real GDP per capita since 1870",
        "paperAbstract": "  The growth rate of real GDP per capita in the biggest OECD countries is\nrepresented as a sum of two components - a steadily decreasing trend and\nfluctuations related to the change in some specific age population. The long\nterm trend in the growth rate is modelled by an inverse function of real GDP\nper capita with a constant numerator. This numerator is equivalent to a\nconstant annual increment of real GDP per capita. For the most advanced\neconomies, the GDP estimates between 1950 and 2007 have shown very weak and\nstatistically insignificant linear trends (both positive and negative) in the\nannual increment. The fluctuations around relevant mean increments are\ncharacterized by practically normal distribution. For many countries, there\nexist historical estimates of real GDP since 1870. These estimates extend the\ntime span of our analysis together with a few new estimates from 2008 to 2011.\nThere are severe structural breaks in the corresponding time series between\n1940 and 1950, with the slope of linear regression increasing by a factor of\n4.0 (Switzerland) to 22.1 (Spain). Therefore, the GDP estimates before 1940 and\nafter 1950 have been analysed separately. All findings of the original study\nare validated by the newly available data. The most important is that all\nslopes (except that for Australia after 1950) of the regression lines obtained\nfor the annual increments of real GDP per capita are small and statistically\ninsignificant, i.e. one cannot reject the null hypothesis of a zero slope and\nthus constant increment. Hence the growth in real GDP per capita is a linear\none since 1870 with a break in slope between 1940 and 1950.\n",
        "query": "CPI GDP",
        "docId": 344672,
        "score": 0.4967184042573578,
        "userScore":2
    },
    {
        "title": "Violation of Invariance of Measurement for GDP Growth Rate and its\n  Consequences",
        "paperAbstract": "  The aim here is to address the origins of sustainability for the real growth\nrate in the United States. For over a century of observations on the real GDP\nper capita of the United States a sustainable two percent growth rate has been\nobserved. To find an explanation for this observation I consider the impact of\nutility preferences and the effect of mobility of labor \\\u0026 capital on every\nprovided measurement. Mobility of labor results in heterogenous rates of\nincrease in prices which is called Baumol\u0027s cost disease phenomenon.\nHeterogeneous rates of inflation then make it impossible to define an invariant\nmeasure for the real growth rate. Paradoxical and ambiguous results already\nhave been observed when different measurements provided by the World Bank have\nbeen compared with the ones from the systems of national accounts (SNA). Such\nambiguity is currently being discussed in economy. I define a toy model for\ncaring out measurements in order to state that this ambiguity can be very\nsignificant. I provide examples in which GDP expands 5 folds while measurements\npercept an expansion around 2 folds. Violation of invariance of the\nmeasurements leads to state that it is hard to compare the growth rate of GDP\nfor a smooth growing country such as the U.S. with a fast growing country such\nas China. Besides, I state that to extrapolate the time that economy of China\npasses the economy of the US we need to consider local metric of the central\nbanks of both countries. Finally I conclude that it is our method of\nmeasurements that leads us to percept the sustainable growth rate.\n",
        "query": "CPI GDP",
        "docId": 642509,
        "score": 0.4718719365335957,
        "userScore":2
    },
    {
        "title": "Inflation and unemployment in Japan: from 1980 to 2050",
        "paperAbstract": "  The evolution of inflation, p(t), and unemployment, UE(t), in Japan has been\nmodeled. Both variables were represented as linear functions of the change rate\nof labor force, dLF/LF. These models provide an accurate description of\ndisinflation in the 1990s and a deflationary period in the 2000s. In Japan,\nthere exists a statistically reliable (R2\u003d0.68) Phillips curve, which is\ncharacterized by a negative relation between inflation and unemployment and\ntheir synchronous evolution: UE(t) \u003d -0.94p(t) + 0.045. Effectively, growing\nunemployment has resulted in decreasing inflation since 1982. A linear and\nlagged generalized relationship between inflation, unemployment and labor force\nhas been also obtained for Japan: p(t) \u003d 2.8*dLF(t)/LF(t) + 0.9*UE(t) - 0.0392.\nLabor force projections allow a prediction of inflation and unemployment in\nJapan: CPI inflation will be negative (between -0.5% and -1% per year) during\nthe next 40 years. Unemployment will increase from ~4.0% in 2010 to 5.3% in\n2050.\n",
        "query": "CPI GDP",
        "docId": 171036,
        "score": 0.47136533172936623,
        "userScore":1
    },
    {
        "title": "Inflation, unemployment, and labour force. Phillips curves and long-term\n  projections for Austria",
        "paperAbstract": "  We model the rate of inflation and unemployment in Austria since the early\n1960s within the Phillips/Fisher framework. The change in labour force is the\ndriving force representing economic activity in the Phillips curve. For\nAustria, this macroeconomic variable was first tested as a predictor of\ninflation and unemployment in 2005 with the involved time series ended in 2003.\nHere we extend all series by nine new readings available since 2003 and\nre-estimate the previously estimated relationships between inflation,\nunemployment, and labour force. As before, a structural break is allowed in\nthese relationships, which is related to numerous changes in definitions in the\n1980s. The break year is estimated together with other model parameters by the\nBoundary Element Method with the LSQ fitting between observed and predicted\nintegral curves. The precision of inflation prediction, as described by the\nroot-mean-square (forecasting) error is by 20% to 70% better than that\nestimated by AR(1) model. The estimates of model forecasting error are\navailable for those time series where the change in labour force leads by one\n(the GDP deflator) or two (CPI) years. For the whole period between 1965 and\n2012 as well as for the intervals before and after the structural break (1986\nfor all inflation models) separately, our model is superior to the na\\\"ive\nforecasting, which in turn, is not worse than any other forecasting model. The\nlevel of statistical reliability and the predictive power of the link between\ninflation and labour force imply that the National Bank of Austria does not\ncontrol inflation and unemployment beyond revisions to definitions. The labour\nforce projection provided by Statistic Austria allows foreseeing inflation at a\nforty-year horizon: the rate of CPI inflation will hover around 1.3% and the\nGDP deflator will likely sink below zero between 2018 and 2034.\n",
        "query": "CPI GDP",
        "docId": 466789,
        "score": 0.46350647458069005,
        "userScore":1
    },
    {
        "title": "Real GDP per capita in developed countries",
        "paperAbstract": "  Growth rate of real GDP per capita is represented as a sum of two components\n-- a monotonically decreasing economic trend and fluctuations related to a\nspecific age population change. The economic trend is modeled by an inverse\nfunction of real GDP per capita with a numerator potentially constant for the\nlargest developed economies. Statistical analysis of 19 selected OECD countries\nfor the period between 1950 and 2004 shows a very weak linear trend in the\nannual GDP per capita increment for the largest economies: the USA, Japan,\nFrance, Italy, and Spain. The UK, Australia, and Canada show a larger positive\nlinear trend. The fluctuations around the trend values are characterized by a\nquasi-normal distribution with potentially Levy distribution for far tails.\nDeveloping countries demonstrate the increment values far below the mean\nincrement for the most developed economies. This indicates an underperformance\nin spite of large relative growth rates.\n",
        "query": "CPI GDP",
        "docId": 92241,
        "score": 0.4592534727229278,
        "userScore":2
    },
    {
        "title": "Rate and cost of adaptation in the Drosophila Genome",
        "paperAbstract": "  Recent studies have consistently inferred high rates of adaptive molecular\nevolution between Drosophila species. At the same time, the Drosophila genome\nevolves under different rates of recombination, which results in partial\ngenetic linkage between alleles at neighboring genomic loci. Here we analyze\nhow linkage correlations affect adaptive evolution. We develop a new inference\nmethod for adaptation that takes into account the effect on an allele at a\nfocal site caused by neighboring deleterious alleles (background selection) and\nby neighboring adaptive substitutions (hitchhiking). Using complete genome\nsequence data and fine-scale recombination maps, we infer a highly\nheterogeneous scenario of adaptation in Drosophila. In high-recombining\nregions, about 50% of all amino acid substitutions are adaptive, together with\nabout 20% of all substitutions in proximal intergenic regions. In\nlow-recombining regions, only a small fraction of the amino acid substitutions\nare adaptive, while hitchhiking accounts for the majority of these changes.\nHitchhiking of deleterious alleles generates a substantial collateral cost of\nadaptation, leading to a fitness decline of about 30/2N per gene and per\nmillion years in the lowest-recombining regions. Our results show how\nrecombination shapes rate and efficacy of the adaptive dynamics in eukaryotic\ngenomes.\n",
        "query": "genomics adaptive evolution",
        "docId": 554371,
        "score": 0.642545730818,
        "userScore":1
    },
    {
        "title": "Adaptive evolution on neutral networks",
        "paperAbstract": "  We study the evolution of large but finite asexual populations evolving in\nfitness landscapes in which all mutations are either neutral or strongly\ndeleterious. We demonstrate that despite the absence of higher fitness\ngenotypes, adaptation takes place as regions with more advantageous\ndistributions of neutral genotypes are discovered. Since these discoveries are\ntypically rare events, the population dynamics can be subdivided into separate\nepochs, with rapid transitions between them. Within one epoch, the average\nfitness in the population is approximately constant. The transitions between\nepochs, however, are generally accompanied by a significant increase in the\naverage fitness. We verify our theoretical considerations with two analytically\ntractable bitstring models.\n",
        "query": "genomics adaptive evolution",
        "docId": 2187490,
        "score": 0.6403135469191863,
        "userScore":2
    },
    {
        "title": "Evolution of genome size in asexual populations",
        "paperAbstract": "  Genome sizes have evolved to vary widely, from 250 bases in viroids to 670\nbillion bases in amoeba. This remarkable variation in genome size is the\noutcome of complex interactions between various evolutionary factors such as\npoint mutation rate, population size, insertions and deletions, and genome\nediting mechanisms that may be specific to certain taxonomic lineages. While\ncomparative genomics analyses have uncovered some of the relationships between\nthese diverse evolutionary factors, we still do not understand what drives\ngenome size evolution. Specifically, it is not clear how primordial mutational\nprocesses of base substitutions, insertions, and deletions influence genome\nsize evolution in asexual organisms. Here, we use digital evolution to\ninvestigate genome size evolution by tracking genome edits and their fitness\neffects in real time. In agreement with empirical data, we find that mutation\nrate is inversely correlated with genome size in asexual populations. We show\nthat at low point mutation rate, insertions are significantly more beneficial\nthan deletions, driving genome expansion and acquisition of phenotypic\ncomplexity. Conversely, high mutational load experienced at high mutation rates\ninhibits genome growth, forcing the genomes to compress genetic information.\nOur analyses suggest that the inverse relationship between mutation rate and\ngenome size is a result of the tradeoff between evolving phenotypic innovation\nand limiting the mutational load.\n",
        "query": "genomics adaptive evolution",
        "docId": 678728,
        "score": 0.6207307979839225,
        "userScore":2
    },
    {
        "title": "Evolution of differentiated expression patterns in digital organisms",
        "paperAbstract": "  We investigate the evolutionary processes behind the development and\noptimization of multiple threads of execution in digital organisms using the\navida platform, a software package that implements Darwinian evolution on\npopulations of self-replicating computer programs. The system is seeded with a\nlinearly executed ancestor capable only of reproducing its own genome, whereas\nits underlying language has the capacity for multiple threads of execution\n(i.e., simultaneous expression of sections of the genome.) We witness the\nevolution to multi-threaded organisms and track the development of distinct\nexpression patterns. Additionally, we examine both the evolvability of\nmulti-threaded organisms and the level of thread differentiation as a function\nof environmental complexity, and find that differentiation is more pronounced\nin complex environments.\n",
        "query": "genomics adaptive evolution",
        "docId": 2186453,
        "score": 0.5950588133439325,
        "userScore":1
    },
    {
        "title": "Evolution: Life has Evolved to Evolve",
        "paperAbstract": "  Jim Shapiro synthesizes a great many observations about the mechanisms of\nevolution to reach the remarkable conclusion that large-scale modification,\nexchange, and rearrangement of the genome are common and should be viewed as\nfundamental features of life. In other words, the genome should be viewed not\nas mostly read-only with a few rare mutations, but rather as a fully-fledged\nread-write library of genetic functions under continuous revision. Revision of\nthe genome occurs during cellular replication, during multicellular\ndevelopment, and during evolution of a population of individuals. DNA\nformatting controls the timing and location of genetic rearrangements, gene\nexpression, and genetic repair. Each of these events is under the control of\nprecise cellular circuits. Shapiro reviews the toolbox of natural genetic\nengineering that provides the functionalities necessary for efficient long-term\ngenome restructuring.\n",
        "query": "genomics adaptive evolution",
        "docId": 497993,
        "score": 0.5924607762453729,
        "userScore":2
    },
    {
        "title": "On the Law of Directionality of Genome Evolution",
        "paperAbstract": "  The problem of the directionality of genome evolution is studied from the\ninformation-theoretic view. We propose that the function-coding information\nquantity of a genome always grows in the course of evolution through sequence\nduplication, expansion of code, and gene transfer between genomes. The\nfunction-coding information quantity of a genome consists of two parts,\np-coding information quantity which encodes functional protein and n-coding\ninformation quantity which encodes other functional elements except amino acid\nsequence. The relation of the proposed law to the thermodynamic laws is\nindicated. The evolutionary trends of DNA sequences revealed by bioinformatics\nare investigated which afford further evidences on the evolutionary law. It is\nargued that the directionality of genome evolution comes from species\ncompetition adaptive to environment. An expression on the evolutionary rate of\ngenome is proposed that the rate is a function of Darwin temperature\n(describing species competition) and fitness slope (describing adaptive\nlandscape). Finally, the problem of directly experimental test on the\nevolutionary directionality is discussed briefly.\n",
        "query": "genomics adaptive evolution",
        "docId": 62262,
        "score": 0.5845099499425697,
        "userScore":3
    },
    {
        "title": "Speed of evolution in large asexual populations with diminishing returns",
        "paperAbstract": "  The adaptive evolution of large asexual populations is generally\ncharacterized by competition between clones carrying different beneficial\nmutations. This interference phenomenon slows down the adaptation speed and\nmakes the theoretical description of the dynamics more complex with respect to\nthe successional occurrence and fixation of beneficial mutations typical of\nsmall populations. A simplified modeling framework considering multiple\nbeneficial mutations with equal and constant fitness advantage captures some of\nthe essential features of the actual complex dynamics, and some key predictions\nfrom this model are verified in laboratory evolution experiments. However, in\nthese experiments the relative advantage of a beneficial mutation is generally\ndependent on the genetic background. In particular, the general pattern is\nthat, as mutations in different loci accumulate, the relative advantage of new\nmutations decreases, trend often referred to as \"diminishing return\" epistasis.\n  In this paper, we propose a phenomenological model that generalizes the\nfixed-advantage framework to include in a simple way this feature. To evaluate\nthe quantitative consequences of diminishing returns on the evolutionary\ndynamics, we approach the model analytically as well as with direct\nsimulations. Finally, we show how the model parameters can be matched with data\nfrom evolutionary experiments in order to infer the mean effect of epistasis\nand derive order-of-magnitude estimates of the rate of beneficial mutations.\nApplying this procedure to two experimental data sets gives values of the\nbeneficial mutation rate within the range of previous measurements.\n",
        "query": "genomics adaptive evolution",
        "docId": 379459,
        "score": 0.5829751621979469,
        "userScore":2
    },
    {
        "title": "Evolution of biosequence search algorithms: a brief survey",
        "paperAbstract": "  The paper surveys the evolution of main algorithmic techniques to compare and\nsearch biological sequences. We highlight key algorithmic ideas emerged in\nresponse to several interconnected factors: shifts of biological analytical\nparadigm, advent of new sequencing technologies, and a substantial increase in\nsize of the available data. We discuss the expansion of alignment-free\ntechniques coming to replace alignment-based algorithms in large-scale\nanalyses. We further emphasize recently emerged and growing applications of\nsketching methods which support comparison of massive datasets, such as\nmetagenomics samples. Finally, we focus on the transition to population\ngenomics and outline associated algorithmic challenges.\n",
        "query": "genomics adaptive evolution",
        "docId": 1010184,
        "score": 0.5828596437048645,
        "userScore":0
    },
    {
        "title": "The structure of the genotype-phenotype map strongly constrains the\n  evolution of non-coding RNA",
        "paperAbstract": "  The prevalence of neutral mutations implies that biological systems typically\nhave many more genotypes than phenotypes. But can the way that genotypes are\ndistributed over phenotypes determine evolutionary outcomes? Answering such\nquestions is difficult because the number of genotypes can be\nhyper-astronomically large. By solving the genotype-phenotype (GP) map for RNA\nsecondary structure for systems up to length $L\u003d126$ nucleotides (where the set\nof all possible RNA strands would weigh more than the mass of the visible\nuniverse) we show that the GP map strongly constrains the evolution of\nnon-coding RNA (ncRNA). Simple random sampling over genotypes predicts the\ndistribution of properties such as the mutational robustness or the number of\nstems per secondary structure found in naturally occurring ncRNA with\nsurprising accuracy. Since we ignore natural selection, this strikingly close\ncorrespondence with the mapping suggests that structures allowing for\nfunctionality are easily discovered, despite the enormous size of the genetic\nspaces. The mapping is extremely biased: the majority of genotypes map to an\nexponentially small portion of the morphospace of all biophysically possible\nstructures. Such strong constraints provide a non-adaptive explanation for the\nconvergent evolution of structures such as the hammerhead ribozyme. These\nresults presents a particularly clear example of bias in the arrival of\nvariation strongly shaping evolutionary outcomes and may be relevant to Mayr\u0027s\ndistinction between proximate and ultimate causes in evolutionary biology.\n",
        "query": "genomics adaptive evolution",
        "docId": 633796,
        "score": 0.5811662985487374,
        "userScore":1
    },
    {
        "title": "Algorithmically probable mutations reproduce aspects of evolution such\n  as convergence rate, genetic memory, and modularity",
        "paperAbstract": "  Natural selection explains how life has evolved over millions of years from\nmore primitive forms. The speed at which this happens, however, has sometimes\ndefied formal explanations when based on random (uniformly distributed)\nmutations. Here we investigate the application of a simplicity bias based on a\nnatural but algorithmic distribution of mutations (no recombination) in various\nexamples, particularly binary matrices in order to compare evolutionary\nconvergence rates. Results both on synthetic and on small biological examples\nindicate an accelerated rate when mutations are not statistical uniform but\n\\textit{algorithmic uniform}. We show that algorithmic distributions can evolve\nmodularity and genetic memory by preservation of structures when they first\noccur sometimes leading to an accelerated production of diversity but also\npopulation extinctions, possibly explaining naturally occurring phenomena such\nas diversity explosions (e.g. the Cambrian) and massive extinctions (e.g. the\nEnd Triassic) whose causes are currently a cause for debate. The natural\napproach introduced here appears to be a better approximation to biological\nevolution than models based exclusively upon random uniform mutations, and it\nalso approaches a formal version of open-ended evolution based on previous\nformal results. These results validate some suggestions in the direction that\ncomputation may be an equally important driver of evolution. We also show that\ninducing the method on problems of optimization, such as genetic algorithms,\nhas the potential to accelerate convergence of artificial evolutionary\nalgorithms.\n",
        "query": "genomics adaptive evolution",
        "docId": 885134,
        "score": 0.5762906218288446,
        "userScore":2
    },
    {
        "title": "H2 distribution during formation of multiphase molecular clouds",
        "paperAbstract": "  H2 is the simplest and the most abundant molecule in the ISM, and its\nformation precedes the formation of other molecules. Understanding the\ndynamical influence of the environment and the interplay between the thermal\nprocesses related to the formation and destruction of H2 and the structure of\nthe cloud is mandatory to understand correctly the observations of H2. We\nperform high resolution MHD colliding flow simulations with the AMR code RAMSES\nin which the physics of H2 has been included. We compare the simulation results\nwith various observations including the column densities of excited rotational\nlevels. Due to a combination of thermal pressure, ram pressure and gravity, the\nclouds produced at the converging point of HI streams are highly inhomogeneous.\nH2 molecules quickly form in relatively dense clumps and spread into the\ndiffuse interclump gas. This in particular leads to the existence of\nsignificant abundances of H2 in the diffuse and warm gas that lies in between\nclumps. Simulations and observations show similar trends, specially for the\nHI-to-H2 transition. The abundances of excited rotational levels, calculated at\nequilibrium in the simulations are very similar to the observed abundances\ninferred from FUSE results. This is a direct consequence of the presence of the\nH2 enriched diffuse and warm gas. Our simulations show that H2 rapidly forms in\nthe dense clumps and, due to the complex structure of molecular clouds, quickly\nspreads at lower densities. Consequently a significant fraction of warm H2\nexists in the low density gas. This warm H2 leads to column densities of\nexcited rotational levels close to the observed ones likely revealing the\ncomplex intermix between the warm and the cold gas in molecular clouds. This\nsuggests that the 2-phase structure of molecular clouds is an essential\ningredient to fully understand molecular hydrogen in these objects.\n",
        "query": "Molecular hydrogen clouds",
        "docId": 688175,
        "score": 0.7351291044998263,
        "userScore":2
    },
    {
        "title": "Chemical Evolution of Turbulent Multiphase Molecular Clouds",
        "paperAbstract": "  Molecular clouds are essentially made up of atomic and molecular hydrogen,\nwhich in spite of being the simplest molecule in the ISM plays a key role in\nthe chemical evolution of molecular clouds. Since its formation time is very\nlong, the H2 molecules can be transported by the turbulent motions within the\ncloud toward low density and warm regions, where its enhanced abundance can\nboost the abundances of molecules with high endothermicities. We present high\nresolution simulations where we include the evolution of the molecular gas\nunder the effect of the dynamics, and we analyze its impact on the abundance of\nCH+.\n",
        "query": "Molecular hydrogen clouds",
        "docId": 908834,
        "score": 0.7340804737275377,
        "userScore":2
    },
    {
        "title": "The Transition from Atomic to Molecular Hydrogen in Interstellar Clouds:\n  21cm Signature of the Evolution of Cold Atomic Hydrogen in Dense Clouds",
        "paperAbstract": "  We have investigated the time scale for formation of molecular clouds by\nexamining the conversion of HI to H2 using a time-dependent model. H2 formation\non dust grains and cosmic ray and photo destruction are included in\none-dimensional model slab clouds which incorporate time-independent density\nand temperature distributions. We calculate 21cm spectral line profiles seen in\nabsorption against a background provided by general Galactic HI emission, and\ncompare the model spectra with HI Narrow Self-Absorption, or HINSA, profiles\nabsorbed in a number of nearby molecular clouds. The time evolution of the HI\nand H2 densities is dramatic, with the atomic hydrogen disappearing in a wave\npropagating from the central, denser regions which have a shorter H2 formation\ntime scale, to the edges, where the density is lower and the time scale for H2\nformation longer. The model 21cm spectra are characterized by very strong\nabsorption at early times, when the HI column density through the model clouds\nis extremely large. The minimum time required for a cloud to have evolved to\nits observed configuration, based on the model spectra, is set by the\nrequirement that most of the HI in the outer portions of the cloud, which\notherwise overwhelms the narrow absorption, be removed. The characteristic time\nthat has elapsed since cloud compression and initiation of the HI to H2\nconversion is a few x 10^{14} s or ~ 10^7 yr. This sets a minimum time for the\nage of these molecular clouds and thus for the star formation that may take\nplace within them.\n",
        "query": "Molecular hydrogen clouds",
        "docId": 1866885,
        "score": 0.726231440642847,
        "userScore":2
    },
    {
        "title": "The Formation of Molecular Clouds",
        "paperAbstract": "  In a recent paper, Elmegreen (2000) has made a cogent case, from an\nobservational point of view, that the lifetimes of molecular clouds are\ncomparable to their dynamical timescales. If so, this has important\nimplications for the mechanisms by which molecular clouds form. In particular\nwe consider the hypothesis that molecular clouds may form not by {\\it in situ}\ncooling of atomic gas, but rather by the agglomeration of the dense phase of\nthe interstellar medium (ISM), much, if not most, of which is already in\nmolecular form.\n",
        "query": "Molecular hydrogen clouds",
        "docId": 1824309,
        "score": 0.7141891135510701,
        "userScore":3
    },
    {
        "title": "H2 distribution during 2-phase Molecular Cloud Formation",
        "paperAbstract": "  We performed high-resolution, 3D MHD simulations and we compared to\nobservations of translucent molecular clouds. We show that the observed\npopulations of rotational levels of H2 can arise as a consequence of the\nmulti-phase structure of the ISM.\n",
        "query": "Molecular hydrogen clouds",
        "docId": 686631,
        "score": 0.7026241298652209,
        "userScore":2
    },
    {
        "title": "Molecular Cloud Formation Behind Shock Waves",
        "paperAbstract": "  We examine the formation of molecular gas behind shocks in atomic gas using a\nchemical/dynamical model, particular emphasis is given to constraints the\nchemistry places on the dynamical evolution. The most important result of this\nstudy is to stress the importance of shielding the molecular gas from the\ndestructive effects of UV radiation. For shock ram pressures comparable to or\nexceeding typical local ISM pressures, self-shielding controls the formation\ntime of H2 but CO formation requires shielding of the interstellar radiation\nfield by dust grains. We find that the molecular hydrogen fractional abundance\ncan become significant well before CO forms. The timescale for (CO) molecular\ncloud formation is not set by H2 formation, but rather by the timescale for\naccumulating a sufficient column density or extinction, A_V \u003e 0.7. The local\nratio of atomic to molecular gas (4:1), coupled with short estimates for cloud\nlifetimes (3-5 Myr), suggests that the timescales for accumulating molecular\nclouds from atomic material typically must be no longer than about 12-20 Myr.\nBased on the shielding requirement, this implies that the typical product of\npre-shock density and velocity must be n*v \u003e 20 cm^-3 km s^-1. Based on these\nresults we find that flow-driven formation of molecular clouds in the local\ninterstellar medium can occur sufficiently rapidly to account for observations.\nWe also provide detailed predictions of atomic and molecular emission and\nabsorption that track molecular cloud formation, with a view toward helping to\nverify cloud formation by shock waves. Finally, we provide an analytic solution\nfor time-dependent H2 formation which may be of use in numerical hydrodynamic\ncalculations.\n",
        "query": "Molecular hydrogen clouds",
        "docId": 1845942,
        "score": 0.6974374793239804,
        "userScore":2
    },
    {
        "title": "Laboratory evidence for the non-detection of excited nascent H2 in dark\n  clouds",
        "paperAbstract": "  There has always been a great deal of interest in the formation of H2 as well\nas in the binding energy released upon its formation on the surface of dust\ngrains. The present work aims at collecting experimental evidence for how the\nbond energy budget of H2 is distributed between the reaction site and the\ninternal energy of the molecule. So far, the non-detection of excited nascent\nH2 in dense quiescent clouds could be a sign that either predictions of\nemission line intensities are not correct or the de-excitation of the newly\nformed molecules proceeds rapidly on the grain surface itself. In this letter\nwe present experimental evidence that interstellar molecular hydrogen is formed\nand then rapidly de-excited on the surface of porous water ice mantles. In\naddition, although we detect ro-vibrationally excited nascent molecules\ndesorbing from a bare non-porous (compact) water ice film, we demonstrate that\nthe amount of excited nascent hydrogen molecules is significantly reduced no\nmatter the morphology of the water ice substrate at 10 K (both on non-porous\nand on porous water ice) in a regime of high molecular coverage as is the case\nin dark molecular clouds.\n",
        "query": "Molecular hydrogen clouds",
        "docId": 127274,
        "score": 0.6950037978826327,
        "userScore":1
    },
    {
        "title": "Formation Pumping of Molecular Hydrogen in Dark Clouds",
        "paperAbstract": "  Many theoretical and laboratory studies predict H2 to be formed in highly\nexcited ro-vibrational states. The consequent relaxation of excited levels via\na cascade of infrared transitions might be observable in emission from suitable\ninterstellar regions. In this work, we model H2 formation pumping in standard\ndense clouds, taking into account the H/H2 transition zone, through an accurate\ndescription of chemistry and radiative transfer. The model includes recent\nlaboratory data on H2 formation, as well as the effects of the interstellar UV\nfield, predicting the populations of gas-phase H2 molecules and their IR\nemission spectra. Calculations suggest that some vibrationally excited states\nof H2 might be detectable towards lines of sight where significant destruction\nof H2 occurs, such as X-ray sources, and provide a possible explanation as to\nwhy observational attempts resulted in no detections reported to date.\n",
        "query": "Molecular hydrogen clouds",
        "docId": 219428,
        "score": 0.6905018873629256,
        "userScore":2
    },
    {
        "title": "Molecular Hydrogen in Star-forming regions: implementation of its\n  micro-physics in Cloudy",
        "paperAbstract": "  Much of the baryonic matter in the Universe is in the form of H2 which\nincludes most of the gas in galactic and extragalactic interstellar clouds.\nMolecular hydrogen plays a significant role in establishing the thermal balance\nin many astrophysical environments and can be important as a spectral\ndiagnostic of the gas. Modeling and interpretation of observations of such\nenvironments requires a quantitatively complete and accurate treatment of H2.\nUsing this micro-physical model of H2, illustrative calculations of\nprototypical astrophysical environments are presented. This work forms the\nfoundation for future investigations of these and other environments where H2\nis an important constituent.\n",
        "query": "Molecular hydrogen clouds",
        "docId": 1851690,
        "score": 0.6870555719888713,
        "userScore":2
    },
    {
        "title": "Destruction of Molecular Hydrogen During Cosmological Reionization",
        "paperAbstract": "  We investigate the ability of primordial gas clouds to retain molecular\nhydrogen (H_2) during the initial phase of the reionization epoch. We find that\nbefore the Stromgren spheres of the individual ionizing sources overlap, the UV\nbackground below the ionization threshold is able to penetrate large clouds and\nsuppress their H_2 abundance. The consequent lack of H_2 cooling could prevent\nthe collapse and fragmentation of clouds with virial temperatures T_vir \u003c 10^4\nK (or masses 10^8 Msun [(1+z_vir)/10]^{-3/2}). This negative feedback on\nstructure-formation arises from the very first ionizing sources, and precedes\nthe feedback due to the photoionization heating.\n",
        "query": "Molecular hydrogen clouds",
        "docId": 1876341,
        "score": 0.6847822131566641,
        "userScore":3
    },
    {
        "title": "Partial Univalence in n-truncated Type Theory",
        "paperAbstract": "  It is well known that univalence is incompatible with uniqueness of identity\nproofs (UIP), the axiom that all types are h-sets. This is due to finite h-sets\nhaving non-trivial automorphisms as soon as they are not h-propositions.\n  A natural question is then whether univalence restricted to h-propositions is\ncompatible with UIP. We answer this affirmatively by constructing a model where\ntypes are elements of a closed universe defined as a higher inductive type in\nhomotopy type theory. This universe has a path constructor for simultaneous\n\"partial\" univalent completion, i.e., restricted to h-propositions.\n  More generally, we show that univalence restricted to $(n-1)$-types is\nconsistent with the assumption that all types are $n$-truncated. Moreover we\nparametrize our construction by a suitably well-behaved container, to abstract\nfrom a concrete choice of type formers for the universe.\n",
        "query": "Types cancer",
        "docId": 1280055,
        "score": 0.3150615220974302,
        "userScore":1
    },
    {
        "title": "Properties of the intermediate type of gamma-ray bursts",
        "paperAbstract": "  Gamma-ray bursts can be divided into three groups (\"short\", \"intermediate\",\n\"long\") with respect to their durations. The third type of gamma-ray bursts -\nas known - has the intermediate duration. We show that the intermediate group\nis the softest one. An anticorrelation between the hardness and the duration is\nfound for this subclass in contrast to the short and long groups.\n",
        "query": "Types cancer",
        "docId": 1869678,
        "score": 0.3138014542637393,
        "userScore":0
    },
    {
        "title": "Newton-Okounkov polytopes of flag varieties for classical groups",
        "paperAbstract": "  For classical groups SL(n), SO(n) and Sp(2n), we define uniformly geometric\nvaluations on the corresponding complete flag varieties. The valuation in every\ntype comes from a natural coordinate system on the open Schubert cell and is\ncombinatorially related to the Gelfand-Zetlin pattern in the same type. In\ntypes A and C, we identify the corresponding Newton-Okounkov polytopes with the\nFeigin-Fourier-Littelmann-Vinberg polytopes. In types B and D, we compute\nlow-dimensional examples and formulate open questions.\n",
        "query": "Types cancer",
        "docId": 1083595,
        "score": 0.310285529259545,
        "userScore":0
    },
    {
        "title": "Homotopy-initial algebras in type theory",
        "paperAbstract": "  We investigate inductive types in type theory, using the insights provided by\nhomotopy type theory and univalent foundations of mathematics. We do so by\nintroducing the new notion of a homotopy-initial algebra. This notion is\ndefined by a purely type-theoretic contractibility condition which replaces the\nstandard, category-theoretic universal property involving the existence and\nuniqueness of appropriate morphisms. Our main result characterises the types\nthat are equivalent to W-types as homotopy-initial algebras.\n",
        "query": "Types cancer",
        "docId": 617436,
        "score": 0.30972775579413536,
        "userScore":0
    },
    {
        "title": "Type theory and homotopy",
        "paperAbstract": "  The purpose of this survey article is to introduce the reader to a connection\nbetween Logic, Geometry, and Algebra which has recently come to light in the\nform of an interpretation of the constructive type theory of Martin-L\\\"of into\nhomotopy theory, resulting in new examples of higher-dimensional categories.\n",
        "query": "Types cancer",
        "docId": 218198,
        "score": 0.30575213363042053,
        "userScore":0
    },
    {
        "title": "Two Classes of Gamma-Ray Bursts",
        "paperAbstract": "  Data from the 3B Catalogue suggest that short and long GRB are the results of\ndifferent classes of events, rather than different parameter values within a\nsingle class: Short bursts have harder spectra in the BATSE bands, but chiefly\nlong bursts are detected at photon energies over 1 MeV, implying that their\nhard photons are radiated by a process not found in short bursts. The values of\n\\langle V/V_{max} \\rangle for short and long bursts differ by 4.3 \\sigma,\nimplying different spatial distributions. Only the soft gamma-ray radiation\nmechanisms are the same in both classes.\n",
        "query": "Types cancer",
        "docId": 1874851,
        "score": 0.29863465024411084,
        "userScore":0
    },
    {
        "title": "Non-wellfounded trees in Homotopy Type Theory",
        "paperAbstract": "  We prove a conjecture about the constructibility of coinductive types - in\nthe principled form of indexed M-types - in Homotopy Type Theory. The\nconjecture says that in the presence of inductive types, coinductive types are\nderivable. Indeed, in this work, we construct coinductive types in a subsystem\nof Homotopy Type Theory; this subsystem is given by Intensional Martin-L\\\"of\ntype theory with natural numbers and Voevodsky\u0027s Univalence Axiom. Our results\nare mechanized in the computer proof assistant Agda.\n",
        "query": "Types cancer",
        "docId": 614854,
        "score": 0.29733865380239166,
        "userScore":0
    },
    {
        "title": "Homotopy Type Theory: A synthetic approach to higher equalities",
        "paperAbstract": "  This is an introduction to Homotopy Type Theory and Univalent Foundations for\nphilosophers, written as a chapter for the book \"Categories for the Working\nPhilosopher\" (ed. Elaine Landry)\n",
        "query": "Types cancer",
        "docId": 697063,
        "score": 0.2930280756178192,
        "userScore":0
    },
    {
        "title": "Homotopies for Free!",
        "paperAbstract": "  We show \"free theorems\" in the style of Wadler for polymorphic functions in\nhomotopy type theory as consequences of the abstraction theorem. As an\napplication, it follows that every space defined as a higher inductive type has\nthe same homotopy groups as some type of polymorphic functions defined without\nunivalence or higher inductive types.\n",
        "query": "Types cancer",
        "docId": 813345,
        "score": 0.2891912121394853,
        "userScore":0
    },
    {
        "title": "Higher Inductive Types as Homotopy-Initial Algebras",
        "paperAbstract": "  Homotopy Type Theory is a new field of mathematics based on the surprising\nand elegant correspondence between Martin-Lofs constructive type theory and\nabstract homotopy theory. We have a powerful interplay between these\ndisciplines - we can use geometric intuition to formulate new concepts in type\ntheory and, conversely, use type-theoretic machinery to verify and often\nsimplify existing mathematical proofs. A crucial ingredient in this new system\nare higher inductive types, which allow us to represent objects such as\nspheres, tori, pushouts, and quotients. We investigate a variant of higher\ninductive types whose computational behavior is determined up to a higher path.\nWe show that in this setting, higher inductive types are characterized by the\nuniversal property of being a homotopy-initial algebra.\n",
        "query": "Types cancer",
        "docId": 498286,
        "score": 0.28655301450635307,
        "userScore":0
    },
    {
        "title": "Schrodinger\u0027s cat versus Darwin",
        "paperAbstract": "  Sun Wu-k\u0027ung, an immortal Monkey-King of Chaos learns modern physics from the\nPatriarch Bodhi and questions the Darwinian evolution. He finds that the modern\nphysics indicates towards the intelligent design as a vastly more probably\norigin of humans than the random evolution by mutations and natural selection.\n",
        "query": "Human beings evolution",
        "docId": 116658,
        "score": 0.5768895275446013,
        "userScore":0
    },
    {
        "title": "How a Generation Was Misled About Natural Selection",
        "paperAbstract": "  This article explains how natural selection works and how it has been\ninappropriately applied to the description of cultural change. It proposes an\nalternative evolutionary explanation for cultural evolution that describes it\nin terms of communal exchange.\n",
        "query": "Human beings evolution",
        "docId": 591139,
        "score": 0.575466194571673,
        "userScore":2
    },
    {
        "title": "Dynamical Disequilibrium, Transformation, and the Evolution and\n  Development of Sustainable Worldviews",
        "paperAbstract": "  This chapter begins by outlining a promising, new theoretical framework for\nthe process by which human culture evolves inspired by the views of complexity\ntheorists on the problem of how life began. Elements of culture, like species,\nevolve over time; that is, they exhibit cumulative change that is adaptive in\nnature. By studying how biological evolution got started, we gain insight into\nnot just the specifics of biological evolution, but also general insights into\nthe initiation of any evolutionary process that may be applicable to culture.\nWe then explore the implications of this new framework for culture on the\ntransformative processes of individuals. Specifically, we will address what\nthis emerging perspective on cultural evolution implies for to go about\nattaining a sustainable worldview; that is, a web of habits, understandings,\nand ways of approaching situations that is conducive to the development of a\nsustainable world.\n",
        "query": "Human beings evolution",
        "docId": 459993,
        "score": 0.5453633165162284,
        "userScore":1
    },
    {
        "title": "Meme and Variations: A Computer Model of Cultural Evolution",
        "paperAbstract": "  Holland\u0027s (1975) genetic algorithm is a minimal computer model of natural\nselection that made it possible to investigate the effect of manipulating\nspecific parameters on the evolutionary process. If culture is, like biology, a\nform of evolution, it should be possible to similarly abstract the underlying\nskeleton of the process and develop a minimal model of it. Meme and Variations,\nor MAV, is a computational model, inspired by the genetic algorithm, of how\nideas evolve in a society of interacting individuals (Gabora 1995). The name is\na pun on the classical music form \u0027theme and variations\u0027, because it is based\non the premise that novel ideas are variations of old ones; they result from\ntweaking or combining existing ideas in new ways (Holland et al. 1981). MAV\nexplores the impact of biological phenomena such as over-dominance and\nepistasis as well as cognitive and social phenomena such as the ability to\nlearn generalizations or imitate others on the fitness and diversity of\ncultural transmissible actions.\n",
        "query": "Human beings evolution",
        "docId": 464532,
        "score": 0.5437925926960343,
        "userScore":0
    },
    {
        "title": "Distilling the Essence of an Evolutionary Process and Implications for a\n  Formal Description of Culture",
        "paperAbstract": "  It has been proposed that, since the origin of life and the ensuing evolution\nof biological species, a second evolutionary process has appeared on our\nplanet. It is the evolution of culture-e.g., ideas, beliefs, and artifacts.\nDoes culture evolve in the same genuine sense as biological life? And if so,\ndoes it evolve through natural selection, or by some other means? Why does no\nother species remotely approach the degree of cultural complexity of humans?\nThese questions lie at the foundation of who we are and what makes our lives\nmeaningful. Although much research has been done on how selective pressures\noperating at the biological level affect cognition and culture, little research\nhas focused on culture as an evolutionary process in its own right. Like\nbiological forms, cultural forms-ideas, attitudes, artifacts, mannerisms,\netc.-incrementally adapt to the constraints and affordances of their\nenvironment through descent with modification. In some respects culture appears\nto be Darwinian, i.e., a process of differential replication and selection\namongst randomly generated variants. This suggests that knowledge of biological\nevolution can be put to use to gain insight into culture. However, attempts to\napply Darwinian theory to culture have not yielded the kind of unifying\nframework for the social sciences that it provided for the biological sciences,\nlargely because of the nonrandom manner in which the mind-the hub of cultural\nchange-generates and assimilates novelty. This paper investigates how and when\nhumans became capable of supporting culture, and what previously held it back,\nfocusing on how we attained the creative powers we now possess. To invent in\nthe strategic, intuitive manner characteristic of humans requires a cognitive\narchitecture that supports the capacity to spontaneously adapt concepts to new\ncircumstances and merge them together to conceptualize new situations.\n",
        "query": "Human beings evolution",
        "docId": 461720,
        "score": 0.5431515687613455,
        "userScore":2
    },
    {
        "title": "Parallels and promising directions in the study of genetic, cultural,\n  and moral evolution",
        "paperAbstract": "  Experimental evolution has yielded surprising insights into human history and\nevolution by shedding light on the roles of chance and contingency in history\nand evolution, and on the deep evolutionary roots of cooperation, conflict and\nkin discrimination. We argue that an interesting research direction would be to\ndevelop computational and experimental systems for studying evolutionary\nprocesses that involve multiple layers of inheritance (such as genes,\nepigenetic inheritance, language, and culture) and feedbacks (such as\ngene-culture coevolution and mate choice) as well as open-ended niche\nconstruction---all of which are important in human history and evolution. Such\nsystems would also be a clear way to motivate evolution and computation to\nscholars and students across diverse cultural and socioeconomic backgrounds, as\nwell as to scholars and students in the social sciences and humanities. In\nprinciple, computational models of cultural evolution could be compared to\ndata, given that large-scale datasets already exist for tracking cultural\nchange in real-time. Thus, experimental evolution, as a laboratory and\ncomputational science, is poised to grow as an educational tool for people to\nquestion and study where we come from, why we believe what we believe, and\nwhere we as a species may be headed.\n",
        "query": "Human beings evolution",
        "docId": 1031688,
        "score": 0.5371112866441603,
        "userScore":2
    },
    {
        "title": "Darwin et la socialit\\\u0027e : entre anthropomorphisme et gradualisme",
        "paperAbstract": "  We propose, in this article, an analysis of the Darwin\u0027s approach to\nsociality. Sociality is perfectly integrated into the selective model, and is\ncaused by the same process as struggle for existence. Thus, the selective\nprocess does not prohibit demonstrating cooperation, but on the contrary,\nexplains it. We will see that the unification of sociality, in a more adequate\nrepresentation to mammals, is not an expression of a naive anthropomorphism,\nbut of a methodological anthropomorphism; normal consequence of his gradualist\napproach of phylogeny.\n",
        "query": "Human beings evolution",
        "docId": 76061,
        "score": 0.535650636116479,
        "userScore":1
    },
    {
        "title": "An Evolutionary Framework for Culture: Selectionism versus Communal\n  Exchange",
        "paperAbstract": "  Dawkins\u0027 replicator-based conception of evolution has led to widespread\nmis-application selectionism across the social sciences because it does not\naddress the paradox that inspired the theory of natural selection in the first\nplace: how do organisms accumulate change when traits acquired over their\nlifetime are obliterated? This is addressed by von Neumann\u0027s concept of a\nself-replicating automaton (SRA). A SRA consists of a self-assembly code that\nis used in two distinct ways: (1) actively deciphered during development to\nconstruct a self-similar replicant, and (2) passively copied to the replicant\nto ensure that it can reproduce. Information that is acquired over a lifetime\nis not transmitted to offspring, whereas information that is inherited during\ncopying is transmitted. In cultural evolution there is no mechanism for\ndiscarding acquired change. Acquired change can accumulate orders of magnitude\nfaster than, and quickly overwhelm, inherited change due to differential\nreplication of variants in response to selection. This prohibits a selectionist\nbut not an evolutionary framework for culture. Recent work on the origin of\nlife suggests that early life evolved through a non-Darwinian process referred\nto as communal exchange that does not involve a self-assembly code, and that\nnatural selection emerged from this more haphazard, ancestral evolutionary\nprocess. It is proposed that communal exchange provides a more appropriate\nevolutionary framework for culture than selectionism. This is supported by a\ncomputational model of cultural evolution and a network-based program for\ndocumenting material cultural history, and it is consistent with high levels of\nhuman cooperation.\n",
        "query": "Human beings evolution",
        "docId": 350479,
        "score": 0.5318310680715896,
        "userScore":1
    },
    {
        "title": "The Extended Evolutionary Synthesis Facilitates Evolutionary Models of\n  Culture Change",
        "paperAbstract": "  The Extended Evolutionary Synthesis (EES) is beginning to fulfill the whole\npromise of Darwinian insight through its extension of evolutionary\nunderstanding from the biological domain to include cultural information\nevolution. Several decades of important foundation-laying work took a social\nDarwinist approach and exhibited and ecologically-deterministic elements. This\nis not the case with more recent developments to the evolutionary study of\nculture, which emphasize non-Darwinian processes such as self-organization,\npotentiality, and epigenetic change.\n",
        "query": "Human beings evolution",
        "docId": 1057717,
        "score": 0.531642101881566,
        "userScore":0
    },
    {
        "title": "Evolutionary foundations of cooperation and group cohesion",
        "paperAbstract": "  In biology, the evolution of increasingly cooperative groups has shaped the\nhistory of life. Genes collaborate in the control of cells; cells efficiently\ndivide tasks to produce cohesive multicellular individuals; individual members\nof insect colonies cooperate in integrated societies. Biological cooperation\nprovides a foundation on which to understand human behavior. Conceptually, the\neconomics of efficient allocation and the game-like processes of strategy are\nwell understood in biology; we find the same essential processes in many\nsuccessful theories of human sociality. Historically, the trace of biological\nevolution informs in two ways. First, the evolutionary transformations in\nbiological cooperation provide insight into how economic and strategic\nprocesses play out over time--a source of analogy that, when applied\nthoughtfully, aids analysis of human sociality. Second, humans arose from\nbiological history--a factual account of the past that tells us much about the\nmaterial basis of human behavior.\n",
        "query": "Human beings evolution",
        "docId": 308601,
        "score": 0.530641954201819,
        "userScore":0
    },
    {
        "title": "The asymptotic expansion for the factorial and Lagrange inversion\n  formula",
        "paperAbstract": "  We obtain an explicit simple formula for the coefficients of the asymptotic\nexpansion for the factorial of a natural number,in terms of derivatives of\npowers of an elementary function. The unique explicit expression for the\ncoefficients that appears to be known is that in the book by L. Comtet, which\nis given in terms of sums of associated Stirling numbers of the first kind. By\nconsidering the bivariate generating function of the associated Stirling\nnumbers of the second kind, another expression for the coefficients in terms of\nthem follows also from our analysis. Comparison with Comtet\u0027s expression yields\ncombinatorial identities between associated Stirling numbers of first and\nsecond kind. It suggests by analogy another possible formula for the\ncoefficients, in terms of a function involving the logarithm, that in fact\nproves to be true. The resulting coefficients, as well as the first ones are\nidentified via the Lagrange inversion formula as the odd coefficients of the\ninverse of a pair of formal series, which permits us to obtain also some\nrecurrences.\n",
        "query": "expansion falling factorial",
        "docId": 174653,
        "score": 0.543657135230859,
        "userScore":0
    },
    {
        "title": "Confluent hypergeometric expansions of the confluent Heun function\n  governed by two-term recurrence relations",
        "paperAbstract": "  We show that there exist infinitely many nontrivial choices of parameters of\nthe single confluent Heun equation for which the three-term recurrence\nrelations governing the expansions of the solutions in terms of the confluent\nhypergeometric functions 1F1 and 0F1 are reduced to two-term ones. In such\ncases the expansion coefficients are explicitly calculated in terms of the\nEuler gamma functions.\n",
        "query": "expansion falling factorial",
        "docId": 1220519,
        "score": 0.5297391866466636,
        "userScore":0
    },
    {
        "title": "A new entire factorial function",
        "paperAbstract": "  We introduce a new factorial function which agrees with the usual Euler gamma\nfunction at both the positive integers and at all half-integers, but which is\nalso entire. We describe the basic features of this function.\n",
        "query": "expansion falling factorial",
        "docId": 1505342,
        "score": 0.4967767672903207,
        "userScore":1
    },
    {
        "title": "Summation of Divergent Power Series by Means of Factorial Series",
        "paperAbstract": "  Factorial series played a major role in Stirling\u0027s classic book \"Methodus\nDifferentialis\" (1730), but now only a few specialists still use them. This\narticle wants to show that this neglect is unjustified, and that factorial\nseries are useful numerical tools for the summation of divergent (inverse)\npower series. This is documented by summing the divergent asymptotic expansion\nfor the exponential integral $E_{1} (z)$ and the factorially divergent\nRayleigh-Schr\\\"{o}dinger perturbation expansion for the quartic anharmonic\noscillator. Stirling numbers play a key role since they occur as coefficients\nin expansions of an inverse power in terms of inverse Pochhammer symbols and\nvice versa. It is shown that the relationships involving Stirling numbers are\nspecial cases of more general orthogonal and triangular transformations.\n",
        "query": "expansion falling factorial",
        "docId": 188008,
        "score": 0.488777952040202,
        "userScore":0
    },
    {
        "title": "Hypergeometric expansions of the general Heun function governed by\n  two-term recurrence relations",
        "paperAbstract": "  We show that there exist infinitely many particular choices of parameters for\nwhich the three-term recurrence relations governing the expansions of the\nsolutions of the general Heun equation in terms of the Gauss hypergeometric\nfunctions become two-term. In these cases the coefficients are explicitly\nwritten in terms of the gamma functions.\n",
        "query": "expansion falling factorial",
        "docId": 979564,
        "score": 0.4844759200827582,
        "userScore":0
    },
    {
        "title": "How to Compute a Puiseux Expansion",
        "paperAbstract": "  In this paper, an explanation of the Newton-Peiseux algorithm is given. This\nexplanation is supplemented with well-worked and explained examples of how to\nuse the algorithm to find fractional power series expansions for all branches\nof a polynomial at the origin.\n",
        "query": "expansion falling factorial",
        "docId": 75667,
        "score": 0.4807108419010717,
        "userScore":0
    },
    {
        "title": "Expansions of the solutions of the biconfluent Heun equation in terms of\n  incomplete Beta and Gamma functions",
        "paperAbstract": "  Starting from equations obeyed by functions involving the first or the second\nderivatives of the biconfluent Heun function, we construct two expansions of\nthe solutions of the biconfluent Heun equation in terms of incomplete Beta\nfunctions. The first series applies single Beta functions as expansion\nfunctions, while the second one involves a combination of two Beta functions.\nThe coefficients of expansions obey four- and five-term recurrence relations,\nrespectively. It is shown that the proposed technique is potent to produce\nseries solutions in terms of other special functions. Two examples of such\nexpansions in terms of the incomplete Gamma functions are presented\n",
        "query": "expansion falling factorial",
        "docId": 569676,
        "score": 0.48023170828121664,
        "userScore":0
    },
    {
        "title": "On the all-order epsilon-expansion of generalized hypergeometric\n  functions with integer values of parameters",
        "paperAbstract": "  We continue our study of the construction of analytical coefficients of the\nepsilon-expansion of hypergeometric functions and their connection with Feynman\ndiagrams. In this paper, we apply the approach of obtaining iteratated\nsolutions to the differential equations associated with hypergeometric\nfunctions to prove the following result (Theorem 1): The epsilon-expansion of a\ngeneralized hypergeometric function with integer values of parameters is\nexpressible in terms of generalized polylogarithms with coefficients that are\nratios of polynomials. The method used in this proof provides an efficient\nalgorithm for calculatiing of the higher-order coefficients of Laurent\nexpansion.\n",
        "query": "expansion falling factorial",
        "docId": 18654,
        "score": 0.4759242089911595,
        "userScore":0
    },
    {
        "title": "Several series expansions for real powers and several formulas for\n  partial Bell polynomials of sinc and sinhc functions in terms of central\n  factorial and Stirling numbers of second kind",
        "paperAbstract": "  In the paper, with the aid of the Fa\\`a di Bruno formula, in terms of central\nfactorial numbers of the second kind, and with the terminology of the Stirling\nnumbers of the second kind, the authors derive several series expansions for\nany positive integer powers of the sinc and sinhc functions, discover several\nclosed-form formulas for partial Bell polynomials of all derivatives of the\nsinc function, establish several series expansions for any real powers of the\nsinc and sinhc functions, and present several identities for central factorial\nnumbers of the second kind and for the Stirling numbers of the second kind.\n",
        "query": "expansion falling factorial",
        "docId": 1635750,
        "score": 0.47255094983460566,
        "userScore":0
    },
    {
        "title": "On Powers of a Hypergeometric Function",
        "paperAbstract": "  A new expansion for integral powers of the hypergeometric function\ncorresponding to a special case of the incomplete beta function is summarized,\nand consequences, including two new sums involving digamma (psi) functions are\npresented.\n",
        "query": "expansion falling factorial",
        "docId": 2140799,
        "score": 0.4699143944330615,
        "userScore":1
    },
    {
        "title": "Quantum Mechanics As A Limiting Case of Classical Mechanics",
        "paperAbstract": "  In spite of its popularity, it has not been possible to vindicate the\nconventional wisdom that classical mechanics is a limiting case of quantum\nmechanics. The purpose of the present paper is to offer an alternative point of\nview in which quantum mechanics emerges as a limiting case of classical\nmechanics in which the classical system is decoupled from its environment.\n",
        "query": "What limitation quantum mechanics?",
        "docId": 2205489,
        "score": 0.7008932521435369,
        "userScore":2
    },
    {
        "title": "Quantum Limits in Nanomechanical Systems",
        "paperAbstract": "  In two articles, the authors claim that the Heisenberg uncertainty principle\nlimits the precision of simultaneous measurements of the position and velocity\nof a particle and refer to experimental evidence that supports their claim.\n  It is true that ever since the inception of quantum mechanics, the\nuncertainty relation that corresponds to a pair of observables represented by\nnon-commuting operators is interpreted by many scientists and engineers,\nincluding Heisenberg himself, as a limitation on the accuracy with which\nobservables can be measured. However, such a limitation cannot be deduced from\nthe postulates and theorems of quantum thermodynamics.\n",
        "query": "What limitation quantum mechanics?",
        "docId": 14808,
        "score": 0.6718279837658656,
        "userScore":2
    },
    {
        "title": "On the limits of quantum theory: contextuality and the quantum-classical\n  cut",
        "paperAbstract": "  This paper is based on four assumptions: 1. Physical reality is made of\nlinearly behaving components combined in non-linear ways. 2. Higher level\nbehaviour emerges from this lower level structure. 3. The way the lower level\nelements behaves depends on the context in which they are imbedded. 4. Quantum\ntheory applies to the lower level entities. An implication is that higher level\neffective laws, based in the outcomes of non-linear combinations of lower level\nlinear interactions, will generically not be unitary; hence the applicability\nof quantum theory at higher levels is strictly limited. This leads to the view\nthat both state vector preparation and the quantum measurement process are\ncrucially based in top-down causal effects, and helps provide criteria for the\nHeisenberg cut that challenge some views on Schroedinger\u0027s cat.\n",
        "query": "What limitation quantum mechanics?",
        "docId": 283299,
        "score": 0.6599277307671922,
        "userScore":1
    },
    {
        "title": "Is Quantum Mechanics Falsifiable? A computational perspective on the\n  foundations of Quantum Mechanics",
        "paperAbstract": "  Quantum computation teaches us that quantum mechanics exhibits exponential\ncomplexity. We argue that the standard scientific paradigm of \"predict and\nverify\" cannot be applied to testing quantum mechanics in this limit of high\ncomplexity. We describe how QM can be tested in this regime by extending the\nusual scientific paradigm to include {\\it interactive experiments}.\n",
        "query": "What limitation quantum mechanics?",
        "docId": 349779,
        "score": 0.6489677524634132,
        "userScore":2
    },
    {
        "title": "Simple explanation of the quantum limits of genuine $n$-body nonlocality",
        "paperAbstract": "  Quantum $n$-body correlations cannot be explained with $(n-1)$-body\nnonlocality. However, this genuine $n$-body nonlocality cannot surpass certain\nbounds. Here we address the problem of identifying the principles responsible\nfor these bounds. We show that, for any $n \\ge 2$, the exclusivity principle,\nas derived from axioms about sharp measurements, and a technical assumption\ngive the exact bounds predicted by quantum theory. This provides a unified\nexplanation of the bounds of single-body contextuality and $n$-body\nnonlocality, and connects two programs towards understanding quantum theory.\n",
        "query": "What limitation quantum mechanics?",
        "docId": 570125,
        "score": 0.6373454791647344,
        "userScore":1
    },
    {
        "title": "Quantum Mechanics reconstruction from invariance of the laws of nature\n  under tensor composition",
        "paperAbstract": "  Quantum and classical mechanics are derived using four natural physical\nprinciples: (1) the laws of nature are invariant under time evolution, (2) the\nlaws of nature are invariant under tensor composition, (3) the laws of nature\nare relational, and (4) positivity (the ability to define a physical state).\nQuantum mechanics is singled out by a fifth experimentally justified postulate:\nnature violates Bell\u0027s inequalities.\n",
        "query": "What limitation quantum mechanics?",
        "docId": 544367,
        "score": 0.6265655846095726,
        "userScore":1
    },
    {
        "title": "No Return to Classical Reality",
        "paperAbstract": "  At a fundamental level, the classical picture of the world is dead, and has\nbeen dead now for almost a century. Pinning down exactly which quantum\nphenomena are responsible for this has proved to be a tricky and controversial\nquestion, but a lot of progress has been made in the past few decades. We now\nhave a range of precise statements showing that whatever the ultimate laws of\nNature are, they cannot be classical. In this article, we review results on the\nfundamental phenomena of quantum theory that cannot be understood in classical\nterms. We proceed by first granting quite a broad notion of classicality,\ndescribe a range of quantum phenomena (such as randomness, discreteness, the\nindistinguishability of states, measurement-uncertainty,\nmeasurement-disturbance, complementarity, noncommutativity, interference, the\nno-cloning theorem, and the collapse of the wave-packet) that do fall under its\nliberal scope, and then finally describe some aspects of quantum physics that\ncan never admit a classical understanding -- the intrinsically quantum\nmechanical aspects of Nature. The most famous of these is Bell\u0027s theorem, but\nwe also review two more recent results in this area. Firstly, Hardy\u0027s theorem\nshows that even a finite dimensional quantum system must contain an infinite\namount of information, and secondly, the Pusey--Barrett--Rudolph theorem shows\nthat the wave-function must be an objective property of an individual quantum\nsystem. Besides being of foundational interest, results of this sort now find\nsurprising practical applications in areas such as quantum information science\nand the simulation of quantum systems.\n",
        "query": "What limitation quantum mechanics?",
        "docId": 589950,
        "score": 0.6248192214629409,
        "userScore":2
    },
    {
        "title": "Demystifying Quantum Mechanics",
        "paperAbstract": "  Why does such a successful theory like Quantum Mechanics have so many\nmysteries? The history of this theory is replete with dubious interpretations\nand controversies, and yet a knowledge of its predictions, however, contributed\nto the amazing technological revolution of the last hundred years. In its very\nbeginning Einstein pointed out that there was something missing, due to\ncontradictions with the relativity theory. So, even though Quantum Mechanics\nexplains all the nanoscale physical phenomena, there were many attempts to find\na way to \"complete\" it, e.g. hidden-variable theories. In this paper, we\ndiscuss some of those enigmas, with special attention to the concepts of\nphysical reality imposed by quantum mechanics, the role of the observer,\nprediction limits, a definition of collapse, and how to deal with correlated\nstates (the basic strategy for quantum computers and quantum teleportation).\nThat discussion is carried out within the framework of accepting that there is\nin fact nothing important missing, rather we are just restricted by the\nlimitations imposed by quantum mechanics. The mysteries are thus explained by a\nproper interpretation of those limitations, which is achieved by introducing\ntwo interpretation rules within the Copenhagen paradigm.\n",
        "query": "What limitation quantum mechanics?",
        "docId": 1479925,
        "score": 0.6220702441644264,
        "userScore":2
    },
    {
        "title": "Between classical and quantum",
        "paperAbstract": "  The relationship between classical and quantum theory is of central\nimportance to the philosophy of physics, and any interpretation of quantum\nmechanics has to clarify it. Our discussion of this relationship is partly\nhistorical and conceptual, but mostly technical and mathematically rigorous,\nincluding over 500 references. On the assumption that quantum mechanics is\nuniversal and complete, we discuss three ways in which classical physics has so\nfar been believed to emerge from quantum physics, namely in the limit h -\u003e 0 of\nsmall Planck\u0027s constant (in a finite system), in the limit of a large system,\nand through decoherence and consistent histores. The first limit is closely\nrelated to modern quantization theory and microlocal analysis, whereas the\nsecond involves methods of C*-algebras and the concepts of superselection\nsectors and macroscopic observables. In these limits, the classical world does\nnot emerge as a sharply defined objective reality, but rather as an approximate\nappearance relative to certain \"classical\" states and observables. Decoherence\nsubsequently clarifies the role of such states, in that they are \"einselected\",\ni.e. robust against coupling to the environment. Furthermore, the nature of\nclassical observables is elucidated by the fact that they typically define\n(approximately) consistent sets of histories. We make the point that\nclassicality results from the elimination of certain states and observables\nfrom quantum theory. Thus the classical world is not created by observation (as\nHeisenberg once claimed), but rather by the lack of it.\n",
        "query": "What limitation quantum mechanics?",
        "docId": 2217266,
        "score": 0.6190922944414297,
        "userScore":2
    },
    {
        "title": "Is Quantum Mechanics Incompatible with Newton\u0027s First Law",
        "paperAbstract": "  Quantum mechanics (QM) clearly violates Newton\u0027s First Law of Motion (NFLM)\nin the quantum domain for one of the simplest problems, yielding an effect in a\nforce-free region much like the Aharonov-Bohm effect. In addition, there is an\nincompatibility between the predictions of QM in the classical limit, and that\nof classical mechanics (CM) with respect to NFLM. A general argument is made\nthat such a disparity may be found commonly for a wide variety of quantum\npredictions in the classical limit. Alternatives to the Schrodinger equation\nare considered that might avoid this problem. The meaning of the classical\nlimit is examined. Critical views regarding QM by Schrodinger, Bohm, Bell,\nClauser, and others are presented to provide a more complete perspective.\n",
        "query": "What limitation quantum mechanics?",
        "docId": 8457,
        "score": 0.6140575169044666,
        "userScore":3
    }
]